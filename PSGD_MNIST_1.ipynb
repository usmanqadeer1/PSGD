{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSGD_MNIST_1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN0SoPHBlGqeBIfb1cRZW8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usmanqadeer1/PSGD/blob/master/PSGD_MNIST_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeqQbb8FMN2Z",
        "colab_type": "text"
      },
      "source": [
        "# SET UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdTAplAZulH0",
        "colab_type": "code",
        "outputId": "c1a23482-514b-445c-b91f-d484f24e0e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# !kill -9 -1\n",
        "import tensorflow as tf\n",
        "# tf.test.gpu_device_name()\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "from tabulate import tabulate\n",
        "import scipy.io\n",
        "from sklearn import metrics\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYg1M-k40VTk",
        "outputId": "ca025b90-195b-40f0-81ff-e49348d9bb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'Colab Notebooks/MS Thesis/Preconditioned SGD/MNIST/'\n",
        "results_dir = base_dir + 'results1/'\n",
        "logs_dir = base_dir + 'log'\n",
        "sys.path.append(base_dir)\n",
        "import preconditioned_stochastic_gradient_descent as psgd "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvL7hlRi6hpP",
        "colab_type": "text"
      },
      "source": [
        "#Defining Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuSY0zLx4Fg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss_metrics(xaxis,yaxis,title, x_label,y_label):\n",
        " \n",
        "  fig = go.Figure()\n",
        "  i = 0\n",
        "  if(xaxis != None):\n",
        "    for opt in opts:\n",
        "      fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n",
        "      i = i + 1\n",
        "  else:\n",
        "    for opt in opts:\n",
        "      fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n",
        "      i = i + 1\n",
        "\n",
        "  fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis_type=\"log\")\n",
        "  fig.show()\n",
        "\n",
        "def plot_acc_metrics(xaxis,yaxis,title, x_label,y_label):\n",
        " \n",
        "  fig = go.Figure()\n",
        "  i = 0\n",
        "  if(xaxis != None):\n",
        "    for opt in opts:\n",
        "      fig.add_trace(go.Scatter(x = xaxis[opt], y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n",
        "      i = i + 1\n",
        "  else:\n",
        "    for opt in opts:\n",
        "      fig.add_trace(go.Scatter(y=yaxis[opt], name = opt, mode='lines', line = dict(color = colors[i])))\n",
        "      i = i + 1\n",
        "\n",
        "  fig.update_layout(title=title, xaxis_title=x_label, yaxis_title=y_label, yaxis=dict(range=[0.95, 1]))\n",
        "  fig.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J6uwPtbEgGG",
        "colab_type": "text"
      },
      "source": [
        "# Set up for MNIST classification CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvF4DWIl_dVV",
        "colab_type": "text"
      },
      "source": [
        "## Download MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNQ-G-V1HMBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"MNIST, classification\"\"\"\n",
        "np.random.seed(0)\n",
        "\n",
        "# Parameter Settings\n",
        "batch_size = 128\n",
        "num_f = 32  # number of features \n",
        "ITERATIONS = 10000\n",
        "GAP = 100\n",
        "\n",
        "dtype = tf.float32\n",
        "\n",
        "mnist = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFdCm3PIHRsV",
        "colab_type": "code",
        "outputId": "8a421c61-c5b4-4987-8b83-0a33d03c61ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "(mnist_train_data, mnist_train_labels), (mnist_test_data, mnist_test_labels) = mnist\n",
        "\n",
        "print('total images:',mnist_train_data.shape[0])\n",
        "print('training images:',mnist_train_data.shape[0])\n",
        "print('test images:',mnist_test_data.shape[0])\n",
        "print('number of features:',mnist_train_data.shape[1])\n",
        "\n",
        "\n",
        "# reshape data as 28x28x1 image \n",
        "train_data = np.reshape(2.0*mnist_train_data - 1.0, [60000, 28, 28, 1])\n",
        "\n",
        "#binarize the labels\n",
        "train_label = tf.keras.utils.to_categorical(mnist_train_labels)\n",
        "test_label = tf.keras.utils.to_categorical(mnist_test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total images: 60000\n",
            "training images: 60000\n",
            "test images: 10000\n",
            "number of features: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8isIm4bHad9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches():\n",
        "    # generate 60000 numbers randomly without replacement\n",
        "    rp = np.random.permutation(train_data.shape[0])\n",
        "    x = -np.ones([batch_size, 32, 32, 1])\n",
        "    \n",
        "    # augumentation: randomly shifting image by +-2 pixels \n",
        "    for i in range(batch_size):\n",
        "        m = math.floor(5.0*np.random.rand())\n",
        "        n = math.floor(5.0*np.random.rand())\n",
        "        x[i, m:m+28, n:n+28] = train_data[rp[i]]\n",
        "    y = train_label[rp[0:batch_size]]\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xss95j_K_tK1",
        "colab_type": "text"
      },
      "source": [
        "## Defining CNN model for training.\n",
        "The model consists of 2 convolutional layers, 1 avgpool layer, 2 convolutional layers, 1 avg pool layer, 1 FC layer, 1 FC layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjGJE_whHdAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = tf.placeholder(dtype, [batch_size, 32, 32, 1])\n",
        "train_outputs = tf.placeholder(dtype, [batch_size, 10])\n",
        "adam_step = tf.placeholder(tf.float32, shape = ())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smTUrJJuHfh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (hight, width, in_ch, out_ch) tensor --> (hight * width * in_ch, out_ch) matrix \n",
        "W1 = tf.Variable(np.random.normal(loc=0.0, scale=1.0/np.sqrt(3*3*1+1), size=[3*3*1+1, num_f]), dtype=dtype, name = 'W1')\n",
        "W2 = tf.Variable(np.random.normal(loc=0.0, scale=1.0/np.sqrt(3*3*num_f+1), size=[3*3*num_f+1, num_f]), dtype=dtype, name = 'W2')\n",
        "W3 = tf.Variable(np.random.normal(loc=0.0, scale=1.0/np.sqrt(3*3*num_f+1), size=[3*3*num_f+1, num_f]), dtype=dtype, name = 'W3')\n",
        "W4 = tf.Variable(np.random.normal(loc=0.0, scale=1.0/np.sqrt(3*3*num_f+1), size=[3*3*num_f+1, num_f]), dtype=dtype, name = 'W4')\n",
        "W5 = tf.Variable(np.random.normal(loc=0.0, scale=1.0/np.sqrt(5*5*num_f+1), size=[5*5*num_f+1, 10]), dtype=dtype, name = 'W5')\n",
        "Ws = [W1, W2, W3, W4, W5]\n",
        "\n",
        "# we use the traditional tanh\n",
        "def model(Ws, inputs):\n",
        "    \n",
        "    W1, W2, W3, W4, W5 = Ws\n",
        "    w1 = tf.reshape(W1[:-1], [3, 3, 1, num_f])\n",
        "    b1 = W1[-1]\n",
        "    x1 = tf.tanh( tf.nn.conv2d(input = inputs,filters = w1, strides = [1,1,1,1], padding = 'VALID', name = 'Conv_1') + b1 )\n",
        "    \n",
        "        \n",
        "    w2 = tf.reshape(W2[:-1], [3, 3, num_f, num_f])\n",
        "    b2 = W2[-1]\n",
        "    x2 = tf.tanh( tf.nn.conv2d(input = x1, filters = w2, strides = [1,1,1,1], padding = 'VALID', name = 'Conv_2') + b2 )\n",
        "    \n",
        "    x2 = tf.nn.avg_pool(value = x2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID', name = 'Avg_pool_2')\n",
        "        \n",
        "    w3 = tf.reshape(W3[:-1], [3, 3, num_f, num_f])\n",
        "    b3 = W3[-1]\n",
        "    x3 = tf.tanh( tf.nn.conv2d(x2, w3, [1,1,1,1], 'VALID',name = 'Conv3') + b3 )\n",
        "        \n",
        "    w4 = tf.reshape(W4[:-1], [3, 3, num_f, num_f])\n",
        "    b4 = W4[-1]\n",
        "    x4 = tf.tanh( tf.nn.conv2d(x3, w4, [1,1,1,1], 'VALID') + b4, name = 'Conv4' )\n",
        "    \n",
        "    x4 = tf.nn.avg_pool(value = x4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID', name = 'Avg_pool4')\n",
        "    \n",
        "    batch_size = inputs.shape.as_list()[0]\n",
        "    x4_flat = tf.reshape(x4, [batch_size, -1], name = 'flatten')\n",
        "    ones = tf.ones([batch_size, 1], dtype=dtype)\n",
        "    y = tf.matmul(tf.concat([x4_flat, ones], axis = 1), W5,name = 'fc')\n",
        "    tf.summary.histogram(\"preds\", y)\n",
        "    return y\n",
        "\n",
        "test_data = -np.ones([10000, 32, 32, 1])\n",
        "test_data[:,2:30,2:30] = np.reshape(2.0*mnist_test_data - 1.0, [10000, 28, 28, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF60qeelAMeK",
        "colab_type": "text"
      },
      "source": [
        "## Define cross entropy as Training Loss function (Loss function) and Test Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_i9ccJDHhW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_criterion(Ws):\n",
        "    y = model(Ws, train_inputs)\n",
        "    train_accuracy =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_outputs, axis = 1), tf.argmax(y, axis = 1)), dtype = dtype))\n",
        "    train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_outputs, logits=y))\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "# classification error rate\n",
        "def test_criterion(Ws):\n",
        "    y = model(Ws, tf.constant(test_data, dtype=dtype))\n",
        "    test_accuracy =tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_label, axis = 1), tf.argmax(y, axis = 1)), dtype = dtype))\n",
        "    test_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=test_label, logits=y))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McV8vWu3HVY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = mnist_test_labels\n",
        "\n",
        "def final_outputs(Ws):\n",
        "  y = model(Ws, tf.constant(test_data, dtype=dtype))\n",
        "  test_preds = [tf.argmax(y[i]) for i in range(10000)]\n",
        "  return test_preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO7o4q8twYoa",
        "colab_type": "text"
      },
      "source": [
        "# First Order Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd6lQisFFwr1",
        "colab_type": "text"
      },
      "source": [
        "## SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uyos-a0hSdNO",
        "outputId": "9138396a-da45-4a21-9437-7d3560098320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 1e0 # may need gradients cliping for RNN training; otherwise, set it to an extremely large value  \n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)\n",
        "\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # writer = tf.summary.FileWriter(base_dir+'1')\n",
        "    # writer.add_graph(sess.graph)  \n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "     \n",
        "        t0 = time.time()\n",
        "        # merge = tf.summary.merge_all()\n",
        "        # _train_loss, _train_accuracy, myWs, summary = sess.run([train_loss, train_accuracy, update_Ws, merge],\n",
        "        #                           {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        _train_loss, _train_accuracy, myWs = sess.run([train_loss, train_accuracy, update_Ws],\n",
        "                                  {train_inputs: _train_inputs, train_outputs: _train_outputs})   \n",
        "        # writer.add_summary(summary)\n",
        "        Time.append(time.time() - t0)\n",
        "        \n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        " \n",
        "scipy.io.savemat(results_dir + 'SGD.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.4049949645996094; test loss: 2.325472831726074; train accuracy: 0.078125; test accuracy: 0.0843999981880188; Time: 6.358717679977417\n",
            "100: train loss: 1.2767315725445745; test loss: 0.5244912505149841; train accuracy: 0.6102406249999998; test accuracy: 0.8628000020980835; Time: 6.869877576828003\n",
            "200: train loss: 0.903284734816955; test loss: 0.2953112721443176; train accuracy: 0.7286362400635681; test accuracy: 0.9226999878883362; Time: 7.373209476470947\n",
            "300: train loss: 0.596813684978206; test loss: 0.19326435029506683; train accuracy: 0.8234783918416219; test accuracy: 0.9491999745368958; Time: 7.8747718334198\n",
            "400: train loss: 0.42241290693218636; test loss: 0.16318072378635406; train accuracy: 0.8759638387545914; test accuracy: 0.9537000060081482; Time: 8.37878680229187\n",
            "500: train loss: 0.3282435389505719; test loss: 0.13811151683330536; train accuracy: 0.9031870348839306; test accuracy: 0.960099995136261; Time: 8.884768724441528\n",
            "600: train loss: 0.27658525001029566; test loss: 0.12450944632291794; train accuracy: 0.9196457759074826; test accuracy: 0.9639999866485596; Time: 9.387304067611694\n",
            "700: train loss: 0.23653208118086747; test loss: 0.1068561002612114; train accuracy: 0.9298730921873732; test accuracy: 0.9690999984741211; Time: 9.88843059539795\n",
            "800: train loss: 0.2131164879885187; test loss: 0.0961662158370018; train accuracy: 0.9385832356074559; test accuracy: 0.9739000201225281; Time: 10.391516923904419\n",
            "900: train loss: 0.19733086896401308; test loss: 0.09203045815229416; train accuracy: 0.9435063573140237; test accuracy: 0.9740999937057495; Time: 10.895530939102173\n",
            "1000: train loss: 0.17506002488029906; test loss: 0.0904703214764595; train accuracy: 0.9502514253255566; test accuracy: 0.9728999733924866; Time: 11.398063659667969\n",
            "1100: train loss: 0.16794195247240634; test loss: 0.07137851417064667; train accuracy: 0.9513245069832581; test accuracy: 0.9805999994277954; Time: 11.901273012161255\n",
            "1200: train loss: 0.16066146429038872; test loss: 0.07562506198883057; train accuracy: 0.9548994209863928; test accuracy: 0.9789999723434448; Time: 12.404558181762695\n",
            "1300: train loss: 0.14937223317025158; test loss: 0.06296169757843018; train accuracy: 0.9576973850827302; test accuracy: 0.9822999835014343; Time: 12.908604145050049\n",
            "1400: train loss: 0.141326313091228; test loss: 0.06348687410354614; train accuracy: 0.9593830071653853; test accuracy: 0.9817000031471252; Time: 13.409936904907227\n",
            "1500: train loss: 0.13894367813090244; test loss: 0.058455321937799454; train accuracy: 0.9600155573952848; test accuracy: 0.9836999773979187; Time: 13.915493249893188\n",
            "1600: train loss: 0.13372452926011497; test loss: 0.06285906583070755; train accuracy: 0.9613976519139271; test accuracy: 0.982699990272522; Time: 14.414607524871826\n",
            "1700: train loss: 0.12424981309770249; test loss: 0.05605369806289673; train accuracy: 0.9645813012757374; test accuracy: 0.9836999773979187; Time: 14.917439222335815\n",
            "1800: train loss: 0.11701027547971916; test loss: 0.05362197756767273; train accuracy: 0.9666172966250469; test accuracy: 0.9854000210762024; Time: 15.4160737991333\n",
            "1900: train loss: 0.1162805080063904; test loss: 0.05878769978880882; train accuracy: 0.9656278906142798; test accuracy: 0.9825999736785889; Time: 15.914243698120117\n",
            "2000: train loss: 0.11058867386722783; test loss: 0.048372168093919754; train accuracy: 0.9679599975949809; test accuracy: 0.986299991607666; Time: 16.412780284881592\n",
            "2100: train loss: 0.10994134931516923; test loss: 0.05212579295039177; train accuracy: 0.9683860402863614; test accuracy: 0.9847000241279602; Time: 16.91021728515625\n",
            "2200: train loss: 0.10272177991084773; test loss: 0.046944182366132736; train accuracy: 0.9695741270090172; test accuracy: 0.9861000180244446; Time: 17.411070823669434\n",
            "2300: train loss: 0.09948857434196966; test loss: 0.046225108206272125; train accuracy: 0.9717180118668136; test accuracy: 0.9872999787330627; Time: 17.923253774642944\n",
            "2400: train loss: 0.09904116175235551; test loss: 0.04413589835166931; train accuracy: 0.9717439658322148; test accuracy: 0.9872000217437744; Time: 18.43296790122986\n",
            "2500: train loss: 0.09801337927501506; test loss: 0.04656948894262314; train accuracy: 0.9717869351020867; test accuracy: 0.9868000149726868; Time: 18.94043254852295\n",
            "2600: train loss: 0.09348750571526544; test loss: 0.04299534484744072; train accuracy: 0.9733375564084918; test accuracy: 0.9873999953269958; Time: 19.448699235916138\n",
            "2700: train loss: 0.0889683988454229; test loss: 0.04093826562166214; train accuracy: 0.9751905163305249; test accuracy: 0.9882000088691711; Time: 19.9601948261261\n",
            "2800: train loss: 0.08915074670953116; test loss: 0.04295707494020462; train accuracy: 0.974165137235301; test accuracy: 0.9872000217437744; Time: 20.471172332763672\n",
            "2900: train loss: 0.08785896339459653; test loss: 0.04065920412540436; train accuracy: 0.9750856822192382; test accuracy: 0.9883000254631042; Time: 20.979570150375366\n",
            "3000: train loss: 0.08344708023745735; test loss: 0.04122518002986908; train accuracy: 0.976559639473266; test accuracy: 0.9883000254631042; Time: 21.481157541275024\n",
            "3100: train loss: 0.08678019761018443; test loss: 0.03909536451101303; train accuracy: 0.9741579412963661; test accuracy: 0.988099992275238; Time: 21.980278968811035\n",
            "3200: train loss: 0.08676557601921576; test loss: 0.03865344077348709; train accuracy: 0.9731456005237418; test accuracy: 0.988099992275238; Time: 22.47986149787903\n",
            "3300: train loss: 0.08268244603143966; test loss: 0.03842073306441307; train accuracy: 0.9752224570744521; test accuracy: 0.9884999990463257; Time: 22.979597806930542\n",
            "3400: train loss: 0.07927763419107689; test loss: 0.038882117718458176; train accuracy: 0.9763805808045481; test accuracy: 0.9878000020980835; Time: 23.47699546813965\n",
            "3500: train loss: 0.07533792202631598; test loss: 0.03672324866056442; train accuracy: 0.9781363355330912; test accuracy: 0.9889000058174133; Time: 23.974621057510376\n",
            "3600: train loss: 0.07534859020395455; test loss: 0.039634376764297485; train accuracy: 0.9785737552295543; test accuracy: 0.9868999719619751; Time: 24.474815845489502\n",
            "3700: train loss: 0.0778255543944482; test loss: 0.04079028591513634; train accuracy: 0.977310194860324; test accuracy: 0.988099992275238; Time: 24.972540616989136\n",
            "3800: train loss: 0.07954269631394595; test loss: 0.03719818964600563; train accuracy: 0.9755303068408164; test accuracy: 0.9884999990463257; Time: 25.470993280410767\n",
            "3900: train loss: 0.07735950228749909; test loss: 0.03708379715681076; train accuracy: 0.9766058850962609; test accuracy: 0.9894000291824341; Time: 25.96945571899414\n",
            "4000: train loss: 0.07569498238510347; test loss: 0.03556264191865921; train accuracy: 0.9786407808248061; test accuracy: 0.989799976348877; Time: 26.46443819999695\n",
            "4100: train loss: 0.07421365416264429; test loss: 0.03684045746922493; train accuracy: 0.9786345303537571; test accuracy: 0.989300012588501; Time: 26.959012746810913\n",
            "4200: train loss: 0.07609702516691935; test loss: 0.034665655344724655; train accuracy: 0.9784388070026229; test accuracy: 0.989300012588501; Time: 27.454585790634155\n",
            "4300: train loss: 0.07133693560028687; test loss: 0.032566528767347336; train accuracy: 0.979418221337755; test accuracy: 0.9902999997138977; Time: 27.949247360229492\n",
            "4400: train loss: 0.06455468267218746; test loss: 0.03240068629384041; train accuracy: 0.9815490879167028; test accuracy: 0.9904000163078308; Time: 28.460023403167725\n",
            "4500: train loss: 0.06840881917168253; test loss: 0.03502164036035538; train accuracy: 0.9805703701245697; test accuracy: 0.9891999959945679; Time: 28.964792728424072\n",
            "4600: train loss: 0.06882994193914048; test loss: 0.03290235623717308; train accuracy: 0.9796699987547798; test accuracy: 0.9897000193595886; Time: 29.468730688095093\n",
            "4700: train loss: 0.06687519306416828; test loss: 0.03533581644296646; train accuracy: 0.9806771299131354; test accuracy: 0.9883999824523926; Time: 29.976808547973633\n",
            "4800: train loss: 0.06553123261144245; test loss: 0.03640139102935791; train accuracy: 0.9816865468340852; test accuracy: 0.9889000058174133; Time: 30.477832078933716\n",
            "4900: train loss: 0.06513551072396695; test loss: 0.03459641709923744; train accuracy: 0.9817345660929044; test accuracy: 0.9896000027656555; Time: 30.980730056762695\n",
            "5000: train loss: 0.06433229123192528; test loss: 0.03669754043221474; train accuracy: 0.9808613398190642; test accuracy: 0.988099992275238; Time: 31.48469376564026\n",
            "5100: train loss: 0.06190344705431113; test loss: 0.03257614001631737; train accuracy: 0.9818720017491028; test accuracy: 0.9898999929428101; Time: 31.983919143676758\n",
            "5200: train loss: 0.060108368139929556; test loss: 0.035459645092487335; train accuracy: 0.9825674076770989; test accuracy: 0.9883000254631042; Time: 32.4811851978302\n",
            "5300: train loss: 0.06050254942857087; test loss: 0.03150054067373276; train accuracy: 0.982484646272013; test accuracy: 0.9904999732971191; Time: 32.98056697845459\n",
            "5400: train loss: 0.06283302729882587; test loss: 0.035403523594141006; train accuracy: 0.9811214827803532; test accuracy: 0.9883999824523926; Time: 33.47706627845764\n",
            "5500: train loss: 0.06390494675816395; test loss: 0.037266168743371964; train accuracy: 0.9811877931550819; test accuracy: 0.9876999855041504; Time: 33.96969437599182\n",
            "5600: train loss: 0.059263940681670284; test loss: 0.031046880409121513; train accuracy: 0.9818303831850332; test accuracy: 0.9901999831199646; Time: 34.464564085006714\n",
            "5700: train loss: 0.05783217583916919; test loss: 0.03193870931863785; train accuracy: 0.9830305210161743; test accuracy: 0.9902999997138977; Time: 34.96106672286987\n",
            "5800: train loss: 0.06127855997632559; test loss: 0.03343866392970085; train accuracy: 0.9820941276800366; test accuracy: 0.9890999794006348; Time: 35.4600715637207\n",
            "5900: train loss: 0.05918827745797751; test loss: 0.030307497829198837; train accuracy: 0.9830125781363993; test accuracy: 0.9904000163078308; Time: 35.96337056159973\n",
            "6000: train loss: 0.05790097614135982; test loss: 0.03066730685532093; train accuracy: 0.9827206921434112; test accuracy: 0.9902999997138977; Time: 36.46254897117615\n",
            "6100: train loss: 0.06033833840303469; test loss: 0.02968337945640087; train accuracy: 0.9822553969939667; test accuracy: 0.9902999997138977; Time: 36.96865630149841\n",
            "6200: train loss: 0.0579434754677111; test loss: 0.029857108369469643; train accuracy: 0.9829568374864082; test accuracy: 0.9909999966621399; Time: 37.47420024871826\n",
            "6300: train loss: 0.06347242630646803; test loss: 0.03337879106402397; train accuracy: 0.9819160155926971; test accuracy: 0.989300012588501; Time: 37.97417378425598\n",
            "6400: train loss: 0.059467578683222305; test loss: 0.030601458624005318; train accuracy: 0.9825248410047905; test accuracy: 0.9898999929428101; Time: 38.47259283065796\n",
            "6500: train loss: 0.056831377333707606; test loss: 0.030194122344255447; train accuracy: 0.9827998994259226; test accuracy: 0.9908000230789185; Time: 38.9726676940918\n",
            "6600: train loss: 0.05865813751385395; test loss: 0.03224660083651543; train accuracy: 0.9822711858042308; test accuracy: 0.9891999959945679; Time: 39.47334051132202\n",
            "6700: train loss: 0.05587414493423131; test loss: 0.02990478277206421; train accuracy: 0.9840865799717825; test accuracy: 0.9907000064849854; Time: 39.97436475753784\n",
            "6800: train loss: 0.05724691077065875; test loss: 0.0306474007666111; train accuracy: 0.98366556771093; test accuracy: 0.9905999898910522; Time: 40.474915742874146\n",
            "6900: train loss: 0.05718996492805747; test loss: 0.0298993531614542; train accuracy: 0.9832488609605238; test accuracy: 0.9904000163078308; Time: 40.972002267837524\n",
            "7000: train loss: 0.057945629756581615; test loss: 0.02743501029908657; train accuracy: 0.9824195519120285; test accuracy: 0.9927999973297119; Time: 41.47070598602295\n",
            "7100: train loss: 0.05420017164337519; test loss: 0.027879012748599052; train accuracy: 0.9832800943835859; test accuracy: 0.9915000200271606; Time: 41.96998047828674\n",
            "7200: train loss: 0.0514348779462588; test loss: 0.02801360748708248; train accuracy: 0.9848329908415125; test accuracy: 0.9916999936103821; Time: 42.46856427192688\n",
            "7300: train loss: 0.05271932450839019; test loss: 0.030974145978689194; train accuracy: 0.9838475692412023; test accuracy: 0.9905999898910522; Time: 42.96602988243103\n",
            "7400: train loss: 0.05424273513120443; test loss: 0.02942522056400776; train accuracy: 0.983364449221931; test accuracy: 0.9909999966621399; Time: 43.47246432304382\n",
            "7500: train loss: 0.0495177342370562; test loss: 0.026228750124573708; train accuracy: 0.9851070857533736; test accuracy: 0.9926999807357788; Time: 43.974825382232666\n",
            "7600: train loss: 0.0533091099875893; test loss: 0.02780449204146862; train accuracy: 0.9840741823269945; test accuracy: 0.9912999868392944; Time: 44.480907917022705\n",
            "7700: train loss: 0.049365031838375276; test loss: 0.02875753864645958; train accuracy: 0.9844645796310281; test accuracy: 0.9911999702453613; Time: 44.979259967803955\n",
            "7800: train loss: 0.051682062884729034; test loss: 0.02671191468834877; train accuracy: 0.9844506935311027; test accuracy: 0.9923999905586243; Time: 45.48525547981262\n",
            "7900: train loss: 0.051415113773047236; test loss: 0.02828226052224636; train accuracy: 0.9846798106638601; test accuracy: 0.991100013256073; Time: 45.9878294467926\n",
            "8000: train loss: 0.05148192702988533; test loss: 0.029030734673142433; train accuracy: 0.9839827944624767; test accuracy: 0.9909999966621399; Time: 46.491333961486816\n",
            "8100: train loss: 0.050455262669583854; test loss: 0.028929617255926132; train accuracy: 0.9848229049522084; test accuracy: 0.991100013256073; Time: 46.992079973220825\n",
            "8200: train loss: 0.04897613958958893; test loss: 0.027503564953804016; train accuracy: 0.9853557758342839; test accuracy: 0.9915000200271606; Time: 47.491863489151\n",
            "8300: train loss: 0.049292755720387355; test loss: 0.026332305744290352; train accuracy: 0.984957343479096; test accuracy: 0.9919000267982483; Time: 47.997519969940186\n",
            "8400: train loss: 0.05199338466041213; test loss: 0.02833632193505764; train accuracy: 0.9844613975552631; test accuracy: 0.9914000034332275; Time: 48.50406837463379\n",
            "8500: train loss: 0.05124961289241545; test loss: 0.029686838388442993; train accuracy: 0.9850188739897816; test accuracy: 0.9905999898910522; Time: 49.00448203086853\n",
            "8600: train loss: 0.049807405100258294; test loss: 0.02638879418373108; train accuracy: 0.9852051284672082; test accuracy: 0.9918000102043152; Time: 49.503809213638306\n",
            "8700: train loss: 0.04885414662063749; test loss: 0.026266027241945267; train accuracy: 0.9857811474329433; test accuracy: 0.9921000003814697; Time: 50.00524282455444\n",
            "8800: train loss: 0.046624895978498906; test loss: 0.02789168618619442; train accuracy: 0.9865308599358147; test accuracy: 0.9911999702453613; Time: 50.506497621536255\n",
            "8900: train loss: 0.04579198345249264; test loss: 0.025123219937086105; train accuracy: 0.9860571123718858; test accuracy: 0.9918000102043152; Time: 51.003214597702026\n",
            "9000: train loss: 0.049461069080736574; test loss: 0.02716895192861557; train accuracy: 0.9849015028910877; test accuracy: 0.9919999837875366; Time: 51.50531816482544\n",
            "9100: train loss: 0.04565850815796304; test loss: 0.030336536467075348; train accuracy: 0.9859939661220984; test accuracy: 0.9914000034332275; Time: 52.005619764328\n",
            "9200: train loss: 0.0469349946044369; test loss: 0.025926804170012474; train accuracy: 0.9853726898245997; test accuracy: 0.991599977016449; Time: 52.50454592704773\n",
            "9300: train loss: 0.04584575305201704; test loss: 0.02741922251880169; train accuracy: 0.9860833902295197; test accuracy: 0.9918000102043152; Time: 53.0043420791626\n",
            "9400: train loss: 0.045241101711525476; test loss: 0.027103766798973083; train accuracy: 0.9859607823255873; test accuracy: 0.9914000034332275; Time: 53.50726246833801\n",
            "9500: train loss: 0.044875782470282746; test loss: 0.030149148777127266; train accuracy: 0.9866886791845582; test accuracy: 0.9897000193595886; Time: 54.007845878601074\n",
            "9600: train loss: 0.044421419748365304; test loss: 0.026309752836823463; train accuracy: 0.9863806174621307; test accuracy: 0.9908999800682068; Time: 54.50733518600464\n",
            "9700: train loss: 0.04211169187378486; test loss: 0.02504543587565422; train accuracy: 0.9874757606068789; test accuracy: 0.9919999837875366; Time: 55.00507640838623\n",
            "9800: train loss: 0.04595852570422837; test loss: 0.025427449494600296; train accuracy: 0.9854055531309468; test accuracy: 0.9912999868392944; Time: 55.50350832939148\n",
            "9900: train loss: 0.044732689606141775; test loss: 0.025263391435146332; train accuracy: 0.9863483246030683; test accuracy: 0.9922999739646912; Time: 56.00663661956787\n",
            "[[ 978    0    0    0    0    0    1    1    0    0]\n",
            " [   0 1130    1    2    0    0    0    2    0    0]\n",
            " [   2    0 1024    0    0    0    0    6    0    0]\n",
            " [   0    0    0 1004    0    3    0    2    0    1]\n",
            " [   0    0    1    0  958    0    0    0    2   21]\n",
            " [   0    0    0    5    0  884    1    1    0    1]\n",
            " [   3    3    0    1    2    4  940    1    4    0]\n",
            " [   0    0    2    1    1    0    0 1021    0    3]\n",
            " [   5    1    2    4    1    2    1    2  952    4]\n",
            " [   0    0    0    2    2    0    0    1    2 1002]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9899    0.9980    0.9939       980\n",
            "           1     0.9965    0.9956    0.9960      1135\n",
            "           2     0.9942    0.9922    0.9932      1032\n",
            "           3     0.9853    0.9941    0.9897      1010\n",
            "           4     0.9938    0.9756    0.9846       982\n",
            "           5     0.9899    0.9910    0.9905       892\n",
            "           6     0.9968    0.9812    0.9890       958\n",
            "           7     0.9846    0.9932    0.9889      1028\n",
            "           8     0.9917    0.9774    0.9845       974\n",
            "           9     0.9709    0.9931    0.9819      1009\n",
            "\n",
            "    accuracy                         0.9893     10000\n",
            "   macro avg     0.9893    0.9891    0.9892     10000\n",
            "weighted avg     0.9894    0.9893    0.9893     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_CbSgI9_XE",
        "colab_type": "text"
      },
      "source": [
        "## ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twEJ_4CL-CEF",
        "colab_type": "code",
        "outputId": "9f8c5a90-6dcb-45e9-b3d4-df1fe388ce0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adam \n",
        "step_size = 0.001\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "         \n",
        "with tf.Session() as sess:\n",
        "    train_writer = tf.summary.FileWriter( './logs/1/train ', sess.graph)\n",
        "    grads_vars = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "    grads_moment = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    new_grads_moment = [beta1*m + (1.0 - beta1)*g for (m, g) in zip(grads_moment, grads)]\n",
        "    new_grads_vars = [beta2*v + (1.0 - beta2)*g*g for (v, g) in zip(grads_vars, grads)]\n",
        "    new_grads_moment_hat = [m/(1-beta1**adam_step) for m in new_grads_moment]\n",
        "    new_grads_vars_hat = [v/(1-beta2**adam_step) for v in new_grads_vars]\n",
        "          \n",
        "    new_Ws = [W - step_size*m/(tf.sqrt(v) + offset) for (W, m, v) in zip(Ws, new_grads_moment_hat, new_grads_vars_hat)]\n",
        "  \n",
        "    update_Ws = [tf.assign(old, new) for (old, new) in zip(Ws, new_Ws)]\n",
        "    update_grads_vars = [tf.assign(old, new) for (old, new) in zip(grads_vars, new_grads_vars)]\n",
        "    update_grads_moment = [tf.assign(old, new) for (old, new) in zip(grads_moment, new_grads_moment)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)\n",
        "\n",
        "    merge = tf.summary.merge_all()\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    t = 0\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches()\n",
        "     \n",
        "        t = t + 1\n",
        "        t0 = time.time()\n",
        "        _train_loss,_train_accuracy ,_,_,_ = sess.run([train_loss, train_accuracy, update_Ws, update_grads_vars, update_grads_moment],\n",
        "                                      {train_inputs: _train_inputs, train_outputs: _train_outputs, adam_step:t})  \n",
        "        \n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "scipy.io.savemat(results_dir +'adam.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3887710571289062; test loss: 2.22733211517334; train accuracy: 0.0703125; test accuracy: 0.2168000042438507; Time: 6.963823080062866\n",
            "100: train loss: 0.8725893724679947; test loss: 0.21460556983947754; train accuracy: 0.7312921874999996; test accuracy: 0.9394999742507935; Time: 7.512877941131592\n",
            "200: train loss: 0.4970981447247418; test loss: 0.10592687875032425; train accuracy: 0.8495780837615549; test accuracy: 0.9703999757766724; Time: 8.060205459594727\n",
            "300: train loss: 0.30328636259727254; test loss: 0.0897936299443245; train accuracy: 0.9093921919939223; test accuracy: 0.9753999710083008; Time: 8.61012887954712\n",
            "400: train loss: 0.20816645816731053; test loss: 0.060148052871227264; train accuracy: 0.9390854232239534; test accuracy: 0.9836999773979187; Time: 9.156958103179932\n",
            "500: train loss: 0.15451323969475314; test loss: 0.051190536469221115; train accuracy: 0.9552806648934129; test accuracy: 0.9860000014305115; Time: 9.704942464828491\n",
            "600: train loss: 0.1252581698125759; test loss: 0.05490294098854065; train accuracy: 0.9635713955084751; test accuracy: 0.9837999939918518; Time: 10.24603009223938\n",
            "700: train loss: 0.10576111775856688; test loss: 0.04666317254304886; train accuracy: 0.9690814697528249; test accuracy: 0.9864000082015991; Time: 10.795929908752441\n",
            "800: train loss: 0.09224927416537011; test loss: 0.04385029897093773; train accuracy: 0.9728456641208661; test accuracy: 0.9866999983787537; Time: 11.340301036834717\n",
            "900: train loss: 0.08931717192804882; test loss: 0.044033266603946686; train accuracy: 0.9724292821061595; test accuracy: 0.9857000112533569; Time: 11.885071992874146\n",
            "1000: train loss: 0.08337718210047712; test loss: 0.03771059215068817; train accuracy: 0.9749389306036622; test accuracy: 0.9901000261306763; Time: 12.437929153442383\n",
            "1100: train loss: 0.07631556986684553; test loss: 0.06528091430664062; train accuracy: 0.9783275865988516; test accuracy: 0.9793999791145325; Time: 12.982921123504639\n",
            "1200: train loss: 0.07747382505451153; test loss: 0.03478015214204788; train accuracy: 0.9766384012271583; test accuracy: 0.9878000020980835; Time: 13.522409915924072\n",
            "1300: train loss: 0.0729865831943353; test loss: 0.03317992016673088; train accuracy: 0.9780635567753764; test accuracy: 0.989300012588501; Time: 14.060613870620728\n",
            "1400: train loss: 0.07045288937294097; test loss: 0.03454690799117088; train accuracy: 0.9790683540350438; test accuracy: 0.9890999794006348; Time: 14.609788179397583\n",
            "1500: train loss: 0.0686598530053292; test loss: 0.03533288463950157; train accuracy: 0.9795667434965746; test accuracy: 0.9900000095367432; Time: 15.158376932144165\n",
            "1600: train loss: 0.0631046801909919; test loss: 0.03818419948220253; train accuracy: 0.9804601278138139; test accuracy: 0.9879999756813049; Time: 15.704749584197998\n",
            "1700: train loss: 0.06300552769665962; test loss: 0.029144225642085075; train accuracy: 0.9798521701907695; test accuracy: 0.991100013256073; Time: 16.25375771522522\n",
            "1800: train loss: 0.05905265903429511; test loss: 0.03367733955383301; train accuracy: 0.9810928660368954; test accuracy: 0.9898999929428101; Time: 16.796168088912964\n",
            "1900: train loss: 0.05896455640513567; test loss: 0.03127853944897652; train accuracy: 0.9815489994503737; test accuracy: 0.9907000064849854; Time: 17.340673685073853\n",
            "2000: train loss: 0.0569551770470399; test loss: 0.030281826853752136; train accuracy: 0.9815281792840131; test accuracy: 0.9900000095367432; Time: 17.88608717918396\n",
            "2100: train loss: 0.054628016643248505; test loss: 0.03353937342762947; train accuracy: 0.9830588501950025; test accuracy: 0.9894000291824341; Time: 18.436712741851807\n",
            "2200: train loss: 0.05102724162294858; test loss: 0.027537645772099495; train accuracy: 0.9842449566810422; test accuracy: 0.991100013256073; Time: 18.98062753677368\n",
            "2300: train loss: 0.04932491344073109; test loss: 0.02806372381746769; train accuracy: 0.9848412617113133; test accuracy: 0.9914000034332275; Time: 19.523045778274536\n",
            "2400: train loss: 0.0479913442938541; test loss: 0.026633795350790024; train accuracy: 0.9855755050865281; test accuracy: 0.9914000034332275; Time: 20.066042184829712\n",
            "2500: train loss: 0.04976557016175105; test loss: 0.030417658388614655; train accuracy: 0.9848716410935234; test accuracy: 0.9894000291824341; Time: 20.60860538482666\n",
            "2600: train loss: 0.048232740218314266; test loss: 0.02646658383309841; train accuracy: 0.9858414902165299; test accuracy: 0.9918000102043152; Time: 21.149489641189575\n",
            "2700: train loss: 0.044430669025953114; test loss: 0.028644677251577377; train accuracy: 0.9855738900207781; test accuracy: 0.9911999702453613; Time: 21.692741632461548\n",
            "2800: train loss: 0.04194523794631891; test loss: 0.02616313472390175; train accuracy: 0.9863347363015087; test accuracy: 0.9912999868392944; Time: 22.23733162879944\n",
            "2900: train loss: 0.04193182152411456; test loss: 0.028573818504810333; train accuracy: 0.9871253376695525; test accuracy: 0.9908999800682068; Time: 22.77842950820923\n",
            "3000: train loss: 0.04300130378186358; test loss: 0.03326132893562317; train accuracy: 0.9867652297095006; test accuracy: 0.9891999959945679; Time: 23.322054862976074\n",
            "3100: train loss: 0.04486050967159874; test loss: 0.030842237174510956; train accuracy: 0.9856929647136559; test accuracy: 0.9897000193595886; Time: 23.87136745452881\n",
            "3200: train loss: 0.04030387368849656; test loss: 0.026989270001649857; train accuracy: 0.9879878726735856; test accuracy: 0.9908999800682068; Time: 24.41876459121704\n",
            "3300: train loss: 0.04140276248740079; test loss: 0.02836032025516033; train accuracy: 0.9872878836690889; test accuracy: 0.9912999868392944; Time: 24.963462829589844\n",
            "3400: train loss: 0.03970078869634658; test loss: 0.02568064257502556; train accuracy: 0.9879508953200594; test accuracy: 0.9914000034332275; Time: 25.508779287338257\n",
            "3500: train loss: 0.03952185903359305; test loss: 0.027466073632240295; train accuracy: 0.9877010475161329; test accuracy: 0.9921000003814697; Time: 26.052741765975952\n",
            "3600: train loss: 0.037799978117715384; test loss: 0.030052749440073967; train accuracy: 0.9888506137405442; test accuracy: 0.9901000261306763; Time: 26.600213766098022\n",
            "3700: train loss: 0.03703341264656898; test loss: 0.025097021833062172; train accuracy: 0.9886108628832083; test accuracy: 0.9921000003814697; Time: 27.14454221725464\n",
            "3800: train loss: 0.03526110667743064; test loss: 0.02819453366100788; train accuracy: 0.9895800110561609; test accuracy: 0.9908000230789185; Time: 27.689721822738647\n",
            "3900: train loss: 0.038418141297672945; test loss: 0.026074359193444252; train accuracy: 0.9882562588477545; test accuracy: 0.991599977016449; Time: 28.234602689743042\n",
            "4000: train loss: 0.03568008714368249; test loss: 0.025638196617364883; train accuracy: 0.9888005508364415; test accuracy: 0.9912999868392944; Time: 28.77781844139099\n",
            "4100: train loss: 0.03629850975788082; test loss: 0.03164996579289436; train accuracy: 0.9885084703463958; test accuracy: 0.9887999892234802; Time: 29.323780059814453\n",
            "4200: train loss: 0.03555053697269669; test loss: 0.029661528766155243; train accuracy: 0.9887192384036704; test accuracy: 0.9894000291824341; Time: 29.865682363510132\n",
            "4300: train loss: 0.03653997396495991; test loss: 0.025733202695846558; train accuracy: 0.9889094967488149; test accuracy: 0.9911999702453613; Time: 30.41515612602234\n",
            "4400: train loss: 0.03699074193892274; test loss: 0.02639186382293701; train accuracy: 0.9884889353159062; test accuracy: 0.9915000200271606; Time: 30.95835590362549\n",
            "4500: train loss: 0.033199700812480075; test loss: 0.022287674248218536; train accuracy: 0.9893794626140242; test accuracy: 0.9925000071525574; Time: 31.50580406188965\n",
            "4600: train loss: 0.030457154515920085; test loss: 0.024045731872320175; train accuracy: 0.9900145202216235; test accuracy: 0.9922999739646912; Time: 32.04733633995056\n",
            "4700: train loss: 0.029236439793135453; test loss: 0.023014793172478676; train accuracy: 0.9901479630743539; test accuracy: 0.992900013923645; Time: 32.60291242599487\n",
            "4800: train loss: 0.03686469094201047; test loss: 0.02731669321656227; train accuracy: 0.9878148066510768; test accuracy: 0.9915000200271606; Time: 33.156930685043335\n",
            "4900: train loss: 0.03327413529718323; test loss: 0.031100165098905563; train accuracy: 0.9888811053308177; test accuracy: 0.9905999898910522; Time: 33.710838079452515\n",
            "5000: train loss: 0.034276793037081116; test loss: 0.027109269052743912; train accuracy: 0.9887556679342822; test accuracy: 0.9915000200271606; Time: 34.26475548744202\n",
            "5100: train loss: 0.035961836064234316; test loss: 0.02181258797645569; train accuracy: 0.9887392532111441; test accuracy: 0.9930999875068665; Time: 34.81870412826538\n",
            "5200: train loss: 0.034482308844019566; test loss: 0.02109220251441002; train accuracy: 0.9889060316577672; test accuracy: 0.993399977684021; Time: 35.374558210372925\n",
            "5300: train loss: 0.029616639801693698; test loss: 0.022807108238339424; train accuracy: 0.9908927754189792; test accuracy: 0.9926999807357788; Time: 35.932836294174194\n",
            "5400: train loss: 0.03217269062972894; test loss: 0.03184030205011368; train accuracy: 0.9904712830968498; test accuracy: 0.9894999861717224; Time: 36.480804681777954\n",
            "5500: train loss: 0.03128045072297072; test loss: 0.02251724898815155; train accuracy: 0.9899877319300768; test accuracy: 0.9927999973297119; Time: 37.031121015548706\n",
            "5600: train loss: 0.027293449704115437; test loss: 0.031396619975566864; train accuracy: 0.991328791853135; test accuracy: 0.9911999702453613; Time: 37.57928419113159\n",
            "5700: train loss: 0.02878696155269448; test loss: 0.02204630710184574; train accuracy: 0.9907260201756543; test accuracy: 0.9921000003814697; Time: 38.128328800201416\n",
            "5800: train loss: 0.02703916716850862; test loss: 0.02019568718969822; train accuracy: 0.9915267583004895; test accuracy: 0.992900013923645; Time: 38.67279124259949\n",
            "5900: train loss: 0.024886648330890444; test loss: 0.021711919456720352; train accuracy: 0.992486202190676; test accuracy: 0.9925000071525574; Time: 39.21991014480591\n",
            "6000: train loss: 0.02690363040331781; test loss: 0.024717167019844055; train accuracy: 0.9914577483825058; test accuracy: 0.9926000237464905; Time: 39.768606662750244\n",
            "6100: train loss: 0.029586306365284348; test loss: 0.025635039433836937; train accuracy: 0.9909125708447712; test accuracy: 0.991599977016449; Time: 40.31995129585266\n",
            "6200: train loss: 0.03121667901991712; test loss: 0.02578616328537464; train accuracy: 0.9903778447728467; test accuracy: 0.9918000102043152; Time: 40.86233305931091\n",
            "6300: train loss: 0.02871027958962608; test loss: 0.020613372325897217; train accuracy: 0.9905513389210838; test accuracy: 0.9930999875068665; Time: 41.40456533432007\n",
            "6400: train loss: 0.025130487964046617; test loss: 0.02776757813990116; train accuracy: 0.9922015781672197; test accuracy: 0.9914000034332275; Time: 41.94716167449951\n",
            "6500: train loss: 0.02714752884222486; test loss: 0.02629903517663479; train accuracy: 0.991436679276703; test accuracy: 0.991599977016449; Time: 42.49197578430176\n",
            "6600: train loss: 0.026627614414467655; test loss: 0.024647317826747894; train accuracy: 0.9912836005173682; test accuracy: 0.991599977016449; Time: 43.03691792488098\n",
            "6700: train loss: 0.025316057176758524; test loss: 0.026625530794262886; train accuracy: 0.9913388393113911; test accuracy: 0.9915000200271606; Time: 43.58432674407959\n",
            "6800: train loss: 0.02737379979199224; test loss: 0.02325536496937275; train accuracy: 0.9910855045374057; test accuracy: 0.9925000071525574; Time: 44.127270460128784\n",
            "6900: train loss: 0.029441725300526737; test loss: 0.020295852795243263; train accuracy: 0.9903753796148408; test accuracy: 0.9934999942779541; Time: 44.670217514038086\n",
            "7000: train loss: 0.02826877266294241; test loss: 0.02118166722357273; train accuracy: 0.9905814869083737; test accuracy: 0.9929999709129333; Time: 45.212533950805664\n",
            "7100: train loss: 0.025970018795827866; test loss: 0.024275321513414383; train accuracy: 0.9915953286661461; test accuracy: 0.9921000003814697; Time: 45.75874471664429\n",
            "7200: train loss: 0.031529692508475766; test loss: 0.02598959393799305; train accuracy: 0.9908100213411075; test accuracy: 0.9918000102043152; Time: 46.305768728256226\n",
            "7300: train loss: 0.02878712710645693; test loss: 0.023177215829491615; train accuracy: 0.991045389532039; test accuracy: 0.9916999936103821; Time: 46.85153675079346\n",
            "7400: train loss: 0.026106842551952884; test loss: 0.02188706398010254; train accuracy: 0.9916067996253873; test accuracy: 0.992900013923645; Time: 47.39474296569824\n",
            "7500: train loss: 0.027593840919500467; test loss: 0.022969147190451622; train accuracy: 0.991359784609843; test accuracy: 0.9925000071525574; Time: 47.940714836120605\n",
            "7600: train loss: 0.03030951630922042; test loss: 0.021715212613344193; train accuracy: 0.9906883180854457; test accuracy: 0.9926000237464905; Time: 48.48126292228699\n",
            "7700: train loss: 0.02893844274145127; test loss: 0.024590706452727318; train accuracy: 0.9906138686906626; test accuracy: 0.9925000071525574; Time: 49.02863574028015\n",
            "7800: train loss: 0.025202938310044876; test loss: 0.018156081438064575; train accuracy: 0.9912600183158766; test accuracy: 0.9937999844551086; Time: 49.56972694396973\n",
            "7900: train loss: 0.022610729750358886; test loss: 0.018179012462496758; train accuracy: 0.9927956862730732; test accuracy: 0.9937000274658203; Time: 50.11490535736084\n",
            "8000: train loss: 0.023154413684072936; test loss: 0.020406339317560196; train accuracy: 0.9923652194174828; test accuracy: 0.9934999942779541; Time: 50.657726526260376\n",
            "8100: train loss: 0.022182083450595042; test loss: 0.023450329899787903; train accuracy: 0.9923384178556618; test accuracy: 0.9932000041007996; Time: 51.20332717895508\n",
            "8200: train loss: 0.02362955388227349; test loss: 0.020523300394415855; train accuracy: 0.9918339290135303; test accuracy: 0.9930999875068665; Time: 51.74661636352539\n",
            "8300: train loss: 0.02394027835353926; test loss: 0.02184285968542099; train accuracy: 0.9924898300684076; test accuracy: 0.9932000041007996; Time: 52.291972398757935\n",
            "8400: train loss: 0.025934605164703757; test loss: 0.021795935928821564; train accuracy: 0.9921612596190394; test accuracy: 0.992900013923645; Time: 52.83599519729614\n",
            "8500: train loss: 0.026172026382244303; test loss: 0.021960671991109848; train accuracy: 0.9916621602772521; test accuracy: 0.9926999807357788; Time: 53.38242292404175\n",
            "8600: train loss: 0.024142805116998678; test loss: 0.01979636587202549; train accuracy: 0.9922617870237879; test accuracy: 0.9929999709129333; Time: 53.924442768096924\n",
            "8700: train loss: 0.026160604341785642; test loss: 0.023041341453790665; train accuracy: 0.9913590312696032; test accuracy: 0.9922999739646912; Time: 54.4655396938324\n",
            "8800: train loss: 0.021605508989661983; test loss: 0.02080479823052883; train accuracy: 0.9930997614176846; test accuracy: 0.9936000108718872; Time: 55.00682759284973\n",
            "8900: train loss: 0.022637737863810237; test loss: 0.026143210008740425; train accuracy: 0.9928059090290217; test accuracy: 0.9915000200271606; Time: 55.549535512924194\n",
            "9000: train loss: 0.02274216739944415; test loss: 0.026490844786167145; train accuracy: 0.9924198713330407; test accuracy: 0.9914000034332275; Time: 56.09299063682556\n",
            "9100: train loss: 0.023285677475151766; test loss: 0.019619908183813095; train accuracy: 0.9923487172039994; test accuracy: 0.9939000010490417; Time: 56.639150619506836\n",
            "9200: train loss: 0.019970637418548178; test loss: 0.022589460015296936; train accuracy: 0.9935375585188263; test accuracy: 0.9927999973297119; Time: 57.1794810295105\n",
            "9300: train loss: 0.021445416125536028; test loss: 0.01850627176463604; train accuracy: 0.9929024479081553; test accuracy: 0.9939000010490417; Time: 57.72124409675598\n",
            "9400: train loss: 0.019861338570344167; test loss: 0.029106704518198967; train accuracy: 0.9935234256829842; test accuracy: 0.9908000230789185; Time: 58.26264953613281\n",
            "9500: train loss: 0.02315058055172997; test loss: 0.024550464004278183; train accuracy: 0.9924303449621313; test accuracy: 0.9916999936103821; Time: 58.808818101882935\n",
            "9600: train loss: 0.02066184466592416; test loss: 0.017699379473924637; train accuracy: 0.9930931016395373; test accuracy: 0.9941999912261963; Time: 59.35233759880066\n",
            "9700: train loss: 0.019689976400050338; test loss: 0.02170753851532936; train accuracy: 0.9935573939277057; test accuracy: 0.9933000206947327; Time: 59.89710855484009\n",
            "9800: train loss: 0.016930945226183058; test loss: 0.018317174166440964; train accuracy: 0.9950845932376907; test accuracy: 0.9944000244140625; Time: 60.4395649433136\n",
            "9900: train loss: 0.017576006302319293; test loss: 0.01872546225786209; train accuracy: 0.9949072374698872; test accuracy: 0.9937999844551086; Time: 60.98554253578186\n",
            "[[ 979    0    0    0    0    0    0    1    0    0]\n",
            " [   0 1128    2    1    2    0    2    0    0    0]\n",
            " [   1    0 1025    1    1    0    0    4    0    0]\n",
            " [   0    0    1 1007    0    0    0    1    0    1]\n",
            " [   0    0    0    0  977    0    0    0    1    4]\n",
            " [   0    0    1    5    0  885    1    0    0    0]\n",
            " [   6    1    1    0    1    2  947    0    0    0]\n",
            " [   1    2    5    0    3    0    0 1016    0    1]\n",
            " [   3    0    2    3    0    1    1    1  962    1]\n",
            " [   1    0    0    0    6    0    0    4    2  996]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9879    0.9990    0.9934       980\n",
            "           1     0.9973    0.9938    0.9956      1135\n",
            "           2     0.9884    0.9932    0.9908      1032\n",
            "           3     0.9902    0.9970    0.9936      1010\n",
            "           4     0.9869    0.9949    0.9909       982\n",
            "           5     0.9966    0.9922    0.9944       892\n",
            "           6     0.9958    0.9885    0.9921       958\n",
            "           7     0.9893    0.9883    0.9888      1028\n",
            "           8     0.9969    0.9877    0.9923       974\n",
            "           9     0.9930    0.9871    0.9901      1009\n",
            "\n",
            "    accuracy                         0.9922     10000\n",
            "   macro avg     0.9922    0.9922    0.9922     10000\n",
            "weighted avg     0.9922    0.9922    0.9922     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbqRTLhkAxGN",
        "colab_type": "text"
      },
      "source": [
        "## RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__zXbHf4AqL8",
        "colab_type": "code",
        "outputId": "3901efb7-1052-4290-afd6-5729f8f4cd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# RMSProp  \n",
        "step_size = 0.0005\n",
        "max_mu = 0.99\n",
        "offset = 1e-9\n",
        "         \n",
        "with tf.Session() as sess:\n",
        "    grads_vars = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "    mu = tf.Variable(initial_value=0.0, trainable=False, dtype=dtype) # forgetting factor for grads**2 estimation \n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    new_grads_vars = [mu*old + (1.0 - mu)*new*new for (old, new) in zip(grads_vars, grads)]\n",
        "    new_Ws = [W - step_size*g/tf.sqrt(v + offset) for (W, g, v) in zip(Ws, grads, new_grads_vars)]\n",
        "    new_mu = tf.minimum(max_mu, 1.0/(2.0 - mu))\n",
        "    \n",
        "    update_Ws = [tf.assign(old, new) for (old, new) in zip(Ws, new_Ws)]\n",
        "    update_grads_vars = [tf.assign(old, new) for (old, new) in zip(grads_vars, new_grads_vars)]\n",
        "    update_mu = tf.assign(mu, new_mu)\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)\n",
        "      \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "     \n",
        "        t0 = time.time()\n",
        "        _train_loss,_train_accuracy ,_,_,_ = sess.run([train_loss, train_accuracy, update_Ws, update_grads_vars, update_mu],\n",
        "                                      {train_inputs: _train_inputs, train_outputs: _train_outputs})  \n",
        "        Time.append(time.time() - t0)\n",
        "        \n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'rmsprop.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3804333209991455; test loss: 2.2963647842407227; train accuracy: 0.078125; test accuracy: 0.09830000251531601; Time: 7.600159168243408\n",
            "100: train loss: 1.1176577069401739; test loss: 0.34561532735824585; train accuracy: 0.6602835937499998; test accuracy: 0.9190999865531921; Time: 8.12697172164917\n",
            "200: train loss: 0.6773411398583398; test loss: 0.17400285601615906; train accuracy: 0.8002087965775432; test accuracy: 0.954800009727478; Time: 8.654669284820557\n",
            "300: train loss: 0.4334770073498253; test loss: 0.13219572603702545; train accuracy: 0.8751217370936799; test accuracy: 0.9635000228881836; Time: 9.17665958404541\n",
            "400: train loss: 0.30815603959024274; test loss: 0.095955029129982; train accuracy: 0.911550494360015; test accuracy: 0.9743000268936157; Time: 9.701686143875122\n",
            "500: train loss: 0.2322795113560987; test loss: 0.09619250893592834; train accuracy: 0.9332260206017013; test accuracy: 0.972599983215332; Time: 10.223303079605103\n",
            "600: train loss: 0.18690569624221973; test loss: 0.07696232944726944; train accuracy: 0.9464060738006642; test accuracy: 0.979200005531311; Time: 10.74030065536499\n",
            "700: train loss: 0.15325758286092678; test loss: 0.0617186538875103; train accuracy: 0.9557492218538839; test accuracy: 0.9830999970436096; Time: 11.255434274673462\n",
            "800: train loss: 0.13504288142831755; test loss: 0.0611736886203289; train accuracy: 0.9602074124564535; test accuracy: 0.9821000099182129; Time: 11.775747299194336\n",
            "900: train loss: 0.12105282247695495; test loss: 0.0526120662689209; train accuracy: 0.9646142022721702; test accuracy: 0.9836000204086304; Time: 12.297751426696777\n",
            "1000: train loss: 0.11181546198462325; test loss: 0.05293542519211769; train accuracy: 0.9673921825892249; test accuracy: 0.984000027179718; Time: 12.818025588989258\n",
            "1100: train loss: 0.10424423981044367; test loss: 0.045642733573913574; train accuracy: 0.968644233766028; test accuracy: 0.9850999712944031; Time: 13.33700966835022\n",
            "1200: train loss: 0.10212606083662447; test loss: 0.04998260363936424; train accuracy: 0.9693296850218719; test accuracy: 0.9835000038146973; Time: 13.856895446777344\n",
            "1300: train loss: 0.09572210349096416; test loss: 0.04445561021566391; train accuracy: 0.9713462374109627; test accuracy: 0.9872000217437744; Time: 14.375567436218262\n",
            "1400: train loss: 0.09047729059028593; test loss: 0.03976566717028618; train accuracy: 0.9733747513238962; test accuracy: 0.9873999953269958; Time: 14.896439790725708\n",
            "1500: train loss: 0.08541660789286101; test loss: 0.03768037632107735; train accuracy: 0.9750548617031408; test accuracy: 0.9889000058174133; Time: 15.418957948684692\n",
            "1600: train loss: 0.0802402233234835; test loss: 0.03922968730330467; train accuracy: 0.9761701544829343; test accuracy: 0.9890000224113464; Time: 15.943645000457764\n",
            "1700: train loss: 0.07760022061367258; test loss: 0.03663475438952446; train accuracy: 0.9769476452938516; test accuracy: 0.9883999824523926; Time: 16.462257385253906\n",
            "1800: train loss: 0.07410281441070474; test loss: 0.033094245940446854; train accuracy: 0.9773911396108927; test accuracy: 0.9897000193595886; Time: 16.978230476379395\n",
            "1900: train loss: 0.0753030208450776; test loss: 0.03238110989332199; train accuracy: 0.9783020549336847; test accuracy: 0.9905999898910522; Time: 17.501647472381592\n",
            "2000: train loss: 0.0737149871563169; test loss: 0.03733794018626213; train accuracy: 0.9787979970848912; test accuracy: 0.9879999756813049; Time: 18.025498390197754\n",
            "2100: train loss: 0.07271847435816867; test loss: 0.03067590668797493; train accuracy: 0.9791848917400352; test accuracy: 0.9904000163078308; Time: 18.54190707206726\n",
            "2200: train loss: 0.06328641992786027; test loss: 0.029201412573456764; train accuracy: 0.9816736347960046; test accuracy: 0.9908999800682068; Time: 19.060789823532104\n",
            "2300: train loss: 0.06340097115452294; test loss: 0.032170768827199936; train accuracy: 0.9813201276060142; test accuracy: 0.989799976348877; Time: 19.576884031295776\n",
            "2400: train loss: 0.0615151981519991; test loss: 0.03013618104159832; train accuracy: 0.9816956369292436; test accuracy: 0.9905999898910522; Time: 20.094900846481323\n",
            "2500: train loss: 0.06157177621552626; test loss: 0.029707828536629677; train accuracy: 0.9812893145722089; test accuracy: 0.9911999702453613; Time: 20.618104219436646\n",
            "2600: train loss: 0.058646807574655034; test loss: 0.0449688695371151; train accuracy: 0.9827468442484403; test accuracy: 0.986299991607666; Time: 21.136934518814087\n",
            "2700: train loss: 0.057533182721151564; test loss: 0.029749222099781036; train accuracy: 0.9830013301606094; test accuracy: 0.9902999997138977; Time: 21.653292179107666\n",
            "2800: train loss: 0.058426531409600704; test loss: 0.03104471042752266; train accuracy: 0.9831875089075263; test accuracy: 0.9900000095367432; Time: 22.17329430580139\n",
            "2900: train loss: 0.05783951661444498; test loss: 0.02929326705634594; train accuracy: 0.9827437798929354; test accuracy: 0.9911999702453613; Time: 22.69523310661316\n",
            "3000: train loss: 0.05607778254649854; test loss: 0.034458521753549576; train accuracy: 0.9827028535507562; test accuracy: 0.9889000058174133; Time: 23.21713876724243\n",
            "3100: train loss: 0.0519312394225942; test loss: 0.026464367285370827; train accuracy: 0.9847525759150926; test accuracy: 0.9914000034332275; Time: 23.735469579696655\n",
            "3200: train loss: 0.05244895718641498; test loss: 0.036042843014001846; train accuracy: 0.9849627950506559; test accuracy: 0.9879999756813049; Time: 24.25137495994568\n",
            "3300: train loss: 0.04930778900393936; test loss: 0.031816937029361725; train accuracy: 0.9853959111628158; test accuracy: 0.9914000034332275; Time: 24.77059817314148\n",
            "3400: train loss: 0.04881710192584167; test loss: 0.02717406675219536; train accuracy: 0.9846793195790068; test accuracy: 0.991100013256073; Time: 25.292988538742065\n",
            "3500: train loss: 0.05093692919175972; test loss: 0.02531958557665348; train accuracy: 0.984587394516745; test accuracy: 0.9921000003814697; Time: 25.81813931465149\n",
            "3600: train loss: 0.04802244355033121; test loss: 0.02826651558279991; train accuracy: 0.9862109233664799; test accuracy: 0.9919999837875366; Time: 26.333263874053955\n",
            "3700: train loss: 0.04743622059797913; test loss: 0.025567704811692238; train accuracy: 0.9856315872576659; test accuracy: 0.9919000267982483; Time: 26.853586673736572\n",
            "3800: train loss: 0.04560481473048251; test loss: 0.024310950189828873; train accuracy: 0.9851939949555742; test accuracy: 0.9922000169754028; Time: 27.375958681106567\n",
            "3900: train loss: 0.04589381937906041; test loss: 0.03009822405874729; train accuracy: 0.9864281061599908; test accuracy: 0.9900000095367432; Time: 27.897041082382202\n",
            "4000: train loss: 0.043957056592745736; test loss: 0.025106355547904968; train accuracy: 0.9870197817899039; test accuracy: 0.9921000003814697; Time: 28.421451807022095\n",
            "4100: train loss: 0.041963827413404944; test loss: 0.026826323941349983; train accuracy: 0.9869463000524759; test accuracy: 0.991599977016449; Time: 28.94330906867981\n",
            "4200: train loss: 0.042417127991357835; test loss: 0.023792844265699387; train accuracy: 0.9866619554100976; test accuracy: 0.9922999739646912; Time: 29.46867036819458\n",
            "4300: train loss: 0.04235184221173834; test loss: 0.023532163351774216; train accuracy: 0.9870140055202796; test accuracy: 0.9925000071525574; Time: 29.985876321792603\n",
            "4400: train loss: 0.04005679553440688; test loss: 0.02459375001490116; train accuracy: 0.987491467268054; test accuracy: 0.9925000071525574; Time: 30.50344705581665\n",
            "4500: train loss: 0.04003666051304669; test loss: 0.030062712728977203; train accuracy: 0.9875600571927444; test accuracy: 0.9907000064849854; Time: 31.020200729370117\n",
            "4600: train loss: 0.0393721445522608; test loss: 0.024571465328335762; train accuracy: 0.9875006986954373; test accuracy: 0.9919999837875366; Time: 31.536375999450684\n",
            "4700: train loss: 0.03769524134799188; test loss: 0.02532145194709301; train accuracy: 0.9881569970669322; test accuracy: 0.9919000267982483; Time: 32.05359768867493\n",
            "4800: train loss: 0.036847254038705875; test loss: 0.025098241865634918; train accuracy: 0.9886318186379655; test accuracy: 0.9921000003814697; Time: 32.57322716712952\n",
            "4900: train loss: 0.03664151389608787; test loss: 0.039185695350170135; train accuracy: 0.989163720537113; test accuracy: 0.9876000285148621; Time: 33.092873334884644\n",
            "5000: train loss: 0.03455703709725233; test loss: 0.024285906925797462; train accuracy: 0.9896226186232678; test accuracy: 0.9914000034332275; Time: 33.60941004753113\n",
            "5100: train loss: 0.037602692130358786; test loss: 0.02233799360692501; train accuracy: 0.9885916212215619; test accuracy: 0.9932000041007996; Time: 34.13249850273132\n",
            "5200: train loss: 0.037594665699121704; test loss: 0.023589001968503; train accuracy: 0.9888727518235233; test accuracy: 0.9919000267982483; Time: 34.653268814086914\n",
            "5300: train loss: 0.040735195705546186; test loss: 0.02239808812737465; train accuracy: 0.9881770165566253; test accuracy: 0.9933000206947327; Time: 35.17299175262451\n",
            "5400: train loss: 0.03840004362784924; test loss: 0.04506707936525345; train accuracy: 0.988150228399006; test accuracy: 0.9851999878883362; Time: 35.693114280700684\n",
            "5500: train loss: 0.03873498741306966; test loss: 0.02105083502829075; train accuracy: 0.9877978859570565; test accuracy: 0.9937000274658203; Time: 36.214781284332275\n",
            "5600: train loss: 0.03497429344222995; test loss: 0.02188160829246044; train accuracy: 0.9889350550044821; test accuracy: 0.9923999905586243; Time: 36.73316311836243\n",
            "5700: train loss: 0.0359285592542234; test loss: 0.025259392336010933; train accuracy: 0.9886961207714534; test accuracy: 0.9911999702453613; Time: 37.25056219100952\n",
            "5800: train loss: 0.03598292678287989; test loss: 0.02146245911717415; train accuracy: 0.9890079397311957; test accuracy: 0.9933000206947327; Time: 37.769164085388184\n",
            "5900: train loss: 0.03383711314022882; test loss: 0.023664452135562897; train accuracy: 0.9894486478228492; test accuracy: 0.9923999905586243; Time: 38.29144024848938\n",
            "6000: train loss: 0.03550649097158062; test loss: 0.020797759294509888; train accuracy: 0.9890652680719657; test accuracy: 0.993399977684021; Time: 38.81129264831543\n",
            "6100: train loss: 0.03293816133448087; test loss: 0.02480487897992134; train accuracy: 0.9896451782270351; test accuracy: 0.9929999709129333; Time: 39.335023164749146\n",
            "6200: train loss: 0.03342617526843972; test loss: 0.023829760029911995; train accuracy: 0.9898059847754701; test accuracy: 0.9919999837875366; Time: 39.85313701629639\n",
            "6300: train loss: 0.03328300513517663; test loss: 0.0291306022554636; train accuracy: 0.9902225357620205; test accuracy: 0.9915000200271606; Time: 40.376662254333496\n",
            "6400: train loss: 0.03309644964286714; test loss: 0.026791632175445557; train accuracy: 0.9896349797243856; test accuracy: 0.9919999837875366; Time: 40.89003133773804\n",
            "6500: train loss: 0.03352001000079735; test loss: 0.022863071411848068; train accuracy: 0.9889500214801228; test accuracy: 0.9929999709129333; Time: 41.40321230888367\n",
            "6600: train loss: 0.03321867752874236; test loss: 0.023126207292079926; train accuracy: 0.9893879741602873; test accuracy: 0.9930999875068665; Time: 41.920981884002686\n",
            "6700: train loss: 0.031033510476762123; test loss: 0.02308999001979828; train accuracy: 0.9903777367365988; test accuracy: 0.9922999739646912; Time: 42.43561553955078\n",
            "6800: train loss: 0.03162437974110913; test loss: 0.02244030311703682; train accuracy: 0.9903800268176093; test accuracy: 0.9933000206947327; Time: 42.954972982406616\n",
            "6900: train loss: 0.03179058248317341; test loss: 0.021884972229599953; train accuracy: 0.9908256017232401; test accuracy: 0.9927999973297119; Time: 43.48515462875366\n",
            "7000: train loss: 0.029796744986939006; test loss: 0.01976204290986061; train accuracy: 0.9909772645468905; test accuracy: 0.9936000108718872; Time: 44.00649619102478\n",
            "7100: train loss: 0.031070216268192095; test loss: 0.022105930373072624; train accuracy: 0.9900600140559224; test accuracy: 0.9926999807357788; Time: 44.533976793289185\n",
            "7200: train loss: 0.030881637609874477; test loss: 0.023131513968110085; train accuracy: 0.9902106856623822; test accuracy: 0.992900013923645; Time: 45.05340337753296\n",
            "7300: train loss: 0.029528696620547423; test loss: 0.022435983642935753; train accuracy: 0.99022759700154; test accuracy: 0.9929999709129333; Time: 45.57326650619507\n",
            "7400: train loss: 0.029856284683588412; test loss: 0.02402111142873764; train accuracy: 0.9906828258274325; test accuracy: 0.9929999709129333; Time: 46.10228180885315\n",
            "7500: train loss: 0.02663322502169144; test loss: 0.024903438985347748; train accuracy: 0.9917394829805691; test accuracy: 0.9912999868392944; Time: 46.63114643096924\n",
            "7600: train loss: 0.028191818901463887; test loss: 0.022251253947615623; train accuracy: 0.9917581730897926; test accuracy: 0.9926000237464905; Time: 47.15782713890076\n",
            "7700: train loss: 0.028005146610417142; test loss: 0.02001534402370453; train accuracy: 0.9914499190096173; test accuracy: 0.993399977684021; Time: 47.67787146568298\n",
            "7800: train loss: 0.02837838700983682; test loss: 0.01999793015420437; train accuracy: 0.9914317747147839; test accuracy: 0.9937999844551086; Time: 48.197043895721436\n",
            "7900: train loss: 0.02849705525772048; test loss: 0.021693376824259758; train accuracy: 0.9915081367915262; test accuracy: 0.9933000206947327; Time: 48.7058219909668\n",
            "8000: train loss: 0.030817512805600182; test loss: 0.029348980635404587; train accuracy: 0.9911608500645755; test accuracy: 0.989799976348877; Time: 49.21925973892212\n",
            "8100: train loss: 0.030070857234349165; test loss: 0.02279767394065857; train accuracy: 0.991514175681672; test accuracy: 0.9923999905586243; Time: 49.7365403175354\n",
            "8200: train loss: 0.028857009840425548; test loss: 0.018575116991996765; train accuracy: 0.9918801673318072; test accuracy: 0.9930999875068665; Time: 50.24748969078064\n",
            "8300: train loss: 0.02547537720037547; test loss: 0.02091512456536293; train accuracy: 0.9919014589359857; test accuracy: 0.9936000108718872; Time: 50.75765919685364\n",
            "8400: train loss: 0.025764526232640732; test loss: 0.02072509191930294; train accuracy: 0.9916540200727697; test accuracy: 0.9930999875068665; Time: 51.26991939544678\n",
            "8500: train loss: 0.025572152156259492; test loss: 0.025120047852396965; train accuracy: 0.9913466009128049; test accuracy: 0.9919000267982483; Time: 51.78612518310547\n",
            "8600: train loss: 0.02630669110478818; test loss: 0.029134323820471764; train accuracy: 0.9910314746906428; test accuracy: 0.9898999929428101; Time: 52.29713249206543\n",
            "8700: train loss: 0.02588700029645877; test loss: 0.02236446551978588; train accuracy: 0.9917008449945054; test accuracy: 0.9925000071525574; Time: 52.80890893936157\n",
            "8800: train loss: 0.02630692152714238; test loss: 0.019465409219264984; train accuracy: 0.9910749656659752; test accuracy: 0.9939000010490417; Time: 53.32166910171509\n",
            "8900: train loss: 0.02584223472910001; test loss: 0.020576264709234238; train accuracy: 0.9917067598446293; test accuracy: 0.9937000274658203; Time: 53.835203886032104\n",
            "9000: train loss: 0.022017443509276927; test loss: 0.019335852935910225; train accuracy: 0.9934041217386077; test accuracy: 0.9937999844551086; Time: 54.345064878463745\n",
            "9100: train loss: 0.02336128286188078; test loss: 0.03218243643641472; train accuracy: 0.9928732157004135; test accuracy: 0.9898999929428101; Time: 54.8555703163147\n",
            "9200: train loss: 0.025095066653681733; test loss: 0.02980090118944645; train accuracy: 0.992802176288548; test accuracy: 0.9919000267982483; Time: 55.37168049812317\n",
            "9300: train loss: 0.02374442101286077; test loss: 0.01997709646821022; train accuracy: 0.9928567397920486; test accuracy: 0.993399977684021; Time: 55.88149046897888\n",
            "9400: train loss: 0.023184566051642635; test loss: 0.022088857367634773; train accuracy: 0.9923525010414208; test accuracy: 0.9932000041007996; Time: 56.39237856864929\n",
            "9500: train loss: 0.024805557952173568; test loss: 0.01966463401913643; train accuracy: 0.9919724507399776; test accuracy: 0.9937999844551086; Time: 56.90079069137573\n",
            "9600: train loss: 0.027399593877939374; test loss: 0.01913626492023468; train accuracy: 0.9919028372872464; test accuracy: 0.9939000010490417; Time: 57.413092374801636\n",
            "9700: train loss: 0.02582419635262299; test loss: 0.020726634189486504; train accuracy: 0.9923938755250465; test accuracy: 0.9926999807357788; Time: 57.9193217754364\n",
            "9800: train loss: 0.02465168636711239; test loss: 0.02066917158663273; train accuracy: 0.9922297159632032; test accuracy: 0.9937999844551086; Time: 58.428746700286865\n",
            "9900: train loss: 0.025251081268054976; test loss: 0.021438544616103172; train accuracy: 0.9913928828356958; test accuracy: 0.9932000041007996; Time: 58.94126653671265\n",
            "[[ 971    0    3    1    0    1    0    1    3    0]\n",
            " [   0 1124    1    4    1    0    0    5    0    0]\n",
            " [   1    0 1029    1    0    0    0    1    0    0]\n",
            " [   0    0    0 1009    0    1    0    0    0    0]\n",
            " [   0    0    0    0  973    0    0    0    1    8]\n",
            " [   0    0    0    6    0  885    1    0    0    0]\n",
            " [   2    1    1    1    0    6  945    1    1    0]\n",
            " [   0    0    4    2    1    0    0 1018    1    2]\n",
            " [   0    0    0    7    0    3    0    0  962    2]\n",
            " [   0    0    1    3    2    0    0    1    1 1001]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9969    0.9908    0.9939       980\n",
            "           1     0.9991    0.9903    0.9947      1135\n",
            "           2     0.9904    0.9971    0.9937      1032\n",
            "           3     0.9758    0.9990    0.9873      1010\n",
            "           4     0.9959    0.9908    0.9934       982\n",
            "           5     0.9877    0.9922    0.9899       892\n",
            "           6     0.9989    0.9864    0.9926       958\n",
            "           7     0.9912    0.9903    0.9908      1028\n",
            "           8     0.9928    0.9877    0.9902       974\n",
            "           9     0.9882    0.9921    0.9901      1009\n",
            "\n",
            "    accuracy                         0.9917     10000\n",
            "   macro avg     0.9917    0.9917    0.9917     10000\n",
            "weighted avg     0.9918    0.9917    0.9917     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPutysg6wzTH",
        "colab_type": "text"
      },
      "source": [
        "# Preconditioning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyt7TjN7S29k",
        "colab_type": "text"
      },
      "source": [
        "## KRON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv3JJKX_TZyN",
        "colab_type": "code",
        "outputId": "5867e831-d163-45e0-e949-7006d4249c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Kronecker Product Preconditioning\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "    \n",
        "    Qs_left = [tf.Variable(tf.eye(row, dtype=dtype), trainable=False) for row in rows]\n",
        "    Qs_right = [tf.Variable(tf.eye(col, dtype=dtype), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "\n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    delta_grads = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, delta_grads)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_approx, qs_r_kron_approx = sess.run([Qs_left, Qs_right])\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'kron_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.352512836456299; test loss: 2.29834246635437; train accuracy: 0.0546875; test accuracy: 0.1054999977350235; Time: 8.061787843704224\n",
            "100: train loss: 0.9352476075857882; test loss: 0.18265344202518463; train accuracy: 0.7125828125; test accuracy: 0.9513999819755554; Time: 9.574213027954102\n",
            "200: train loss: 0.48318386733946594; test loss: 0.08360634744167328; train accuracy: 0.8557823398936957; test accuracy: 0.977400004863739; Time: 11.074598550796509\n",
            "300: train loss: 0.261775392530891; test loss: 0.05746502801775932; train accuracy: 0.922872444180076; test accuracy: 0.9836000204086304; Time: 12.57418942451477\n",
            "400: train loss: 0.1607586518939462; test loss: 0.043203771114349365; train accuracy: 0.9538522777390354; test accuracy: 0.9887999892234802; Time: 14.072478771209717\n",
            "500: train loss: 0.109429627991525; test loss: 0.039045240730047226; train accuracy: 0.9691152518754109; test accuracy: 0.9882000088691711; Time: 15.571375131607056\n",
            "600: train loss: 0.08395208891039702; test loss: 0.03634175285696983; train accuracy: 0.9765134362398405; test accuracy: 0.989300012588501; Time: 17.070086002349854\n",
            "700: train loss: 0.0677481686993997; test loss: 0.033884499222040176; train accuracy: 0.9807960435983458; test accuracy: 0.9897000193595886; Time: 18.56498074531555\n",
            "800: train loss: 0.059801861913555285; test loss: 0.031180985271930695; train accuracy: 0.9825526718912497; test accuracy: 0.9894999861717224; Time: 20.05809235572815\n",
            "900: train loss: 0.05353024969620371; test loss: 0.028613895177841187; train accuracy: 0.9851305416645314; test accuracy: 0.9907000064849854; Time: 21.553781509399414\n",
            "1000: train loss: 0.04666436607860704; test loss: 0.02880285121500492; train accuracy: 0.9872085460541734; test accuracy: 0.9904000163078308; Time: 23.051234483718872\n",
            "1100: train loss: 0.046815247172657976; test loss: 0.02793186716735363; train accuracy: 0.986774728612307; test accuracy: 0.9908000230789185; Time: 24.551048040390015\n",
            "1200: train loss: 0.04261707776091163; test loss: 0.025308864191174507; train accuracy: 0.987957389740689; test accuracy: 0.9923999905586243; Time: 26.04725933074951\n",
            "1300: train loss: 0.039180214067692544; test loss: 0.02353266254067421; train accuracy: 0.988500537431065; test accuracy: 0.9922000169754028; Time: 27.545729637145996\n",
            "1400: train loss: 0.034986934685701396; test loss: 0.025162894278764725; train accuracy: 0.9893019774012788; test accuracy: 0.9911999702453613; Time: 29.047775745391846\n",
            "1500: train loss: 0.03274677175507723; test loss: 0.025180136784911156; train accuracy: 0.9904724742507199; test accuracy: 0.9912999868392944; Time: 30.546262979507446\n",
            "1600: train loss: 0.03333892151487563; test loss: 0.025884177535772324; train accuracy: 0.9901713523147613; test accuracy: 0.9915000200271606; Time: 32.04916286468506\n",
            "1700: train loss: 0.02968957115206821; test loss: 0.02257772907614708; train accuracy: 0.9909972909562301; test accuracy: 0.9925000071525574; Time: 33.5431752204895\n",
            "1800: train loss: 0.028283799274867075; test loss: 0.021787183359265327; train accuracy: 0.9914632353173802; test accuracy: 0.9927999973297119; Time: 35.03448176383972\n",
            "1900: train loss: 0.024760951209782178; test loss: 0.02118293195962906; train accuracy: 0.9931809104185235; test accuracy: 0.992900013923645; Time: 36.5362811088562\n",
            "2000: train loss: 0.026073151105245558; test loss: 0.021381909027695656; train accuracy: 0.9920331393963419; test accuracy: 0.9926999807357788; Time: 38.0286602973938\n",
            "2100: train loss: 0.02510683454511737; test loss: 0.020787959918379784; train accuracy: 0.9923044534554737; test accuracy: 0.9933000206947327; Time: 39.52543520927429\n",
            "2200: train loss: 0.022050926119202403; test loss: 0.020773788914084435; train accuracy: 0.9934863092563988; test accuracy: 0.9933000206947327; Time: 41.02452039718628\n",
            "2300: train loss: 0.0202354256407571; test loss: 0.020865466445684433; train accuracy: 0.9946052492162343; test accuracy: 0.9930999875068665; Time: 42.51980662345886\n",
            "2400: train loss: 0.02318299073005999; test loss: 0.02157963253557682; train accuracy: 0.9930036643255833; test accuracy: 0.993399977684021; Time: 44.01384663581848\n",
            "2500: train loss: 0.022911544639535812; test loss: 0.020128972828388214; train accuracy: 0.9932701820790651; test accuracy: 0.9925000071525574; Time: 45.51646375656128\n",
            "2600: train loss: 0.02199010142846487; test loss: 0.021912313997745514; train accuracy: 0.9935568394715488; test accuracy: 0.9932000041007996; Time: 47.009684801101685\n",
            "2700: train loss: 0.02116225750390214; test loss: 0.023760952055454254; train accuracy: 0.9937381796166872; test accuracy: 0.9919999837875366; Time: 48.51023769378662\n",
            "2800: train loss: 0.019880344855455017; test loss: 0.02053695358335972; train accuracy: 0.9941186043749721; test accuracy: 0.9932000041007996; Time: 50.00145149230957\n",
            "2900: train loss: 0.019580780242225374; test loss: 0.020734114572405815; train accuracy: 0.9946952814839236; test accuracy: 0.9926000237464905; Time: 51.49261951446533\n",
            "3000: train loss: 0.021256279095153253; test loss: 0.01778951846063137; train accuracy: 0.9940138789283993; test accuracy: 0.9945999979972839; Time: 52.99430465698242\n",
            "3100: train loss: 0.017296771420959523; test loss: 0.01855454593896866; train accuracy: 0.995109856586477; test accuracy: 0.9934999942779541; Time: 54.4897096157074\n",
            "3200: train loss: 0.015457117985323052; test loss: 0.01886979304254055; train accuracy: 0.9958841631019517; test accuracy: 0.9940999746322632; Time: 55.98624324798584\n",
            "3300: train loss: 0.015570095665686751; test loss: 0.022569961845874786; train accuracy: 0.9958274623620136; test accuracy: 0.9933000206947327; Time: 57.48117995262146\n",
            "3400: train loss: 0.016367968362967125; test loss: 0.019392287358641624; train accuracy: 0.9950272628057201; test accuracy: 0.9937999844551086; Time: 58.979748249053955\n",
            "3500: train loss: 0.016112575367343392; test loss: 0.018875528126955032; train accuracy: 0.9954840082395476; test accuracy: 0.9944999814033508; Time: 60.471388816833496\n",
            "3600: train loss: 0.015270690283258274; test loss: 0.017893007025122643; train accuracy: 0.9956117026416362; test accuracy: 0.9940999746322632; Time: 61.96318078041077\n",
            "3700: train loss: 0.015112075191957473; test loss: 0.018230294808745384; train accuracy: 0.9957063334501691; test accuracy: 0.9940999746322632; Time: 63.45920276641846\n",
            "3800: train loss: 0.013072941376485032; test loss: 0.018674621358513832; train accuracy: 0.9961260747378738; test accuracy: 0.9936000108718872; Time: 64.95984196662903\n",
            "3900: train loss: 0.01250994706891076; test loss: 0.019642462953925133; train accuracy: 0.9962636974266629; test accuracy: 0.9940999746322632; Time: 66.45713400840759\n",
            "4000: train loss: 0.013138289646219896; test loss: 0.017610356211662292; train accuracy: 0.9959973849937168; test accuracy: 0.9936000108718872; Time: 67.96031999588013\n",
            "4100: train loss: 0.01241107871348376; test loss: 0.018429936841130257; train accuracy: 0.9965639747495944; test accuracy: 0.9934999942779541; Time: 69.4612627029419\n",
            "4200: train loss: 0.012382885438465574; test loss: 0.019730033352971077; train accuracy: 0.9964841129819203; test accuracy: 0.993399977684021; Time: 70.96030521392822\n",
            "4300: train loss: 0.01228044564452774; test loss: 0.017109425738453865; train accuracy: 0.9963481787078676; test accuracy: 0.9944000244140625; Time: 72.45864868164062\n",
            "4400: train loss: 0.01219773258203092; test loss: 0.020541099831461906; train accuracy: 0.9968105460906783; test accuracy: 0.9933000206947327; Time: 73.95718193054199\n",
            "4500: train loss: 0.011913140842019632; test loss: 0.0179127287119627; train accuracy: 0.9965496827068234; test accuracy: 0.9937999844551086; Time: 75.45136189460754\n",
            "4600: train loss: 0.010920022654445374; test loss: 0.019107045605778694; train accuracy: 0.9969640985871268; test accuracy: 0.9940999746322632; Time: 76.94962334632874\n",
            "4700: train loss: 0.011789292615400893; test loss: 0.01714700646698475; train accuracy: 0.9967895586774327; test accuracy: 0.9940999746322632; Time: 78.44364929199219\n",
            "4800: train loss: 0.010684866173322783; test loss: 0.022116513922810555; train accuracy: 0.9971936382553277; test accuracy: 0.9926000237464905; Time: 79.94002676010132\n",
            "4900: train loss: 0.009375289938498767; test loss: 0.01750069297850132; train accuracy: 0.9973876113793998; test accuracy: 0.9944000244140625; Time: 81.44270181655884\n",
            "5000: train loss: 0.009336616359238494; test loss: 0.018392575904726982; train accuracy: 0.9970349198273507; test accuracy: 0.9937000274658203; Time: 82.94440197944641\n",
            "5100: train loss: 0.011382024917858181; test loss: 0.01989995688199997; train accuracy: 0.9964188361516564; test accuracy: 0.9933000206947327; Time: 84.44401264190674\n",
            "5200: train loss: 0.009521836056248472; test loss: 0.023048989474773407; train accuracy: 0.9972627875915935; test accuracy: 0.9929999709129333; Time: 85.93818163871765\n",
            "5300: train loss: 0.008013505808316036; test loss: 0.018905626609921455; train accuracy: 0.9976783748096859; test accuracy: 0.9941999912261963; Time: 87.43200421333313\n",
            "5400: train loss: 0.009359412658331574; test loss: 0.019282761961221695; train accuracy: 0.997498073920592; test accuracy: 0.9934999942779541; Time: 88.92584586143494\n",
            "5500: train loss: 0.009894996366838464; test loss: 0.01999056339263916; train accuracy: 0.997190248738478; test accuracy: 0.993399977684021; Time: 90.42576169967651\n",
            "5600: train loss: 0.009399347232109244; test loss: 0.018604187294840813; train accuracy: 0.997339095770679; test accuracy: 0.9934999942779541; Time: 91.92030000686646\n",
            "5700: train loss: 0.01015525824393843; test loss: 0.01880546286702156; train accuracy: 0.997735714154264; test accuracy: 0.9941999912261963; Time: 93.41878747940063\n",
            "5800: train loss: 0.008735404283718527; test loss: 0.020490238443017006; train accuracy: 0.997736279240701; test accuracy: 0.9937999844551086; Time: 94.9221441745758\n",
            "5900: train loss: 0.008753979382230144; test loss: 0.01814294420182705; train accuracy: 0.9978438409685098; test accuracy: 0.9945999979972839; Time: 96.42916774749756\n",
            "6000: train loss: 0.007830429438733959; test loss: 0.02054981328547001; train accuracy: 0.9980429251529943; test accuracy: 0.9937999844551086; Time: 97.93403220176697\n",
            "6100: train loss: 0.007499064962062621; test loss: 0.017947690561413765; train accuracy: 0.9978487548081093; test accuracy: 0.9937000274658203; Time: 99.43445801734924\n",
            "6200: train loss: 0.008466390688637268; test loss: 0.019786028191447258; train accuracy: 0.9973935431487271; test accuracy: 0.9939000010490417; Time: 100.93705821037292\n",
            "6300: train loss: 0.008226537117431442; test loss: 0.01849425956606865; train accuracy: 0.9975491862718104; test accuracy: 0.9937999844551086; Time: 102.43459963798523\n",
            "6400: train loss: 0.006317583022166673; test loss: 0.016287270933389664; train accuracy: 0.9982837230944576; test accuracy: 0.995199978351593; Time: 103.92695546150208\n",
            "6500: train loss: 0.006738372875422476; test loss: 0.018902942538261414; train accuracy: 0.998203831454212; test accuracy: 0.9936000108718872; Time: 105.42594385147095\n",
            "6600: train loss: 0.007032447857293996; test loss: 0.018498804420232773; train accuracy: 0.9980467983961732; test accuracy: 0.9939000010490417; Time: 106.92780017852783\n",
            "6700: train loss: 0.006859195513361418; test loss: 0.017788249999284744; train accuracy: 0.9979342462395341; test accuracy: 0.9937000274658203; Time: 108.4267144203186\n",
            "6800: train loss: 0.006116491289219801; test loss: 0.01900181919336319; train accuracy: 0.9983150516737319; test accuracy: 0.9944000244140625; Time: 109.92689371109009\n",
            "6900: train loss: 0.006094655340395931; test loss: 0.019850220531225204; train accuracy: 0.998532441697529; test accuracy: 0.9930999875068665; Time: 111.425124168396\n",
            "7000: train loss: 0.0077853628235261185; test loss: 0.0207259189337492; train accuracy: 0.997849087978847; test accuracy: 0.9933000206947327; Time: 112.91762208938599\n",
            "7100: train loss: 0.007445699646304736; test loss: 0.020441576838493347; train accuracy: 0.9978209676005134; test accuracy: 0.9933000206947327; Time: 114.41547131538391\n",
            "7200: train loss: 0.006776436930685317; test loss: 0.02173955924808979; train accuracy: 0.9976968289312107; test accuracy: 0.9936000108718872; Time: 115.91757154464722\n",
            "7300: train loss: 0.005415353746441593; test loss: 0.026974933221936226; train accuracy: 0.998470671012913; test accuracy: 0.9909999966621399; Time: 117.41263675689697\n",
            "7400: train loss: 0.005651906164924808; test loss: 0.020929940044879913; train accuracy: 0.99893886138504; test accuracy: 0.992900013923645; Time: 118.91625261306763\n",
            "7500: train loss: 0.005482242320286441; test loss: 0.022205663844943047; train accuracy: 0.9986863959838324; test accuracy: 0.9930999875068665; Time: 120.41453552246094\n",
            "7600: train loss: 0.005964044369775084; test loss: 0.018628817051649094; train accuracy: 0.9985385574388946; test accuracy: 0.9937999844551086; Time: 121.91914963722229\n",
            "7700: train loss: 0.005713756824158386; test loss: 0.020247161388397217; train accuracy: 0.9986123725132438; test accuracy: 0.9934999942779541; Time: 123.42107701301575\n",
            "7800: train loss: 0.0052322452508487905; test loss: 0.021902065724134445; train accuracy: 0.9983987335293026; test accuracy: 0.9921000003814697; Time: 124.9170606136322\n",
            "7900: train loss: 0.006685157043571333; test loss: 0.020041612908244133; train accuracy: 0.9980191665648418; test accuracy: 0.9936000108718872; Time: 126.41775274276733\n",
            "8000: train loss: 0.006487573964342254; test loss: 0.022935131564736366; train accuracy: 0.9977681313868995; test accuracy: 0.9923999905586243; Time: 127.91769981384277\n",
            "8100: train loss: 0.00547693562795553; test loss: 0.022439241409301758; train accuracy: 0.9984257583002513; test accuracy: 0.993399977684021; Time: 129.4167275428772\n",
            "8200: train loss: 0.00648802970408904; test loss: 0.02223227359354496; train accuracy: 0.9983255222050889; test accuracy: 0.993399977684021; Time: 130.9137921333313\n",
            "8300: train loss: 0.005365419194369388; test loss: 0.01932326704263687; train accuracy: 0.9985314007459056; test accuracy: 0.9945999979972839; Time: 132.40952110290527\n",
            "8400: train loss: 0.0060532931627068; test loss: 0.022431328892707825; train accuracy: 0.9982555468342222; test accuracy: 0.9933000206947327; Time: 133.90308570861816\n",
            "8500: train loss: 0.004579645115878853; test loss: 0.02212469093501568; train accuracy: 0.9986907406880045; test accuracy: 0.9939000010490417; Time: 135.40157413482666\n",
            "8600: train loss: 0.004577442995813663; test loss: 0.021255286410450935; train accuracy: 0.9991592198215585; test accuracy: 0.9929999709129333; Time: 136.89930057525635\n",
            "8700: train loss: 0.004413684374866755; test loss: 0.021744558587670326; train accuracy: 0.998916869401314; test accuracy: 0.9929999709129333; Time: 138.39684128761292\n",
            "8800: train loss: 0.005394904562188783; test loss: 0.022453486919403076; train accuracy: 0.9986124189465972; test accuracy: 0.9937999844551086; Time: 139.89130544662476\n",
            "8900: train loss: 0.004074522151142544; test loss: 0.021373078227043152; train accuracy: 0.9989320151695412; test accuracy: 0.9939000010490417; Time: 141.39253687858582\n",
            "9000: train loss: 0.004122102147303513; test loss: 0.022992104291915894; train accuracy: 0.9988900049896968; test accuracy: 0.9930999875068665; Time: 142.90285754203796\n",
            "9100: train loss: 0.005379147803440342; test loss: 0.023971278220415115; train accuracy: 0.9981870337702148; test accuracy: 0.992900013923645; Time: 144.40808129310608\n",
            "9200: train loss: 0.004773926892007125; test loss: 0.0240417942404747; train accuracy: 0.9984636311473684; test accuracy: 0.9923999905586243; Time: 145.90900373458862\n",
            "9300: train loss: 0.004982046396693458; test loss: 0.02466689795255661; train accuracy: 0.9986371298133347; test accuracy: 0.992900013923645; Time: 147.4052631855011\n",
            "9400: train loss: 0.005564323524810486; test loss: 0.021422747522592545; train accuracy: 0.9985775929422325; test accuracy: 0.9936000108718872; Time: 148.90123414993286\n",
            "9500: train loss: 0.005234018040954702; test loss: 0.019303342327475548; train accuracy: 0.9985539077996063; test accuracy: 0.9937000274658203; Time: 150.3931725025177\n",
            "9600: train loss: 0.0076338462941721984; test loss: 0.024015186354517937; train accuracy: 0.99765085825671; test accuracy: 0.9926999807357788; Time: 151.8860559463501\n",
            "9700: train loss: 0.005836983022922946; test loss: 0.021552853286266327; train accuracy: 0.9982153965902183; test accuracy: 0.9934999942779541; Time: 153.38351559638977\n",
            "9800: train loss: 0.0046228204261788974; test loss: 0.020225705578923225; train accuracy: 0.9987371563454913; test accuracy: 0.9937999844551086; Time: 154.88139390945435\n",
            "9900: train loss: 0.0037415183272224177; test loss: 0.020210498943924904; train accuracy: 0.9992026582014905; test accuracy: 0.9937000274658203; Time: 156.37227749824524\n",
            "[[ 977    0    0    0    0    1    1    1    0    0]\n",
            " [   0 1133    1    1    0    0    0    0    0    0]\n",
            " [   1    0 1029    0    1    0    0    1    0    0]\n",
            " [   0    1    0 1001    0    6    0    1    1    0]\n",
            " [   0    0    0    0  975    0    0    1    1    5]\n",
            " [   1    0    0    2    0  887    1    0    1    0]\n",
            " [   4    2    1    0    0    1  948    0    1    1]\n",
            " [   0    7    4    0    1    0    0 1014    0    2]\n",
            " [   0    0    0    1    0    0    1    0  971    1]\n",
            " [   0    1    0    0    4    0    0    1    1 1002]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9939    0.9969    0.9954       980\n",
            "           1     0.9904    0.9982    0.9943      1135\n",
            "           2     0.9942    0.9971    0.9956      1032\n",
            "           3     0.9960    0.9911    0.9935      1010\n",
            "           4     0.9939    0.9929    0.9934       982\n",
            "           5     0.9911    0.9944    0.9927       892\n",
            "           6     0.9968    0.9896    0.9932       958\n",
            "           7     0.9951    0.9864    0.9907      1028\n",
            "           8     0.9949    0.9969    0.9959       974\n",
            "           9     0.9911    0.9931    0.9921      1009\n",
            "\n",
            "    accuracy                         0.9937     10000\n",
            "   macro avg     0.9937    0.9937    0.9937     10000\n",
            "weighted avg     0.9937    0.9937    0.9937     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRb761Av7xO",
        "colab_type": "text"
      },
      "source": [
        "## SCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUvgorbmwA9J",
        "colab_type": "code",
        "outputId": "63a2cb4c-58bc-4648-c722-5ce7b440cf0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with SCAW Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "  \n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1,row)), tf.zeros((1, row))], axis=0), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.ones((1, col)), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_scan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_scan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'scan_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3422911167144775; test loss: 2.293926954269409; train accuracy: 0.1328125; test accuracy: 0.13989999890327454; Time: 8.682962417602539\n",
            "100: train loss: 0.9978855101197953; test loss: 0.22634975612163544; train accuracy: 0.6891453124999997; test accuracy: 0.9365000128746033; Time: 10.027372360229492\n",
            "200: train loss: 0.5514038897665152; test loss: 0.10817119479179382; train accuracy: 0.8284217718227347; test accuracy: 0.9678999781608582; Time: 11.363677740097046\n",
            "300: train loss: 0.31653387096998886; test loss: 0.06137393042445183; train accuracy: 0.8999262419215982; test accuracy: 0.9811999797821045; Time: 12.697064399719238\n",
            "400: train loss: 0.1957667648789433; test loss: 0.08449813723564148; train accuracy: 0.9380837742543915; test accuracy: 0.974399983882904; Time: 14.033806324005127\n",
            "500: train loss: 0.14027595180281924; test loss: 0.053428057581186295; train accuracy: 0.9563551888310258; test accuracy: 0.983299970626831; Time: 15.369342565536499\n",
            "600: train loss: 0.11430599504703225; test loss: 0.04542835056781769; train accuracy: 0.9653292530093474; test accuracy: 0.984000027179718; Time: 16.700052499771118\n",
            "700: train loss: 0.09758604086961976; test loss: 0.036706894636154175; train accuracy: 0.9695966998441973; test accuracy: 0.9873999953269958; Time: 18.034841775894165\n",
            "800: train loss: 0.08203463422879993; test loss: 0.0350533127784729; train accuracy: 0.9749880036322548; test accuracy: 0.9869999885559082; Time: 19.364856243133545\n",
            "900: train loss: 0.07589108256476337; test loss: 0.03628876432776451; train accuracy: 0.9766869221116499; test accuracy: 0.9883000254631042; Time: 20.70013999938965\n",
            "1000: train loss: 0.06929784617150596; test loss: 0.03705168515443802; train accuracy: 0.9779387295465236; test accuracy: 0.9882000088691711; Time: 22.027313709259033\n",
            "1100: train loss: 0.06550352042029436; test loss: 0.038928352296352386; train accuracy: 0.9794463658370165; test accuracy: 0.9865000247955322; Time: 23.362450122833252\n",
            "1200: train loss: 0.06168739670543942; test loss: 0.03043418563902378; train accuracy: 0.9807550380565168; test accuracy: 0.9908999800682068; Time: 24.69202733039856\n",
            "1300: train loss: 0.057296252367160204; test loss: 0.025870760902762413; train accuracy: 0.9824827746379248; test accuracy: 0.9905999898910522; Time: 26.029784679412842\n",
            "1400: train loss: 0.05478990233492333; test loss: 0.027435513213276863; train accuracy: 0.9835854047740858; test accuracy: 0.9904999732971191; Time: 27.36566662788391\n",
            "1500: train loss: 0.05110716576261791; test loss: 0.028656646609306335; train accuracy: 0.9833770960141408; test accuracy: 0.9904000163078308; Time: 28.695607900619507\n",
            "1600: train loss: 0.05255107282972662; test loss: 0.03749934211373329; train accuracy: 0.9827726927506284; test accuracy: 0.9879999756813049; Time: 30.032113075256348\n",
            "1700: train loss: 0.05304023669784594; test loss: 0.024134060367941856; train accuracy: 0.9833457568527995; test accuracy: 0.9908000230789185; Time: 31.361485481262207\n",
            "1800: train loss: 0.048292614360899565; test loss: 0.025027725845575333; train accuracy: 0.9854936761270258; test accuracy: 0.9908000230789185; Time: 32.70591759681702\n",
            "1900: train loss: 0.04749646326963619; test loss: 0.027561116963624954; train accuracy: 0.9853552974974518; test accuracy: 0.9914000034332275; Time: 34.04483962059021\n",
            "2000: train loss: 0.04496844302151843; test loss: 0.024947145953774452; train accuracy: 0.9863904608114703; test accuracy: 0.9918000102043152; Time: 35.38378429412842\n",
            "2100: train loss: 0.04417816755823276; test loss: 0.03137778490781784; train accuracy: 0.9862437280836236; test accuracy: 0.9901999831199646; Time: 36.721216440200806\n",
            "2200: train loss: 0.04274214698013668; test loss: 0.025733157992362976; train accuracy: 0.9864070798775124; test accuracy: 0.9922000169754028; Time: 38.057257413864136\n",
            "2300: train loss: 0.04058135159135876; test loss: 0.024335959926247597; train accuracy: 0.9877014418380632; test accuracy: 0.992900013923645; Time: 39.39380693435669\n",
            "2400: train loss: 0.03806608405453903; test loss: 0.027120763435959816; train accuracy: 0.9878942360953124; test accuracy: 0.9914000034332275; Time: 40.726123094558716\n",
            "2500: train loss: 0.033909188640445044; test loss: 0.021456735208630562; train accuracy: 0.9894060640685034; test accuracy: 0.992900013923645; Time: 42.06195569038391\n",
            "2600: train loss: 0.03862005067489721; test loss: 0.026657050475478172; train accuracy: 0.9878039256485628; test accuracy: 0.9907000064849854; Time: 43.396925926208496\n",
            "2700: train loss: 0.03867677245812422; test loss: 0.023360475897789; train accuracy: 0.9878750413607822; test accuracy: 0.9925000071525574; Time: 44.732869148254395\n",
            "2800: train loss: 0.03676688157445286; test loss: 0.02462565153837204; train accuracy: 0.98851063050844; test accuracy: 0.9911999702453613; Time: 46.06780004501343\n",
            "2900: train loss: 0.0350091938382042; test loss: 0.019115570932626724; train accuracy: 0.9892104344085152; test accuracy: 0.9927999973297119; Time: 47.401142835617065\n",
            "3000: train loss: 0.037163872863306475; test loss: 0.028034325689077377; train accuracy: 0.9882813368131381; test accuracy: 0.9905999898910522; Time: 48.731924057006836\n",
            "3100: train loss: 0.034140996217311814; test loss: 0.024415964260697365; train accuracy: 0.9890501779645929; test accuracy: 0.9914000034332275; Time: 50.06670594215393\n",
            "3200: train loss: 0.03406589508440314; test loss: 0.020401937887072563; train accuracy: 0.989548348904127; test accuracy: 0.9940999746322632; Time: 51.40555214881897\n",
            "3300: train loss: 0.033296842366487445; test loss: 0.020708972588181496; train accuracy: 0.9898912813648055; test accuracy: 0.9929999709129333; Time: 52.73546314239502\n",
            "3400: train loss: 0.030026676119195583; test loss: 0.01921297423541546; train accuracy: 0.9903857974722912; test accuracy: 0.9930999875068665; Time: 54.074042558670044\n",
            "3500: train loss: 0.031831723673254424; test loss: 0.02574324980378151; train accuracy: 0.9895793398577548; test accuracy: 0.9911999702453613; Time: 55.40980410575867\n",
            "3600: train loss: 0.03208851412329386; test loss: 0.023858878761529922; train accuracy: 0.9894686704060431; test accuracy: 0.9922000169754028; Time: 56.74872398376465\n",
            "3700: train loss: 0.030195746242516218; test loss: 0.021819178014993668; train accuracy: 0.9905026298220244; test accuracy: 0.9915000200271606; Time: 58.087258100509644\n",
            "3800: train loss: 0.030746669472159194; test loss: 0.024971501901745796; train accuracy: 0.9906328174243905; test accuracy: 0.991100013256073; Time: 59.42480540275574\n",
            "3900: train loss: 0.03252267066469072; test loss: 0.01854202337563038; train accuracy: 0.9910280705685446; test accuracy: 0.9937000274658203; Time: 60.76633644104004\n",
            "4000: train loss: 0.03205269368213974; test loss: 0.022533362731337547; train accuracy: 0.9902167715810977; test accuracy: 0.9933000206947327; Time: 62.105183124542236\n",
            "4100: train loss: 0.02966009857390828; test loss: 0.02252401039004326; train accuracy: 0.9914663527311285; test accuracy: 0.9932000041007996; Time: 63.43668222427368\n",
            "4200: train loss: 0.026638302098278333; test loss: 0.018488086760044098; train accuracy: 0.992031998549719; test accuracy: 0.9939000010490417; Time: 64.76703763008118\n",
            "4300: train loss: 0.026898155950952636; test loss: 0.02238292433321476; train accuracy: 0.9911414777690735; test accuracy: 0.9922000169754028; Time: 66.10809278488159\n",
            "4400: train loss: 0.027129201086203952; test loss: 0.022220904007554054; train accuracy: 0.9917514804079057; test accuracy: 0.9930999875068665; Time: 67.44681787490845\n",
            "4500: train loss: 0.024967015077521477; test loss: 0.02170505002140999; train accuracy: 0.9925733346467569; test accuracy: 0.9927999973297119; Time: 68.77711868286133\n",
            "4600: train loss: 0.028992504834426513; test loss: 0.021946540102362633; train accuracy: 0.99098256408394; test accuracy: 0.9926999807357788; Time: 70.10936951637268\n",
            "4700: train loss: 0.027188847829380886; test loss: 0.018718264997005463; train accuracy: 0.9910346943496932; test accuracy: 0.9940000176429749; Time: 71.44327521324158\n",
            "4800: train loss: 0.022513666904265664; test loss: 0.019991211593151093; train accuracy: 0.9927891829033713; test accuracy: 0.9936000108718872; Time: 72.77060794830322\n",
            "4900: train loss: 0.025131148218447644; test loss: 0.017776068300008774; train accuracy: 0.9924478988444462; test accuracy: 0.9945999979972839; Time: 74.10273933410645\n",
            "5000: train loss: 0.022638685178455627; test loss: 0.01961134560406208; train accuracy: 0.9922076335374026; test accuracy: 0.9936000108718872; Time: 75.4406189918518\n",
            "5100: train loss: 0.02218643791658715; test loss: 0.020426016300916672; train accuracy: 0.9923702368652848; test accuracy: 0.9925000071525574; Time: 76.77093982696533\n",
            "5200: train loss: 0.023204489989032556; test loss: 0.01890098676085472; train accuracy: 0.9924228797839783; test accuracy: 0.9943000078201294; Time: 78.10452103614807\n",
            "5300: train loss: 0.022901852886673013; test loss: 0.02056899480521679; train accuracy: 0.9921872220132738; test accuracy: 0.9926999807357788; Time: 79.43479323387146\n",
            "5400: train loss: 0.022238004552896824; test loss: 0.02532626874744892; train accuracy: 0.9929554344327117; test accuracy: 0.9909999966621399; Time: 80.77124857902527\n",
            "5500: train loss: 0.024555046919940107; test loss: 0.025158563628792763; train accuracy: 0.99219095464123; test accuracy: 0.9923999905586243; Time: 82.10647654533386\n",
            "5600: train loss: 0.0234470026231579; test loss: 0.019768591970205307; train accuracy: 0.9928028730059848; test accuracy: 0.9932000041007996; Time: 83.43785691261292\n",
            "5700: train loss: 0.02298341497207585; test loss: 0.02102065458893776; train accuracy: 0.9924865481371986; test accuracy: 0.9926999807357788; Time: 84.77687692642212\n",
            "5800: train loss: 0.02163397017895653; test loss: 0.018428316339850426; train accuracy: 0.9927613195760585; test accuracy: 0.9933000206947327; Time: 86.11004734039307\n",
            "5900: train loss: 0.021018314788881148; test loss: 0.023952776566147804; train accuracy: 0.9930469645317029; test accuracy: 0.9921000003814697; Time: 87.44120836257935\n",
            "6000: train loss: 0.02217844117471868; test loss: 0.027011597529053688; train accuracy: 0.992795445518417; test accuracy: 0.991599977016449; Time: 88.77389073371887\n",
            "6100: train loss: 0.021830642520217713; test loss: 0.021964477375149727; train accuracy: 0.993108855648288; test accuracy: 0.9927999973297119; Time: 90.10531735420227\n",
            "6200: train loss: 0.023070390308133334; test loss: 0.02018129825592041; train accuracy: 0.9932650719047832; test accuracy: 0.992900013923645; Time: 91.44126749038696\n",
            "6300: train loss: 0.022947987078808817; test loss: 0.0195983424782753; train accuracy: 0.9933559586221795; test accuracy: 0.9930999875068665; Time: 92.77219605445862\n",
            "6400: train loss: 0.020085247382075323; test loss: 0.02191104181110859; train accuracy: 0.9937815079957387; test accuracy: 0.9927999973297119; Time: 94.10582542419434\n",
            "6500: train loss: 0.018394018359595393; test loss: 0.018750153481960297; train accuracy: 0.9938140380737284; test accuracy: 0.9941999912261963; Time: 95.44280099868774\n",
            "6600: train loss: 0.02030102760852593; test loss: 0.02032926119863987; train accuracy: 0.9937179868143715; test accuracy: 0.9940000176429749; Time: 96.77746176719666\n",
            "6700: train loss: 0.01981032574626915; test loss: 0.018630197271704674; train accuracy: 0.9939076511599136; test accuracy: 0.9937999844551086; Time: 98.10877394676208\n",
            "6800: train loss: 0.02041937370708261; test loss: 0.017605043947696686; train accuracy: 0.9938642623148767; test accuracy: 0.9937999844551086; Time: 99.43696022033691\n",
            "6900: train loss: 0.018856048304970206; test loss: 0.018152279779314995; train accuracy: 0.9941450644213558; test accuracy: 0.9937000274658203; Time: 100.76628017425537\n",
            "7000: train loss: 0.016554131807207467; test loss: 0.01742180436849594; train accuracy: 0.9951191955633059; test accuracy: 0.9944000244140625; Time: 102.1039879322052\n",
            "7100: train loss: 0.016045015157085677; test loss: 0.016642743721604347; train accuracy: 0.9952228116452632; test accuracy: 0.9947999715805054; Time: 103.44735360145569\n",
            "7200: train loss: 0.013828102214103046; test loss: 0.01839354634284973; train accuracy: 0.995251151856834; test accuracy: 0.992900013923645; Time: 104.79076600074768\n",
            "7300: train loss: 0.014153459903264798; test loss: 0.017069311812520027; train accuracy: 0.9955661089855722; test accuracy: 0.9943000078201294; Time: 106.12946152687073\n",
            "7400: train loss: 0.015741653884733645; test loss: 0.017037920653820038; train accuracy: 0.9948652860635222; test accuracy: 0.9944000244140625; Time: 107.47265672683716\n",
            "7500: train loss: 0.013663214797568756; test loss: 0.01629576086997986; train accuracy: 0.9958207962273838; test accuracy: 0.9950000047683716; Time: 108.816823720932\n",
            "7600: train loss: 0.01720740771016291; test loss: 0.016715586185455322; train accuracy: 0.9951203782386548; test accuracy: 0.9940999746322632; Time: 110.15895128250122\n",
            "7700: train loss: 0.014597787666753327; test loss: 0.015242760069668293; train accuracy: 0.995716604582305; test accuracy: 0.994700014591217; Time: 111.49633169174194\n",
            "7800: train loss: 0.012575700341745705; test loss: 0.016665175557136536; train accuracy: 0.9964507021110971; test accuracy: 0.994700014591217; Time: 112.84048438072205\n",
            "7900: train loss: 0.012644629174222552; test loss: 0.01528421975672245; train accuracy: 0.9958706365614828; test accuracy: 0.9950000047683716; Time: 114.17465162277222\n",
            "8000: train loss: 0.012345993559018723; test loss: 0.015171913430094719; train accuracy: 0.9960884203831806; test accuracy: 0.9950000047683716; Time: 115.51082444190979\n",
            "8100: train loss: 0.012603519662531436; test loss: 0.015968486666679382; train accuracy: 0.9963922237768663; test accuracy: 0.9945999979972839; Time: 116.85157489776611\n",
            "8200: train loss: 0.013527407195001149; test loss: 0.01553739607334137; train accuracy: 0.9961067367800973; test accuracy: 0.9948999881744385; Time: 118.18539786338806\n",
            "8300: train loss: 0.01312052669764908; test loss: 0.015456330962479115; train accuracy: 0.99624527656311; test accuracy: 0.9950000047683716; Time: 119.52456951141357\n",
            "8400: train loss: 0.012944761385442616; test loss: 0.014849452301859856; train accuracy: 0.9962501616110064; test accuracy: 0.9950000047683716; Time: 120.86372900009155\n",
            "8500: train loss: 0.010437404056002443; test loss: 0.014856509864330292; train accuracy: 0.9966920954984118; test accuracy: 0.9945999979972839; Time: 122.1971549987793\n",
            "8600: train loss: 0.011706814365824636; test loss: 0.014655407518148422; train accuracy: 0.9962968955698217; test accuracy: 0.9948999881744385; Time: 123.53392219543457\n",
            "8700: train loss: 0.010830577390414492; test loss: 0.01592177152633667; train accuracy: 0.9966378168426746; test accuracy: 0.9947999715805054; Time: 124.86419820785522\n",
            "8800: train loss: 0.011404859200270645; test loss: 0.014396710321307182; train accuracy: 0.9961609526102253; test accuracy: 0.9954000115394592; Time: 126.19921946525574\n",
            "8900: train loss: 0.011351402640793904; test loss: 0.014973752200603485; train accuracy: 0.9962182686760999; test accuracy: 0.995199978351593; Time: 127.538907289505\n",
            "9000: train loss: 0.01029997495708825; test loss: 0.015167124569416046; train accuracy: 0.9964387972722532; test accuracy: 0.9950000047683716; Time: 128.87459015846252\n",
            "9100: train loss: 0.011216765153843601; test loss: 0.014592696912586689; train accuracy: 0.996522190428399; test accuracy: 0.9955999851226807; Time: 130.21551513671875\n",
            "9200: train loss: 0.012756268204519937; test loss: 0.014596903696656227; train accuracy: 0.9962339180634231; test accuracy: 0.9952999949455261; Time: 131.5514416694641\n",
            "9300: train loss: 0.010310745845222522; test loss: 0.014957064762711525; train accuracy: 0.9968742819757354; test accuracy: 0.9948999881744385; Time: 132.88311767578125\n",
            "9400: train loss: 0.013047547340956473; test loss: 0.01511237770318985; train accuracy: 0.9960284331279056; test accuracy: 0.995199978351593; Time: 134.21388959884644\n",
            "9500: train loss: 0.011674291178325982; test loss: 0.015456032939255238; train accuracy: 0.9964329057052453; test accuracy: 0.9944999814033508; Time: 135.5489203929901\n",
            "9600: train loss: 0.011294809320620307; test loss: 0.015392260625958443; train accuracy: 0.9968950362307678; test accuracy: 0.9948999881744385; Time: 136.88869190216064\n",
            "9700: train loss: 0.009819764249403196; test loss: 0.015686316415667534; train accuracy: 0.9969264912063102; test accuracy: 0.9947999715805054; Time: 138.23691654205322\n",
            "9800: train loss: 0.0094217381479227; test loss: 0.016550512984395027; train accuracy: 0.9974228609811362; test accuracy: 0.9950000047683716; Time: 139.57493686676025\n",
            "9900: train loss: 0.011618022914087296; test loss: 0.014658289961516857; train accuracy: 0.9965599325156148; test accuracy: 0.9948999881744385; Time: 140.91234397888184\n",
            "[[ 979    0    0    0    0    0    0    1    0    0]\n",
            " [   0 1129    1    1    0    1    1    2    0    0]\n",
            " [   1    0 1028    0    0    0    0    3    0    0]\n",
            " [   0    0    0 1008    0    1    0    0    1    0]\n",
            " [   0    0    0    0  978    0    0    0    0    4]\n",
            " [   0    0    0    4    0  887    1    0    0    0]\n",
            " [   1    2    0    1    1    4  947    0    2    0]\n",
            " [   0    0    1    0    1    0    0 1024    1    1]\n",
            " [   1    0    1    2    0    0    0    0  969    1]\n",
            " [   1    0    1    0    6    1    0    3    1  996]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9959    0.9990    0.9975       980\n",
            "           1     0.9982    0.9947    0.9965      1135\n",
            "           2     0.9961    0.9961    0.9961      1032\n",
            "           3     0.9921    0.9980    0.9951      1010\n",
            "           4     0.9919    0.9959    0.9939       982\n",
            "           5     0.9922    0.9944    0.9933       892\n",
            "           6     0.9979    0.9885    0.9932       958\n",
            "           7     0.9913    0.9961    0.9937      1028\n",
            "           8     0.9949    0.9949    0.9949       974\n",
            "           9     0.9940    0.9871    0.9906      1009\n",
            "\n",
            "    accuracy                         0.9945     10000\n",
            "   macro avg     0.9945    0.9945    0.9945     10000\n",
            "weighted avg     0.9945    0.9945    0.9945     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBkpQRNIv9_J",
        "colab_type": "text"
      },
      "source": [
        "## SCAW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaM8otH-wCHp",
        "colab_type": "code",
        "outputId": "8783d4eb-ffe8-4ff0-8060-a5f303e85fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with SCAW Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "\n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    qs_right =  [tf.Variable(tf.ones(shape = (1, W.shape.as_list()[1]), dtype=dtype), trainable=False) for W in Ws]\n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_scaw(ql, qr, g) for (ql, qr, g) in zip(Qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_scaw(ql, qr, dw, dx) for (ql, qr, dw, dx) in zip(Qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(ql, new_q[0]), tf.assign(qr, new_q[1])] for (ql, qr, new_q) in zip(Qs_left, qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'scaw_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.406951665878296; test loss: 2.280700445175171; train accuracy: 0.0390625; test accuracy: 0.13850000500679016; Time: 9.061846256256104\n",
            "100: train loss: 0.9543156398713591; test loss: 0.1850489228963852; train accuracy: 0.7070843749999999; test accuracy: 0.9516000151634216; Time: 10.525571823120117\n",
            "200: train loss: 0.5216721737273958; test loss: 0.07960118353366852; train accuracy: 0.8405636188079111; test accuracy: 0.9754999876022339; Time: 11.983959913253784\n",
            "300: train loss: 0.3033535008732886; test loss: 0.07525373995304108; train accuracy: 0.9071256938884742; test accuracy: 0.9758999943733215; Time: 13.44930100440979\n",
            "400: train loss: 0.18942805916044772; test loss: 0.07641396671533585; train accuracy: 0.941123154249782; test accuracy: 0.9767000079154968; Time: 14.921849250793457\n",
            "500: train loss: 0.1350907170476837; test loss: 0.053063634783029556; train accuracy: 0.9587841912442171; test accuracy: 0.9825000166893005; Time: 16.42529821395874\n",
            "600: train loss: 0.10895862649466244; test loss: 0.09895879030227661; train accuracy: 0.9674376831989766; test accuracy: 0.9674999713897705; Time: 17.89572501182556\n",
            "700: train loss: 0.09195047500474571; test loss: 0.06053784117102623; train accuracy: 0.9722728121468976; test accuracy: 0.9786999821662903; Time: 19.351572513580322\n",
            "800: train loss: 0.0779288702079668; test loss: 0.04324423894286156; train accuracy: 0.9751564148578075; test accuracy: 0.9868999719619751; Time: 20.806944131851196\n",
            "900: train loss: 0.06915471900605108; test loss: 0.03246212750673294; train accuracy: 0.9782021220278825; test accuracy: 0.9887999892234802; Time: 22.262081623077393\n",
            "1000: train loss: 0.06586397007688766; test loss: 0.04752073809504509; train accuracy: 0.9789731333486089; test accuracy: 0.9843999743461609; Time: 23.721218585968018\n",
            "1100: train loss: 0.06634229174766698; test loss: 0.040777552872896194; train accuracy: 0.979469831198121; test accuracy: 0.9868999719619751; Time: 25.17480182647705\n",
            "1200: train loss: 0.06165050917906921; test loss: 0.030355146154761314; train accuracy: 0.980259820586909; test accuracy: 0.9898999929428101; Time: 26.632750511169434\n",
            "1300: train loss: 0.053384820001042915; test loss: 0.03193628415465355; train accuracy: 0.9835511923894404; test accuracy: 0.989799976348877; Time: 28.088703632354736\n",
            "1400: train loss: 0.053605843890907987; test loss: 0.03528237342834473; train accuracy: 0.9828953121181936; test accuracy: 0.9890000224113464; Time: 29.54399037361145\n",
            "1500: train loss: 0.048588229839298946; test loss: 0.03423921763896942; train accuracy: 0.9852548155366544; test accuracy: 0.9890999794006348; Time: 31.004430532455444\n",
            "1600: train loss: 0.048587667906416984; test loss: 0.03804000839591026; train accuracy: 0.9848538108065125; test accuracy: 0.9879000186920166; Time: 32.460057973861694\n",
            "1700: train loss: 0.0471568722413839; test loss: 0.0317571721971035; train accuracy: 0.9850323772569562; test accuracy: 0.9886000156402588; Time: 33.91968894004822\n",
            "1800: train loss: 0.04326650177915985; test loss: 0.026472076773643494; train accuracy: 0.9865277347564393; test accuracy: 0.9907000064849854; Time: 35.377601861953735\n",
            "1900: train loss: 0.043705979442589714; test loss: 0.025797946378588676; train accuracy: 0.9861988524779882; test accuracy: 0.9918000102043152; Time: 36.835723876953125\n",
            "2000: train loss: 0.04556549038486682; test loss: 0.039918944239616394; train accuracy: 0.9858152825811412; test accuracy: 0.9878000020980835; Time: 38.29243993759155\n",
            "2100: train loss: 0.042214341376654224; test loss: 0.026582397520542145; train accuracy: 0.9870184022628876; test accuracy: 0.9918000102043152; Time: 39.7531681060791\n",
            "2200: train loss: 0.03803642781533684; test loss: 0.033570222556591034; train accuracy: 0.9881339262438117; test accuracy: 0.9886000156402588; Time: 41.21240758895874\n",
            "2300: train loss: 0.04187187951168484; test loss: 0.026987368240952492; train accuracy: 0.9867480574479742; test accuracy: 0.991599977016449; Time: 42.66585874557495\n",
            "2400: train loss: 0.037075459170020045; test loss: 0.025414805859327316; train accuracy: 0.9879129708413096; test accuracy: 0.9923999905586243; Time: 44.12893342971802\n",
            "2500: train loss: 0.03458408529833095; test loss: 0.030956216156482697; train accuracy: 0.9886424652897292; test accuracy: 0.9909999966621399; Time: 45.5879168510437\n",
            "2600: train loss: 0.03541456962510518; test loss: 0.027932854369282722; train accuracy: 0.9884491838824329; test accuracy: 0.991599977016449; Time: 47.0455858707428\n",
            "2700: train loss: 0.03670533026527384; test loss: 0.028477881103754044; train accuracy: 0.9885389209613441; test accuracy: 0.9904000163078308; Time: 48.4984130859375\n",
            "2800: train loss: 0.03633768113825961; test loss: 0.02162284404039383; train accuracy: 0.9880620010292773; test accuracy: 0.9919999837875366; Time: 49.95920276641846\n",
            "2900: train loss: 0.03622952436469415; test loss: 0.026361264288425446; train accuracy: 0.9886550393047201; test accuracy: 0.9908999800682068; Time: 51.41887950897217\n",
            "3000: train loss: 0.033785714597001075; test loss: 0.027583008632063866; train accuracy: 0.9895083278161203; test accuracy: 0.9919999837875366; Time: 52.87975597381592\n",
            "3100: train loss: 0.03089846781640529; test loss: 0.02400081790983677; train accuracy: 0.9900900730198962; test accuracy: 0.9922000169754028; Time: 54.33759117126465\n",
            "3200: train loss: 0.03279172152489628; test loss: 0.030976172536611557; train accuracy: 0.9896833293389886; test accuracy: 0.9907000064849854; Time: 55.794153928756714\n",
            "3300: train loss: 0.032672385244654184; test loss: 0.03113282099366188; train accuracy: 0.9892936817312034; test accuracy: 0.9896000027656555; Time: 57.249680519104004\n",
            "3400: train loss: 0.0340894618573835; test loss: 0.026324883103370667; train accuracy: 0.9888444575625069; test accuracy: 0.9912999868392944; Time: 58.70685625076294\n",
            "3500: train loss: 0.0343865258724618; test loss: 0.024760659784078598; train accuracy: 0.9892533981230613; test accuracy: 0.9919000267982483; Time: 60.16123843193054\n",
            "3600: train loss: 0.03159590971457478; test loss: 0.043105460703372955; train accuracy: 0.9903949886903414; test accuracy: 0.986299991607666; Time: 61.61627984046936\n",
            "3700: train loss: 0.03190340524565747; test loss: 0.025461211800575256; train accuracy: 0.9909048640463506; test accuracy: 0.9919000267982483; Time: 63.072773694992065\n",
            "3800: train loss: 0.031214230828520175; test loss: 0.033679358661174774; train accuracy: 0.9909630651695748; test accuracy: 0.9887999892234802; Time: 64.52912712097168\n",
            "3900: train loss: 0.026758648904916683; test loss: 0.02278362214565277; train accuracy: 0.9913806169574915; test accuracy: 0.9926000237464905; Time: 65.9833333492279\n",
            "4000: train loss: 0.029087774286366764; test loss: 0.03416401147842407; train accuracy: 0.9909621860158393; test accuracy: 0.9890999794006348; Time: 67.43587827682495\n",
            "4100: train loss: 0.02776589757323465; test loss: 0.027339830994606018; train accuracy: 0.9916614803748861; test accuracy: 0.991599977016449; Time: 68.89098358154297\n",
            "4200: train loss: 0.02957229742990037; test loss: 0.02282330021262169; train accuracy: 0.9911861021804488; test accuracy: 0.9927999973297119; Time: 70.34691429138184\n",
            "4300: train loss: 0.025300125430845283; test loss: 0.02869332954287529; train accuracy: 0.9922609948202512; test accuracy: 0.9905999898910522; Time: 71.80420160293579\n",
            "4400: train loss: 0.026866344198612262; test loss: 0.038152795284986496; train accuracy: 0.9919956134342449; test accuracy: 0.9883000254631042; Time: 73.25737619400024\n",
            "4500: train loss: 0.02656642374840259; test loss: 0.02406340464949608; train accuracy: 0.9910830850994885; test accuracy: 0.992900013923645; Time: 74.7147147655487\n",
            "4600: train loss: 0.026046414551923875; test loss: 0.021931173279881477; train accuracy: 0.9915706993277411; test accuracy: 0.9933000206947327; Time: 76.16861724853516\n",
            "4700: train loss: 0.027270021513845527; test loss: 0.026019079610705376; train accuracy: 0.9915699503922033; test accuracy: 0.9921000003814697; Time: 77.62437510490417\n",
            "4800: train loss: 0.027447402434428232; test loss: 0.034925676882267; train accuracy: 0.9914817720997481; test accuracy: 0.9897000193595886; Time: 79.08159232139587\n",
            "4900: train loss: 0.023548379459926563; test loss: 0.025170855224132538; train accuracy: 0.9926835180399953; test accuracy: 0.9922999739646912; Time: 80.53847765922546\n",
            "5000: train loss: 0.023213249623653297; test loss: 0.024542775005102158; train accuracy: 0.99311459260367; test accuracy: 0.9919999837875366; Time: 81.99436736106873\n",
            "5100: train loss: 0.02277657202316955; test loss: 0.02807478979229927; train accuracy: 0.9925758168949029; test accuracy: 0.9908999800682068; Time: 83.44811081886292\n",
            "5200: train loss: 0.0253778075117786; test loss: 0.0282859168946743; train accuracy: 0.992200534858255; test accuracy: 0.991100013256073; Time: 84.89794945716858\n",
            "5300: train loss: 0.023929639698069518; test loss: 0.017664358019828796; train accuracy: 0.9927441380539088; test accuracy: 0.9937000274658203; Time: 86.35035753250122\n",
            "5400: train loss: 0.022997262111617175; test loss: 0.02677498199045658; train accuracy: 0.9929352334903546; test accuracy: 0.9912999868392944; Time: 87.80492520332336\n",
            "5500: train loss: 0.022657557367226643; test loss: 0.023497164249420166; train accuracy: 0.992912541818796; test accuracy: 0.9927999973297119; Time: 89.2611494064331\n",
            "5600: train loss: 0.022654388987838104; test loss: 0.023487964645028114; train accuracy: 0.9931589373622645; test accuracy: 0.9932000041007996; Time: 90.71721243858337\n",
            "5700: train loss: 0.019602911446286158; test loss: 0.019892388954758644; train accuracy: 0.9935293243713219; test accuracy: 0.992900013923645; Time: 92.17198085784912\n",
            "5800: train loss: 0.02189947831261025; test loss: 0.04584549739956856; train accuracy: 0.9927575109707063; test accuracy: 0.98580002784729; Time: 93.63432216644287\n",
            "5900: train loss: 0.0186777948319109; test loss: 0.022953897714614868; train accuracy: 0.9935891966718449; test accuracy: 0.992900013923645; Time: 95.132803440094\n",
            "6000: train loss: 0.0200464526560483; test loss: 0.03320365399122238; train accuracy: 0.9935541688951652; test accuracy: 0.9894000291824341; Time: 96.61485266685486\n",
            "6100: train loss: 0.021571303859290493; test loss: 0.02613363042473793; train accuracy: 0.9930113749505665; test accuracy: 0.9923999905586243; Time: 98.06407833099365\n",
            "6200: train loss: 0.022265815384820014; test loss: 0.02525860257446766; train accuracy: 0.9933451414421953; test accuracy: 0.992900013923645; Time: 99.51881551742554\n",
            "6300: train loss: 0.02225762085738075; test loss: 0.03612946718931198; train accuracy: 0.993016188594195; test accuracy: 0.9896000027656555; Time: 100.9775722026825\n",
            "6400: train loss: 0.021010409376421322; test loss: 0.02605707012116909; train accuracy: 0.9931885992281998; test accuracy: 0.9921000003814697; Time: 102.4290828704834\n",
            "6500: train loss: 0.02197326438921543; test loss: 0.02206094190478325; train accuracy: 0.9937345006751283; test accuracy: 0.9932000041007996; Time: 103.88395571708679\n",
            "6600: train loss: 0.021783467033954815; test loss: 0.025754712522029877; train accuracy: 0.9932701625771976; test accuracy: 0.9922000169754028; Time: 105.3409652709961\n",
            "6700: train loss: 0.02216519087173515; test loss: 0.02912338823080063; train accuracy: 0.9930939252202221; test accuracy: 0.9914000034332275; Time: 106.79297494888306\n",
            "6800: train loss: 0.019298595507080886; test loss: 0.02567601576447487; train accuracy: 0.9936357114219361; test accuracy: 0.9923999905586243; Time: 108.24517798423767\n",
            "6900: train loss: 0.01950161503144098; test loss: 0.0251453947275877; train accuracy: 0.9946224783267377; test accuracy: 0.9934999942779541; Time: 109.69536638259888\n",
            "7000: train loss: 0.022538144642877945; test loss: 0.026113051921129227; train accuracy: 0.9932431854811976; test accuracy: 0.9929999709129333; Time: 111.14754986763\n",
            "7100: train loss: 0.017917105754254026; test loss: 0.028970886021852493; train accuracy: 0.994637543046985; test accuracy: 0.991599977016449; Time: 112.60613417625427\n",
            "7200: train loss: 0.015949327622350705; test loss: 0.0220782533288002; train accuracy: 0.9944994966001773; test accuracy: 0.9937000274658203; Time: 114.05965518951416\n",
            "7300: train loss: 0.01813531531914352; test loss: 0.021648665890097618; train accuracy: 0.9942228151178534; test accuracy: 0.9932000041007996; Time: 115.51695966720581\n",
            "7400: train loss: 0.014821790587222824; test loss: 0.020571937784552574; train accuracy: 0.9951478150892451; test accuracy: 0.9940999746322632; Time: 116.96905970573425\n",
            "7500: train loss: 0.01674501198402064; test loss: 0.03718741610646248; train accuracy: 0.9946880620341593; test accuracy: 0.9886000156402588; Time: 118.42530655860901\n",
            "7600: train loss: 0.016916505749765946; test loss: 0.025019511580467224; train accuracy: 0.9946908332899117; test accuracy: 0.9923999905586243; Time: 119.87867903709412\n",
            "7700: train loss: 0.017165524005340396; test loss: 0.019967641681432724; train accuracy: 0.994123611741111; test accuracy: 0.9940999746322632; Time: 121.33428049087524\n",
            "7800: train loss: 0.017385996199497624; test loss: 0.018862411379814148; train accuracy: 0.9939051611887382; test accuracy: 0.994700014591217; Time: 122.7884418964386\n",
            "7900: train loss: 0.018154016283237553; test loss: 0.020573273301124573; train accuracy: 0.9942155972845479; test accuracy: 0.9934999942779541; Time: 124.24224591255188\n",
            "8000: train loss: 0.01883727925442506; test loss: 0.024088559672236443; train accuracy: 0.9946440389140455; test accuracy: 0.992900013923645; Time: 125.7069742679596\n",
            "8100: train loss: 0.015557584607032484; test loss: 0.022674795240163803; train accuracy: 0.9949788511068658; test accuracy: 0.9936000108718872; Time: 127.17324995994568\n",
            "8200: train loss: 0.013298788371978933; test loss: 0.02231036126613617; train accuracy: 0.995496610481097; test accuracy: 0.9929999709129333; Time: 128.62873125076294\n",
            "8300: train loss: 0.013771976862422573; test loss: 0.0275148656219244; train accuracy: 0.9958466137049815; test accuracy: 0.9926000237464905; Time: 130.08391070365906\n",
            "8400: train loss: 0.015266639757931487; test loss: 0.02090216614305973; train accuracy: 0.9953440858915421; test accuracy: 0.9937000274658203; Time: 131.5319390296936\n",
            "8500: train loss: 0.01700081320480177; test loss: 0.022754143923521042; train accuracy: 0.9949278476477866; test accuracy: 0.9932000041007996; Time: 132.990136384964\n",
            "8600: train loss: 0.014888227572349304; test loss: 0.0269656740128994; train accuracy: 0.9953950736113645; test accuracy: 0.9929999709129333; Time: 134.44895219802856\n",
            "8700: train loss: 0.013217173187360104; test loss: 0.02542666345834732; train accuracy: 0.995939828686209; test accuracy: 0.9929999709129333; Time: 135.90343856811523\n",
            "8800: train loss: 0.013333335763972208; test loss: 0.028881650418043137; train accuracy: 0.9958723513190284; test accuracy: 0.9921000003814697; Time: 137.35945057868958\n",
            "8900: train loss: 0.012730055977200791; test loss: 0.03412218764424324; train accuracy: 0.9957122581157859; test accuracy: 0.9908000230789185; Time: 138.81503319740295\n",
            "9000: train loss: 0.015886888917684035; test loss: 0.020157841965556145; train accuracy: 0.9952574957779146; test accuracy: 0.9945999979972839; Time: 140.26864767074585\n",
            "9100: train loss: 0.014946398269678042; test loss: 0.024443717673420906; train accuracy: 0.9953904552615264; test accuracy: 0.993399977684021; Time: 141.72345566749573\n",
            "9200: train loss: 0.013030759813136806; test loss: 0.02392580732703209; train accuracy: 0.9953433861189352; test accuracy: 0.9927999973297119; Time: 143.17871952056885\n",
            "9300: train loss: 0.014670983523547867; test loss: 0.029270511120557785; train accuracy: 0.9953692561254873; test accuracy: 0.9914000034332275; Time: 144.63465428352356\n",
            "9400: train loss: 0.012179593117995798; test loss: 0.020981164649128914; train accuracy: 0.9958479682412242; test accuracy: 0.9937999844551086; Time: 146.08849000930786\n",
            "9500: train loss: 0.014359223053409375; test loss: 0.02428443357348442; train accuracy: 0.9953849771670201; test accuracy: 0.9934999942779541; Time: 147.542733669281\n",
            "9600: train loss: 0.012651241960993426; test loss: 0.021856706589460373; train accuracy: 0.9955585752212751; test accuracy: 0.9941999912261963; Time: 148.99557948112488\n",
            "9700: train loss: 0.01322903291256002; test loss: 0.02360384166240692; train accuracy: 0.9953189564100494; test accuracy: 0.9933000206947327; Time: 150.4487624168396\n",
            "9800: train loss: 0.013599341346737324; test loss: 0.021874139085412025; train accuracy: 0.9955658733903732; test accuracy: 0.9936000108718872; Time: 151.94252228736877\n",
            "9900: train loss: 0.014211331147778698; test loss: 0.026080142706632614; train accuracy: 0.9953774384644458; test accuracy: 0.9934999942779541; Time: 153.4270613193512\n",
            "[[ 973    0    2    0    0    1    1    1    2    0]\n",
            " [   0 1127    2    1    0    0    3    2    0    0]\n",
            " [   1    1 1028    0    0    0    1    1    0    0]\n",
            " [   1    0    0 1005    0    4    0    0    0    0]\n",
            " [   0    0    0    0  978    0    0    1    1    2]\n",
            " [   0    0    0    2    0  889    1    0    0    0]\n",
            " [   0    0    0    0    0    4  953    0    1    0]\n",
            " [   0    2    7    0    0    0    0 1016    1    2]\n",
            " [   1    0    1    1    0    1    0    0  969    1]\n",
            " [   0    0    3    0    5    4    0    3    5  989]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9969    0.9929    0.9949       980\n",
            "           1     0.9973    0.9930    0.9951      1135\n",
            "           2     0.9856    0.9961    0.9908      1032\n",
            "           3     0.9960    0.9950    0.9955      1010\n",
            "           4     0.9949    0.9959    0.9954       982\n",
            "           5     0.9845    0.9966    0.9905       892\n",
            "           6     0.9937    0.9948    0.9943       958\n",
            "           7     0.9922    0.9883    0.9903      1028\n",
            "           8     0.9898    0.9949    0.9923       974\n",
            "           9     0.9950    0.9802    0.9875      1009\n",
            "\n",
            "    accuracy                         0.9927     10000\n",
            "   macro avg     0.9926    0.9928    0.9927     10000\n",
            "weighted avg     0.9927    0.9927    0.9927     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OocDWq-2asw",
        "colab_type": "text"
      },
      "source": [
        "## Reverse SCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frvjIreA2fUT",
        "colab_type": "code",
        "outputId": "acfea0dd-f256-4792-e850-7b0081ccfa06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Reverse SCAN Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:  \n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left = [tf.Variable(tf.ones((row, 1)), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.concat([tf.ones((1, col)), tf.zeros((1,col))], axis=0), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_reverse_scan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_reverse_scan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'reverse_scan_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3989572525024414; test loss: 2.274468421936035; train accuracy: 0.03125; test accuracy: 0.11259999871253967; Time: 9.966453313827515\n",
            "100: train loss: 1.0567421861797572; test loss: 0.2626641094684601; train accuracy: 0.66702265625; test accuracy: 0.9283999800682068; Time: 11.369228839874268\n",
            "200: train loss: 0.6132335827668665; test loss: 0.15888215601444244; train accuracy: 0.8075778018368527; test accuracy: 0.949400007724762; Time: 12.764479875564575\n",
            "300: train loss: 0.36061918752643624; test loss: 0.083712138235569; train accuracy: 0.8852634732935418; test accuracy: 0.9742000102996826; Time: 14.160664081573486\n",
            "400: train loss: 0.2309555365214612; test loss: 0.053850773721933365; train accuracy: 0.927263204477854; test accuracy: 0.9836000204086304; Time: 15.556456804275513\n",
            "500: train loss: 0.17147340771280872; test loss: 0.07865551859140396; train accuracy: 0.9464382924947589; test accuracy: 0.9754999876022339; Time: 16.94869875907898\n",
            "600: train loss: 0.13591962183849482; test loss: 0.0819237157702446; train accuracy: 0.9578705782455196; test accuracy: 0.9724000096321106; Time: 18.338969230651855\n",
            "700: train loss: 0.11575593031361744; test loss: 0.05198509991168976; train accuracy: 0.9652509647630914; test accuracy: 0.9825000166893005; Time: 19.734254598617554\n",
            "800: train loss: 0.10489930537342233; test loss: 0.04177123308181763; train accuracy: 0.9671254120908624; test accuracy: 0.9873999953269958; Time: 21.130493879318237\n",
            "900: train loss: 0.09315637930068255; test loss: 0.05169074237346649; train accuracy: 0.9703944369369433; test accuracy: 0.9843999743461609; Time: 22.526479482650757\n",
            "1000: train loss: 0.08702536029900272; test loss: 0.0460493303835392; train accuracy: 0.9727175828130913; test accuracy: 0.9847000241279602; Time: 23.919107913970947\n",
            "1100: train loss: 0.07729535174412401; test loss: 0.03513793274760246; train accuracy: 0.9765990538578277; test accuracy: 0.9887999892234802; Time: 25.310837030410767\n",
            "1200: train loss: 0.07197281966548247; test loss: 0.0382048524916172; train accuracy: 0.9771969110017998; test accuracy: 0.987500011920929; Time: 26.70282483100891\n",
            "1300: train loss: 0.07139062052875783; test loss: 0.0386902317404747; train accuracy: 0.9778634518172111; test accuracy: 0.9886999726295471; Time: 28.09287714958191\n",
            "1400: train loss: 0.06366803753674388; test loss: 0.0313602052628994; train accuracy: 0.9805121139660584; test accuracy: 0.989799976348877; Time: 29.48919129371643\n",
            "1500: train loss: 0.06529701828998458; test loss: 0.03362411633133888; train accuracy: 0.9803920439215384; test accuracy: 0.9886999726295471; Time: 30.886306762695312\n",
            "1600: train loss: 0.06854326839613094; test loss: 0.03920731693506241; train accuracy: 0.9789693157052222; test accuracy: 0.9876999855041504; Time: 32.28350210189819\n",
            "1700: train loss: 0.06768507531192042; test loss: 0.04051617532968521; train accuracy: 0.9797655669860194; test accuracy: 0.987500011920929; Time: 33.672425985336304\n",
            "1800: train loss: 0.06450455496412762; test loss: 0.034378502517938614; train accuracy: 0.9801042825847817; test accuracy: 0.9890000224113464; Time: 35.064571380615234\n",
            "1900: train loss: 0.05802591322596954; test loss: 0.043952763080596924; train accuracy: 0.9820600852194155; test accuracy: 0.9854000210762024; Time: 36.462759017944336\n",
            "2000: train loss: 0.056979754970073064; test loss: 0.02672884613275528; train accuracy: 0.9827731439542853; test accuracy: 0.9919000267982483; Time: 37.86657738685608\n",
            "2100: train loss: 0.05182501993161715; test loss: 0.029221953824162483; train accuracy: 0.9835562804335451; test accuracy: 0.9911999702453613; Time: 39.2536416053772\n",
            "2200: train loss: 0.04760665617870683; test loss: 0.027566909790039062; train accuracy: 0.9852734767681687; test accuracy: 0.9919999837875366; Time: 40.65185761451721\n",
            "2300: train loss: 0.05143058202374894; test loss: 0.06347844004631042; train accuracy: 0.9837986348378814; test accuracy: 0.9797000288963318; Time: 42.043150663375854\n",
            "2400: train loss: 0.05271161282737847; test loss: 0.04295261949300766; train accuracy: 0.9832981826426016; test accuracy: 0.9848999977111816; Time: 43.43567514419556\n",
            "2500: train loss: 0.049346134426228636; test loss: 0.02574235573410988; train accuracy: 0.9839495439573755; test accuracy: 0.9915000200271606; Time: 44.82813811302185\n",
            "2600: train loss: 0.04871133242368642; test loss: 0.024591675028204918; train accuracy: 0.9838972536126256; test accuracy: 0.9926999807357788; Time: 46.219765186309814\n",
            "2700: train loss: 0.04840424565535115; test loss: 0.030646832659840584; train accuracy: 0.9848464601588792; test accuracy: 0.9900000095367432; Time: 47.61009955406189\n",
            "2800: train loss: 0.04725232919048473; test loss: 0.030950451269745827; train accuracy: 0.9852049724726082; test accuracy: 0.989300012588501; Time: 49.00726389884949\n",
            "2900: train loss: 0.05000195124215862; test loss: 0.026137515902519226; train accuracy: 0.9855180925944369; test accuracy: 0.9912999868392944; Time: 50.402517318725586\n",
            "3000: train loss: 0.044213645685488134; test loss: 0.031417366117239; train accuracy: 0.985915842818715; test accuracy: 0.9902999997138977; Time: 51.79584240913391\n",
            "3100: train loss: 0.044589194840111694; test loss: 0.02289699763059616; train accuracy: 0.9871247790396308; test accuracy: 0.9925000071525574; Time: 53.19705057144165\n",
            "3200: train loss: 0.04823096540817181; test loss: 0.030652258545160294; train accuracy: 0.9854657658594007; test accuracy: 0.9902999997138977; Time: 54.59032654762268\n",
            "3300: train loss: 0.04441368155494329; test loss: 0.032444972544908524; train accuracy: 0.9863338227758937; test accuracy: 0.9894000291824341; Time: 55.97146034240723\n",
            "3400: train loss: 0.044183555091123955; test loss: 0.03588062897324562; train accuracy: 0.9864830229738593; test accuracy: 0.9908999800682068; Time: 57.366649866104126\n",
            "3500: train loss: 0.0457916625139326; test loss: 0.03290466219186783; train accuracy: 0.9856967613058827; test accuracy: 0.9900000095367432; Time: 58.76661920547485\n",
            "3600: train loss: 0.045191285600388424; test loss: 0.02688826061785221; train accuracy: 0.9854845862303223; test accuracy: 0.991599977016449; Time: 60.164703369140625\n",
            "3700: train loss: 0.04243163286186125; test loss: 0.024336639791727066; train accuracy: 0.9868972593244588; test accuracy: 0.9923999905586243; Time: 61.55614423751831\n",
            "3800: train loss: 0.041182244959413944; test loss: 0.02324528433382511; train accuracy: 0.9877625363806097; test accuracy: 0.9923999905586243; Time: 62.947853565216064\n",
            "3900: train loss: 0.039844658284216114; test loss: 0.028262801468372345; train accuracy: 0.9877353715066144; test accuracy: 0.991100013256073; Time: 64.33237099647522\n",
            "4000: train loss: 0.040116169105006816; test loss: 0.03146427124738693; train accuracy: 0.9880792236020898; test accuracy: 0.9911999702453613; Time: 65.7226459980011\n",
            "4100: train loss: 0.03655033152451708; test loss: 0.027984967455267906; train accuracy: 0.9886586228248704; test accuracy: 0.991599977016449; Time: 67.11661791801453\n",
            "4200: train loss: 0.03909294038493656; test loss: 0.035487737506628036; train accuracy: 0.9880399898553618; test accuracy: 0.9900000095367432; Time: 68.50525641441345\n",
            "4300: train loss: 0.04076673375167377; test loss: 0.02728402055799961; train accuracy: 0.9879163473561459; test accuracy: 0.989799976348877; Time: 69.89873957633972\n",
            "4400: train loss: 0.038763129711889485; test loss: 0.03160550817847252; train accuracy: 0.9883684996541718; test accuracy: 0.9908000230789185; Time: 71.28952288627625\n",
            "4500: train loss: 0.038360228266332674; test loss: 0.024654144421219826; train accuracy: 0.9883818783253349; test accuracy: 0.9922000169754028; Time: 72.67967915534973\n",
            "4600: train loss: 0.03671918672970011; test loss: 0.028246158733963966; train accuracy: 0.9883421628544429; test accuracy: 0.9909999966621399; Time: 74.07526588439941\n",
            "4700: train loss: 0.0338970688022541; test loss: 0.02938421256840229; train accuracy: 0.9886875249334419; test accuracy: 0.9915000200271606; Time: 75.4696319103241\n",
            "4800: train loss: 0.03571171886574355; test loss: 0.024358030408620834; train accuracy: 0.9888596351937275; test accuracy: 0.9908999800682068; Time: 76.85738253593445\n",
            "4900: train loss: 0.03472561655247234; test loss: 0.03244180977344513; train accuracy: 0.9889028194330277; test accuracy: 0.9902999997138977; Time: 78.2505750656128\n",
            "5000: train loss: 0.03701306557797393; test loss: 0.029132599011063576; train accuracy: 0.9889963805848977; test accuracy: 0.9919000267982483; Time: 79.64836144447327\n",
            "5100: train loss: 0.035010208868184144; test loss: 0.031782008707523346; train accuracy: 0.9895270873848944; test accuracy: 0.9904000163078308; Time: 81.04117608070374\n",
            "5200: train loss: 0.037063354180201284; test loss: 0.03538256138563156; train accuracy: 0.9885309520076461; test accuracy: 0.9889000058174133; Time: 82.43209266662598\n",
            "5300: train loss: 0.0348546626978032; test loss: 0.034434378147125244; train accuracy: 0.9892336688737331; test accuracy: 0.9907000064849854; Time: 83.8210084438324\n",
            "5400: train loss: 0.03323044545969261; test loss: 0.023415349423885345; train accuracy: 0.9899393380554815; test accuracy: 0.992900013923645; Time: 85.21208477020264\n",
            "5500: train loss: 0.03197875494663055; test loss: 0.02863984927535057; train accuracy: 0.9897587445473964; test accuracy: 0.9907000064849854; Time: 86.60319662094116\n",
            "5600: train loss: 0.032987400124781414; test loss: 0.02332158200442791; train accuracy: 0.9896958114957392; test accuracy: 0.9923999905586243; Time: 87.99681448936462\n",
            "5700: train loss: 0.03148741884650208; test loss: 0.03407256677746773; train accuracy: 0.9903021153328633; test accuracy: 0.9900000095367432; Time: 89.38521647453308\n",
            "5800: train loss: 0.03119668792189111; test loss: 0.02806496061384678; train accuracy: 0.9901652525272003; test accuracy: 0.9914000034332275; Time: 90.77329516410828\n",
            "5900: train loss: 0.03260833585185267; test loss: 0.021973080933094025; train accuracy: 0.9895629048278574; test accuracy: 0.993399977684021; Time: 92.17469453811646\n",
            "6000: train loss: 0.03279121333350532; test loss: 0.02707178331911564; train accuracy: 0.9892016849000212; test accuracy: 0.9919999837875366; Time: 93.56966423988342\n",
            "6100: train loss: 0.03276097925743998; test loss: 0.02736201137304306; train accuracy: 0.9893033509292167; test accuracy: 0.9914000034332275; Time: 94.95780444145203\n",
            "6200: train loss: 0.034707340291718874; test loss: 0.024701563641428947; train accuracy: 0.9898232959565422; test accuracy: 0.9921000003814697; Time: 96.34461617469788\n",
            "6300: train loss: 0.02724403815813869; test loss: 0.025094278156757355; train accuracy: 0.9917364139032281; test accuracy: 0.991599977016449; Time: 97.73486852645874\n",
            "6400: train loss: 0.029977299623114247; test loss: 0.027774427086114883; train accuracy: 0.9911469651266882; test accuracy: 0.9902999997138977; Time: 99.12954187393188\n",
            "6500: train loss: 0.029207795705369195; test loss: 0.026079097762703896; train accuracy: 0.9909809762082689; test accuracy: 0.9921000003814697; Time: 100.51852416992188\n",
            "6600: train loss: 0.030195631798787367; test loss: 0.024595016613602638; train accuracy: 0.9901373191300392; test accuracy: 0.9918000102043152; Time: 101.9142074584961\n",
            "6700: train loss: 0.029688991434006287; test loss: 0.025989996269345284; train accuracy: 0.990278060086165; test accuracy: 0.9916999936103821; Time: 103.30603098869324\n",
            "6800: train loss: 0.030487290611042665; test loss: 0.027796676382422447; train accuracy: 0.990020988301235; test accuracy: 0.9916999936103821; Time: 104.70155930519104\n",
            "6900: train loss: 0.028132329883639843; test loss: 0.03175555169582367; train accuracy: 0.9912327363123101; test accuracy: 0.9905999898910522; Time: 106.09559965133667\n",
            "7000: train loss: 0.02896801586102526; test loss: 0.026503683999180794; train accuracy: 0.9907246149031772; test accuracy: 0.9908999800682068; Time: 107.48656749725342\n",
            "7100: train loss: 0.02858090962183099; test loss: 0.025973523035645485; train accuracy: 0.9906795844060987; test accuracy: 0.9919999837875366; Time: 108.8790123462677\n",
            "7200: train loss: 0.03319546987596742; test loss: 0.027645865455269814; train accuracy: 0.9902260804979888; test accuracy: 0.9922999739646912; Time: 110.26660251617432\n",
            "7300: train loss: 0.03394806334471979; test loss: 0.03831225633621216; train accuracy: 0.9889678690889682; test accuracy: 0.9896000027656555; Time: 111.66418313980103\n",
            "7400: train loss: 0.02965293470996532; test loss: 0.022494353353977203; train accuracy: 0.990794684363299; test accuracy: 0.992900013923645; Time: 113.05434727668762\n",
            "7500: train loss: 0.032512071029573264; test loss: 0.023178135976195335; train accuracy: 0.9906207630258799; test accuracy: 0.9932000041007996; Time: 114.44578266143799\n",
            "7600: train loss: 0.03417616735337477; test loss: 0.02969948947429657; train accuracy: 0.9901039462133174; test accuracy: 0.9914000034332275; Time: 115.84071040153503\n",
            "7700: train loss: 0.028513701291686566; test loss: 0.025768466293811798; train accuracy: 0.9914314374298943; test accuracy: 0.9926000237464905; Time: 117.23390364646912\n",
            "7800: train loss: 0.030133183633722055; test loss: 0.028845971450209618; train accuracy: 0.9910124176690192; test accuracy: 0.9909999966621399; Time: 118.62786817550659\n",
            "7900: train loss: 0.03394758912844161; test loss: 0.02574554830789566; train accuracy: 0.9894831406128656; test accuracy: 0.9912999868392944; Time: 120.02400279045105\n",
            "8000: train loss: 0.02842564638113761; test loss: 0.02784447930753231; train accuracy: 0.9909644476005433; test accuracy: 0.9916999936103821; Time: 121.42030835151672\n",
            "8100: train loss: 0.02877846958340204; test loss: 0.028720075264573097; train accuracy: 0.9911492091519939; test accuracy: 0.9925000071525574; Time: 122.8200957775116\n",
            "8200: train loss: 0.02666533122083134; test loss: 0.02442505769431591; train accuracy: 0.9918144625078527; test accuracy: 0.9929999709129333; Time: 124.21181225776672\n",
            "8300: train loss: 0.025233411953845902; test loss: 0.02629508078098297; train accuracy: 0.9924511171008894; test accuracy: 0.9916999936103821; Time: 125.61194062232971\n",
            "8400: train loss: 0.026074279675389746; test loss: 0.022066211327910423; train accuracy: 0.9920547705608362; test accuracy: 0.992900013923645; Time: 127.00465822219849\n",
            "8500: train loss: 0.024036999813131836; test loss: 0.022744692862033844; train accuracy: 0.9923556194602526; test accuracy: 0.9936000108718872; Time: 128.3970091342926\n",
            "8600: train loss: 0.024161949042218894; test loss: 0.031973447650671005; train accuracy: 0.9929466952072893; test accuracy: 0.991100013256073; Time: 129.79406070709229\n",
            "8700: train loss: 0.026911523241954394; test loss: 0.023175667971372604; train accuracy: 0.9920817874633406; test accuracy: 0.9932000041007996; Time: 131.18600392341614\n",
            "8800: train loss: 0.02884780269366554; test loss: 0.02625679410994053; train accuracy: 0.9915813884570316; test accuracy: 0.9926999807357788; Time: 132.57856035232544\n",
            "8900: train loss: 0.027146196831722296; test loss: 0.04230775311589241; train accuracy: 0.9916845267826572; test accuracy: 0.9871000051498413; Time: 133.9747965335846\n",
            "9000: train loss: 0.02816531358412582; test loss: 0.021423883736133575; train accuracy: 0.9919344833353436; test accuracy: 0.9933000206947327; Time: 135.364342212677\n",
            "9100: train loss: 0.0260503847332951; test loss: 0.026307882741093636; train accuracy: 0.9924818281600958; test accuracy: 0.9926999807357788; Time: 136.75714826583862\n",
            "9200: train loss: 0.02686866203411044; test loss: 0.025944553315639496; train accuracy: 0.990953951429322; test accuracy: 0.9929999709129333; Time: 138.15176963806152\n",
            "9300: train loss: 0.028049727687003697; test loss: 0.024685492739081383; train accuracy: 0.9904541200309348; test accuracy: 0.9936000108718872; Time: 139.54672193527222\n",
            "9400: train loss: 0.02839600562826391; test loss: 0.0264279842376709; train accuracy: 0.9904482556794074; test accuracy: 0.9929999709129333; Time: 140.92046761512756\n",
            "9500: train loss: 0.027896526586643104; test loss: 0.01938178390264511; train accuracy: 0.9905876814515937; test accuracy: 0.9939000010490417; Time: 142.3000090122223\n",
            "9600: train loss: 0.026071938647411187; test loss: 0.025079727172851562; train accuracy: 0.9914793161092477; test accuracy: 0.9927999973297119; Time: 143.69745302200317\n",
            "9700: train loss: 0.024336332235548465; test loss: 0.03390929102897644; train accuracy: 0.991769936713069; test accuracy: 0.9904999732971191; Time: 145.09085488319397\n",
            "9800: train loss: 0.022970202736137348; test loss: 0.030295269563794136; train accuracy: 0.9923896044429816; test accuracy: 0.9908000230789185; Time: 146.48530530929565\n",
            "9900: train loss: 0.02461264894967943; test loss: 0.02349330484867096; train accuracy: 0.9927519389252196; test accuracy: 0.9932000041007996; Time: 147.8758523464203\n",
            "[[ 977    0    1    0    0    0    0    1    1    0]\n",
            " [   0 1128    3    0    0    0    1    3    0    0]\n",
            " [   1    1 1027    0    0    0    0    1    2    0]\n",
            " [   0    0    2 1001    0    6    0    0    1    0]\n",
            " [   0    0    1    0  972    0    1    2    2    4]\n",
            " [   0    0    0    3    0  888    1    0    0    0]\n",
            " [   3    1    0    0    2    1  950    0    1    0]\n",
            " [   0    0    4    0    0    0    0 1019    2    3]\n",
            " [   0    0    2    1    0    1    0    0  970    0]\n",
            " [   1    0    2    0    4    1    0    1    6  994]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9949    0.9969    0.9959       980\n",
            "           1     0.9982    0.9938    0.9960      1135\n",
            "           2     0.9856    0.9952    0.9904      1032\n",
            "           3     0.9960    0.9911    0.9935      1010\n",
            "           4     0.9939    0.9898    0.9918       982\n",
            "           5     0.9900    0.9955    0.9927       892\n",
            "           6     0.9969    0.9916    0.9942       958\n",
            "           7     0.9922    0.9912    0.9917      1028\n",
            "           8     0.9848    0.9959    0.9903       974\n",
            "           9     0.9930    0.9851    0.9891      1009\n",
            "\n",
            "    accuracy                         0.9926     10000\n",
            "   macro avg     0.9925    0.9926    0.9926     10000\n",
            "weighted avg     0.9926    0.9926    0.9926     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OYZ7Bil4PML",
        "colab_type": "text"
      },
      "source": [
        "##Reverse SCAW "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb4P2xjP4WsS",
        "colab_type": "code",
        "outputId": "ed6dfcba-a3dd-4b4a-c791-8d34ab9cb445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Reverse SCAW Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "    \n",
        "    qs_left =  [tf.Variable(tf.ones((row,1), dtype=dtype), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.eye(col, dtype=dtype), trainable=False) for col in cols]\n",
        "\n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_reverse_scaw(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_reverse_scaw(ql, qr, dw, dx) for (ql, qr, dw, dx) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(ql, new_q[0]), tf.assign(qr, new_q[1])] for (ql, qr, new_q) in zip(qs_left, qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'reverse_scaw_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.39605712890625; test loss: 2.278923273086548; train accuracy: 0.0703125; test accuracy: 0.13449999690055847; Time: 10.48609709739685\n",
            "100: train loss: 0.9916962894797321; test loss: 0.2340300977230072; train accuracy: 0.6934687500000001; test accuracy: 0.9384999871253967; Time: 11.836423397064209\n",
            "200: train loss: 0.5527473058086424; test loss: 0.10474731773138046; train accuracy: 0.8290452164736322; test accuracy: 0.972100019454956; Time: 13.162334680557251\n",
            "300: train loss: 0.31719461523478604; test loss: 0.08558320254087448; train accuracy: 0.9029190762179533; test accuracy: 0.9742000102996826; Time: 14.490496397018433\n",
            "400: train loss: 0.21467854254808244; test loss: 0.07513278722763062; train accuracy: 0.933296175570173; test accuracy: 0.9771000146865845; Time: 15.817685842514038\n",
            "500: train loss: 0.15205770964737178; test loss: 0.051178913563489914; train accuracy: 0.9519744119199359; test accuracy: 0.9843999743461609; Time: 17.143138885498047\n",
            "600: train loss: 0.1251543580970792; test loss: 0.051111750304698944; train accuracy: 0.9612151295866174; test accuracy: 0.9847000241279602; Time: 18.466296434402466\n",
            "700: train loss: 0.09823687808647104; test loss: 0.04583456367254257; train accuracy: 0.9693542739346115; test accuracy: 0.9869999885559082; Time: 19.790683031082153\n",
            "800: train loss: 0.0932505284960355; test loss: 0.04727921262383461; train accuracy: 0.9713049574892798; test accuracy: 0.984499990940094; Time: 21.111132383346558\n",
            "900: train loss: 0.08795637533035044; test loss: 0.0413367860019207; train accuracy: 0.9734311586429313; test accuracy: 0.9872000217437744; Time: 22.432074546813965\n",
            "1000: train loss: 0.07908923257160831; test loss: 0.043764978647232056; train accuracy: 0.9752819008426454; test accuracy: 0.9851999878883362; Time: 23.761926889419556\n",
            "1100: train loss: 0.07521427350972416; test loss: 0.03801088407635689; train accuracy: 0.9767196391506592; test accuracy: 0.9871000051498413; Time: 25.091712951660156\n",
            "1200: train loss: 0.07012938577096452; test loss: 0.032093871384859085; train accuracy: 0.9780808783479434; test accuracy: 0.9886000156402588; Time: 26.41638946533203\n",
            "1300: train loss: 0.06868058601066632; test loss: 0.05587571859359741; train accuracy: 0.9778532156835489; test accuracy: 0.9828000068664551; Time: 27.74491596221924\n",
            "1400: train loss: 0.06373978960488678; test loss: 0.03478523716330528; train accuracy: 0.9797013517438625; test accuracy: 0.9876999855041504; Time: 29.06453776359558\n",
            "1500: train loss: 0.057403391412539174; test loss: 0.030524341389536858; train accuracy: 0.9822059774453117; test accuracy: 0.9902999997138977; Time: 30.395501852035522\n",
            "1600: train loss: 0.06028561241906198; test loss: 0.032133765518665314; train accuracy: 0.9819409180145553; test accuracy: 0.989300012588501; Time: 31.72115159034729\n",
            "1700: train loss: 0.05530307217671205; test loss: 0.0321231372654438; train accuracy: 0.9829840285932503; test accuracy: 0.9897000193595886; Time: 33.02516555786133\n",
            "1800: train loss: 0.053422534814616354; test loss: 0.03077290952205658; train accuracy: 0.9834169198047296; test accuracy: 0.989300012588501; Time: 34.335890769958496\n",
            "1900: train loss: 0.0531340563322496; test loss: 0.05221811681985855; train accuracy: 0.9835243737345204; test accuracy: 0.9833999872207642; Time: 35.67218518257141\n",
            "2000: train loss: 0.05294287402510492; test loss: 0.02875489741563797; train accuracy: 0.9826801494322771; test accuracy: 0.9896000027656555; Time: 37.0018253326416\n",
            "2100: train loss: 0.05633756205351713; test loss: 0.027648108080029488; train accuracy: 0.9820256255316259; test accuracy: 0.9908999800682068; Time: 38.33641290664673\n",
            "2200: train loss: 0.04936035559229251; test loss: 0.0657278373837471; train accuracy: 0.9847909887810198; test accuracy: 0.9789999723434448; Time: 39.67132759094238\n",
            "2300: train loss: 0.048140529171834756; test loss: 0.027600310742855072; train accuracy: 0.9847070188672034; test accuracy: 0.9921000003814697; Time: 40.99416732788086\n",
            "2400: train loss: 0.04933803513134827; test loss: 0.031569793820381165; train accuracy: 0.985449825601279; test accuracy: 0.9884999990463257; Time: 42.3068528175354\n",
            "2500: train loss: 0.0450400920204908; test loss: 0.028382757678627968; train accuracy: 0.9862858040838084; test accuracy: 0.9908000230789185; Time: 43.64591574668884\n",
            "2600: train loss: 0.0415328867851778; test loss: 0.024484414607286453; train accuracy: 0.9867725611414919; test accuracy: 0.9922999739646912; Time: 44.98158383369446\n",
            "2700: train loss: 0.04565736716641786; test loss: 0.027947906404733658; train accuracy: 0.9855549343990196; test accuracy: 0.991100013256073; Time: 46.30616092681885\n",
            "2800: train loss: 0.044625795603696124; test loss: 0.02518353797495365; train accuracy: 0.9860540492906595; test accuracy: 0.9919999837875366; Time: 47.63813042640686\n",
            "2900: train loss: 0.04340008659983212; test loss: 0.040131617337465286; train accuracy: 0.9863397362572968; test accuracy: 0.9883999824523926; Time: 48.96323323249817\n",
            "3000: train loss: 0.04348555390794212; test loss: 0.022791648283600807; train accuracy: 0.9867646104023419; test accuracy: 0.9933000206947327; Time: 50.28870177268982\n",
            "3100: train loss: 0.03783723976591909; test loss: 0.03855712711811066; train accuracy: 0.9881522760006707; test accuracy: 0.987500011920929; Time: 51.61413836479187\n",
            "3200: train loss: 0.04096575173393293; test loss: 0.030725372955203056; train accuracy: 0.9866951587840789; test accuracy: 0.9901999831199646; Time: 52.94849181175232\n",
            "3300: train loss: 0.03869549303275545; test loss: 0.032896488904953; train accuracy: 0.987614076088764; test accuracy: 0.989300012588501; Time: 54.27921223640442\n",
            "3400: train loss: 0.03973930984466174; test loss: 0.02550056390464306; train accuracy: 0.9873064568530244; test accuracy: 0.9911999702453613; Time: 55.61301636695862\n",
            "3500: train loss: 0.03952194577227778; test loss: 0.03405391052365303; train accuracy: 0.9872734332100712; test accuracy: 0.9905999898910522; Time: 56.942338943481445\n",
            "3600: train loss: 0.03675930772104699; test loss: 0.027105847373604774; train accuracy: 0.9888443132032229; test accuracy: 0.991599977016449; Time: 58.27322220802307\n",
            "3700: train loss: 0.03699775763236032; test loss: 0.027197739109396935; train accuracy: 0.988196431401036; test accuracy: 0.9908000230789185; Time: 59.59476161003113\n",
            "3800: train loss: 0.03624986994679062; test loss: 0.029084280133247375; train accuracy: 0.9888766313577938; test accuracy: 0.9905999898910522; Time: 60.92206931114197\n",
            "3900: train loss: 0.039814100914448855; test loss: 0.026151485741138458; train accuracy: 0.9882081761114361; test accuracy: 0.9911999702453613; Time: 62.25533080101013\n",
            "4000: train loss: 0.03567572426775196; test loss: 0.057125549763441086; train accuracy: 0.9891205897538028; test accuracy: 0.9818000197410583; Time: 63.57566595077515\n",
            "4100: train loss: 0.03776807040584222; test loss: 0.022240575402975082; train accuracy: 0.988455495796976; test accuracy: 0.9930999875068665; Time: 64.90827751159668\n",
            "4200: train loss: 0.031146148526453355; test loss: 0.03666228801012039; train accuracy: 0.9899607699547099; test accuracy: 0.9876000285148621; Time: 66.23687601089478\n",
            "4300: train loss: 0.03911147397113285; test loss: 0.024875430390238762; train accuracy: 0.9875592759423831; test accuracy: 0.9915000200271606; Time: 67.56737756729126\n",
            "4400: train loss: 0.03311322534152906; test loss: 0.05200052633881569; train accuracy: 0.9887708405483359; test accuracy: 0.9836999773979187; Time: 68.88673281669617\n",
            "4500: train loss: 0.03866000489617408; test loss: 0.032661158591508865; train accuracy: 0.9878875564322558; test accuracy: 0.9898999929428101; Time: 70.21195363998413\n",
            "4600: train loss: 0.0387484984617427; test loss: 0.02358618751168251; train accuracy: 0.9882854335984878; test accuracy: 0.9919999837875366; Time: 71.53193521499634\n",
            "4700: train loss: 0.03501165027998535; test loss: 0.02636711485683918; train accuracy: 0.9886767628244328; test accuracy: 0.9922999739646912; Time: 72.85685014724731\n",
            "4800: train loss: 0.03421688479382925; test loss: 0.027894318103790283; train accuracy: 0.9886104106311538; test accuracy: 0.9901000261306763; Time: 74.17961597442627\n",
            "4900: train loss: 0.032326258892290745; test loss: 0.031031258404254913; train accuracy: 0.9889589922615324; test accuracy: 0.9908000230789185; Time: 75.50056552886963\n",
            "5000: train loss: 0.03586127453222905; test loss: 0.0283378716558218; train accuracy: 0.987961493923994; test accuracy: 0.9905999898910522; Time: 76.82255244255066\n",
            "5100: train loss: 0.035276273046426575; test loss: 0.024070821702480316; train accuracy: 0.9887446517682821; test accuracy: 0.9926999807357788; Time: 78.14556694030762\n",
            "5200: train loss: 0.032359908358250485; test loss: 0.028477618470788002; train accuracy: 0.9893908246353842; test accuracy: 0.9919000267982483; Time: 79.4685218334198\n",
            "5300: train loss: 0.034617220176669024; test loss: 0.026878729462623596; train accuracy: 0.9883724061549015; test accuracy: 0.9926000237464905; Time: 80.79388523101807\n",
            "5400: train loss: 0.03639984574918447; test loss: 0.02457466535270214; train accuracy: 0.9884491979680529; test accuracy: 0.9926000237464905; Time: 82.13147020339966\n",
            "5500: train loss: 0.03278949239498029; test loss: 0.04825690761208534; train accuracy: 0.9895005248965061; test accuracy: 0.9858999848365784; Time: 83.46096849441528\n",
            "5600: train loss: 0.03332183326911009; test loss: 0.03051849640905857; train accuracy: 0.989959622974211; test accuracy: 0.9905999898910522; Time: 84.79235053062439\n",
            "5700: train loss: 0.029178528962814317; test loss: 0.03716663271188736; train accuracy: 0.9909967554332159; test accuracy: 0.9882000088691711; Time: 86.12206625938416\n",
            "5800: train loss: 0.030471850969771074; test loss: 0.025325579568743706; train accuracy: 0.9901625468859071; test accuracy: 0.9922999739646912; Time: 87.456378698349\n",
            "5900: train loss: 0.027328161502376908; test loss: 0.025879425927996635; train accuracy: 0.991375366077105; test accuracy: 0.9934999942779541; Time: 88.78223037719727\n",
            "6000: train loss: 0.029056292170966586; test loss: 0.025638436898589134; train accuracy: 0.991174138326399; test accuracy: 0.9916999936103821; Time: 90.10790777206421\n",
            "6100: train loss: 0.02992013606000829; test loss: 0.021246667951345444; train accuracy: 0.991234019794421; test accuracy: 0.9922000169754028; Time: 91.43698859214783\n",
            "6200: train loss: 0.02937932240515463; test loss: 0.024700434878468513; train accuracy: 0.9907700693999129; test accuracy: 0.9915000200271606; Time: 92.75545907020569\n",
            "6300: train loss: 0.02799110151426977; test loss: 0.022904720157384872; train accuracy: 0.9906452328526633; test accuracy: 0.9919999837875366; Time: 94.08107709884644\n",
            "6400: train loss: 0.026362235284547807; test loss: 0.03458600491285324; train accuracy: 0.9911195105766457; test accuracy: 0.9902999997138977; Time: 95.40754246711731\n",
            "6500: train loss: 0.025959371299292558; test loss: 0.024303626269102097; train accuracy: 0.9918968109366318; test accuracy: 0.9922000169754028; Time: 96.73812317848206\n",
            "6600: train loss: 0.027610924632072237; test loss: 0.024196438491344452; train accuracy: 0.991440353664163; test accuracy: 0.9936000108718872; Time: 98.06688022613525\n",
            "6700: train loss: 0.028285208919472202; test loss: 0.020090200006961823; train accuracy: 0.9911924502351542; test accuracy: 0.9936000108718872; Time: 99.39613699913025\n",
            "6800: train loss: 0.0263700029767518; test loss: 0.018378278240561485; train accuracy: 0.9919116621406634; test accuracy: 0.9937999844551086; Time: 100.72494792938232\n",
            "6900: train loss: 0.027473061065978053; test loss: 0.024775618687272072; train accuracy: 0.9910365230827138; test accuracy: 0.9921000003814697; Time: 102.04702734947205\n",
            "7000: train loss: 0.0256133975012329; test loss: 0.041405826807022095; train accuracy: 0.9909718074790517; test accuracy: 0.9884999990463257; Time: 103.3768937587738\n",
            "7100: train loss: 0.02851980216907872; test loss: 0.022917719557881355; train accuracy: 0.9913219736259373; test accuracy: 0.9930999875068665; Time: 104.71082186698914\n",
            "7200: train loss: 0.026325648276637952; test loss: 0.03697371855378151; train accuracy: 0.9917037543302972; test accuracy: 0.9894000291824341; Time: 106.04881763458252\n",
            "7300: train loss: 0.0259991641872636; test loss: 0.029495632275938988; train accuracy: 0.9921606953866628; test accuracy: 0.9914000034332275; Time: 107.37365102767944\n",
            "7400: train loss: 0.025669944619356747; test loss: 0.026594487950205803; train accuracy: 0.9921319325005915; test accuracy: 0.9922999739646912; Time: 108.69271111488342\n",
            "7500: train loss: 0.026310115110975196; test loss: 0.02394961006939411; train accuracy: 0.9920581580475256; test accuracy: 0.9922000169754028; Time: 110.01586699485779\n",
            "7600: train loss: 0.02723977137945745; test loss: 0.023662477731704712; train accuracy: 0.9916859079953483; test accuracy: 0.9925000071525574; Time: 111.3412561416626\n",
            "7700: train loss: 0.027185787199737577; test loss: 0.022026166319847107; train accuracy: 0.9917564175414958; test accuracy: 0.9933000206947327; Time: 112.66084551811218\n",
            "7800: train loss: 0.02813433868743506; test loss: 0.02525397390127182; train accuracy: 0.9907778170113435; test accuracy: 0.9927999973297119; Time: 113.98758816719055\n",
            "7900: train loss: 0.028220018748082228; test loss: 0.023873450234532356; train accuracy: 0.991397437202065; test accuracy: 0.9927999973297119; Time: 115.31158113479614\n",
            "8000: train loss: 0.024704860058882883; test loss: 0.024364804849028587; train accuracy: 0.9918559203371945; test accuracy: 0.992900013923645; Time: 116.63628125190735\n",
            "8100: train loss: 0.028706352508748572; test loss: 0.028589727357029915; train accuracy: 0.9913065580720906; test accuracy: 0.9922000169754028; Time: 117.94256067276001\n",
            "8200: train loss: 0.026698649755167874; test loss: 0.027996454387903214; train accuracy: 0.9918352219692936; test accuracy: 0.9922999739646912; Time: 119.26276659965515\n",
            "8300: train loss: 0.02389859430056907; test loss: 0.03836725279688835; train accuracy: 0.9917824877228202; test accuracy: 0.9886999726295471; Time: 120.60354948043823\n",
            "8400: train loss: 0.02672313567720945; test loss: 0.03155887499451637; train accuracy: 0.9917018779677136; test accuracy: 0.9915000200271606; Time: 121.94153094291687\n",
            "8500: train loss: 0.02346814896681427; test loss: 0.020133361220359802; train accuracy: 0.9926186127033741; test accuracy: 0.9936000108718872; Time: 123.27245855331421\n",
            "8600: train loss: 0.027508863763400587; test loss: 0.023535549640655518; train accuracy: 0.9915541828643031; test accuracy: 0.9934999942779541; Time: 124.59899139404297\n",
            "8700: train loss: 0.024057047590356054; test loss: 0.03410555049777031; train accuracy: 0.9920266319179045; test accuracy: 0.9900000095367432; Time: 125.91862058639526\n",
            "8800: train loss: 0.028420092046496124; test loss: 0.02819950506091118; train accuracy: 0.991188363147279; test accuracy: 0.9926000237464905; Time: 127.22335910797119\n",
            "8900: train loss: 0.02715151252028274; test loss: 0.024750536307692528; train accuracy: 0.9919165591475061; test accuracy: 0.9930999875068665; Time: 128.55598902702332\n",
            "9000: train loss: 0.02438272764692097; test loss: 0.030021753162145615; train accuracy: 0.9923403528920193; test accuracy: 0.9919000267982483; Time: 129.8868510723114\n",
            "9100: train loss: 0.0246245890503738; test loss: 0.026083627715706825; train accuracy: 0.9918510057116914; test accuracy: 0.9929999709129333; Time: 131.2115502357483\n",
            "9200: train loss: 0.02386640987933607; test loss: 0.02632628194987774; train accuracy: 0.9921945497995115; test accuracy: 0.9926000237464905; Time: 132.5326590538025\n",
            "9300: train loss: 0.023240033482255517; test loss: 0.02637694776058197; train accuracy: 0.9930989529363091; test accuracy: 0.992900013923645; Time: 133.85986161231995\n",
            "9400: train loss: 0.025009841750150728; test loss: 0.024382498115301132; train accuracy: 0.9925858467894478; test accuracy: 0.9922999739646912; Time: 135.17797780036926\n",
            "9500: train loss: 0.024200286994629536; test loss: 0.031184548512101173; train accuracy: 0.9922775090067036; test accuracy: 0.9901999831199646; Time: 136.50227332115173\n",
            "9600: train loss: 0.020282845865554323; test loss: 0.030490320175886154; train accuracy: 0.9926665748936678; test accuracy: 0.991100013256073; Time: 137.83150219917297\n",
            "9700: train loss: 0.023270448465597907; test loss: 0.02974773198366165; train accuracy: 0.9922525073684113; test accuracy: 0.9922999739646912; Time: 139.1572289466858\n",
            "9800: train loss: 0.02739174672818329; test loss: 0.02637944370508194; train accuracy: 0.9921469377447326; test accuracy: 0.9921000003814697; Time: 140.48735976219177\n",
            "9900: train loss: 0.02580312973173951; test loss: 0.024539057165384293; train accuracy: 0.99213522846613; test accuracy: 0.9927999973297119; Time: 141.81695461273193\n",
            "[[ 979    0    1    0    0    0    0    0    0    0]\n",
            " [   0 1131    2    1    1    0    0    0    0    0]\n",
            " [   2    1 1028    0    0    0    0    0    1    0]\n",
            " [   0    0    1 1007    0    0    0    1    1    0]\n",
            " [   0    0    0    0  978    0    3    0    0    1]\n",
            " [   1    0    0   12    0  878    1    0    0    0]\n",
            " [   4    1    1    0    0    1  949    0    2    0]\n",
            " [   0    3    7    0    3    0    0 1012    1    2]\n",
            " [   0    0    1    1    0    0    1    0  970    1]\n",
            " [   1    0    0    1    7    1    0    0    6  993]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9919    0.9990    0.9954       980\n",
            "           1     0.9956    0.9965    0.9960      1135\n",
            "           2     0.9875    0.9961    0.9918      1032\n",
            "           3     0.9853    0.9970    0.9911      1010\n",
            "           4     0.9889    0.9959    0.9924       982\n",
            "           5     0.9977    0.9843    0.9910       892\n",
            "           6     0.9948    0.9906    0.9927       958\n",
            "           7     0.9990    0.9844    0.9917      1028\n",
            "           8     0.9888    0.9959    0.9923       974\n",
            "           9     0.9960    0.9841    0.9900      1009\n",
            "\n",
            "    accuracy                         0.9925     10000\n",
            "   macro avg     0.9925    0.9924    0.9924     10000\n",
            "weighted avg     0.9925    0.9925    0.9925     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC15juPW622s",
        "colab_type": "text"
      },
      "source": [
        "## SCAN Pro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4n45V6_65Bl",
        "colab_type": "code",
        "outputId": "54b4a87a-d986-4412-9691-6adc7446869c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with SCAN Pro Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "\n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1, row)), tf.zeros((1,row))], axis=0), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.concat([tf.ones((1, col)), tf.zeros((1,col))], axis=0), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_scan_pro(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_scan_pro(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'scan_pro_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3781280517578125; test loss: 2.2839367389678955; train accuracy: 0.0859375; test accuracy: 0.11969999969005585; Time: 11.206511974334717\n",
            "100: train loss: 1.0221012464970338; test loss: 0.24603205919265747; train accuracy: 0.6810210937499998; test accuracy: 0.9283999800682068; Time: 12.70463752746582\n",
            "200: train loss: 0.5920223488680645; test loss: 0.10427120327949524; train accuracy: 0.81553481076288; test accuracy: 0.9696999788284302; Time: 14.205026865005493\n",
            "300: train loss: 0.35963396321214314; test loss: 0.10865043848752975; train accuracy: 0.8890100693742983; test accuracy: 0.9664000272750854; Time: 15.707662582397461\n",
            "400: train loss: 0.23657467427487283; test loss: 0.07656712830066681; train accuracy: 0.9273975119306718; test accuracy: 0.9753999710083008; Time: 17.185430765151978\n",
            "500: train loss: 0.17364982834987713; test loss: 0.046841174364089966; train accuracy: 0.9446162953189742; test accuracy: 0.9846000075340271; Time: 18.681775331497192\n",
            "600: train loss: 0.13848492444439378; test loss: 0.05442803353071213; train accuracy: 0.9568916965166429; test accuracy: 0.9832000136375427; Time: 20.176980018615723\n",
            "700: train loss: 0.11079209689977437; test loss: 0.04393687844276428; train accuracy: 0.9652745347588413; test accuracy: 0.9850999712944031; Time: 21.675593376159668\n",
            "800: train loss: 0.098632793292164; test loss: 0.05082323029637337; train accuracy: 0.9693068450628295; test accuracy: 0.984000027179718; Time: 23.168513536453247\n",
            "900: train loss: 0.08356770441512854; test loss: 0.0348503403365612; train accuracy: 0.9740585518787458; test accuracy: 0.9876000285148621; Time: 24.66881513595581\n",
            "1000: train loss: 0.08223490204445384; test loss: 0.03352045640349388; train accuracy: 0.9749724022903838; test accuracy: 0.9894999861717224; Time: 26.16936707496643\n",
            "1100: train loss: 0.07672678513471848; test loss: 0.05630958080291748; train accuracy: 0.9769634766679552; test accuracy: 0.9821000099182129; Time: 27.663129568099976\n",
            "1200: train loss: 0.0706162451018332; test loss: 0.035973478108644485; train accuracy: 0.9776626479613778; test accuracy: 0.9886000156402588; Time: 29.164647340774536\n",
            "1300: train loss: 0.0702426239538022; test loss: 0.04270020127296448; train accuracy: 0.979423653754299; test accuracy: 0.9855999946594238; Time: 30.661974906921387\n",
            "1400: train loss: 0.07094581051733234; test loss: 0.03591551631689072; train accuracy: 0.9775271694727338; test accuracy: 0.9879000186920166; Time: 32.153032302856445\n",
            "1500: train loss: 0.06950929555193697; test loss: 0.0313907116651535; train accuracy: 0.9786112796084809; test accuracy: 0.9901999831199646; Time: 33.65515851974487\n",
            "1600: train loss: 0.0645980833192623; test loss: 0.03342948853969574; train accuracy: 0.9798413954246292; test accuracy: 0.9879999756813049; Time: 35.155245542526245\n",
            "1700: train loss: 0.06469930442945408; test loss: 0.036260124295949936; train accuracy: 0.9802870947102724; test accuracy: 0.9879999756813049; Time: 36.65198612213135\n",
            "1800: train loss: 0.05820799846134264; test loss: 0.028713446110486984; train accuracy: 0.9819512723056072; test accuracy: 0.9904000163078308; Time: 38.15557932853699\n",
            "1900: train loss: 0.060858318202440614; test loss: 0.031211407855153084; train accuracy: 0.9810731618831979; test accuracy: 0.989300012588501; Time: 39.659324169158936\n",
            "2000: train loss: 0.05359158637716339; test loss: 0.02919825352728367; train accuracy: 0.9829617643711824; test accuracy: 0.9900000095367432; Time: 41.16860342025757\n",
            "2100: train loss: 0.05576101433279426; test loss: 0.04503951594233513; train accuracy: 0.9831469453523962; test accuracy: 0.9833999872207642; Time: 42.66555881500244\n",
            "2200: train loss: 0.05527055519216336; test loss: 0.03434240072965622; train accuracy: 0.9829232118348821; test accuracy: 0.9890000224113464; Time: 44.158772468566895\n",
            "2300: train loss: 0.0536693052183846; test loss: 0.057131700217723846; train accuracy: 0.9839417967791082; test accuracy: 0.9828000068664551; Time: 45.65655255317688\n",
            "2400: train loss: 0.0496571272686002; test loss: 0.025266781449317932; train accuracy: 0.9846330736761109; test accuracy: 0.9907000064849854; Time: 47.156014919281006\n",
            "2500: train loss: 0.05290080891601553; test loss: 0.02676791325211525; train accuracy: 0.9844557856463133; test accuracy: 0.9904000163078308; Time: 48.65851187705994\n",
            "2600: train loss: 0.054232552634349604; test loss: 0.02903785929083824; train accuracy: 0.984091771380088; test accuracy: 0.9907000064849854; Time: 50.15438151359558\n",
            "2700: train loss: 0.04468660984669448; test loss: 0.024040646851062775; train accuracy: 0.9868282901860673; test accuracy: 0.9922000169754028; Time: 51.651118993759155\n",
            "2800: train loss: 0.045586656504502; test loss: 0.03585914149880409; train accuracy: 0.9851762541804648; test accuracy: 0.9886000156402588; Time: 53.14729452133179\n",
            "2900: train loss: 0.03915647856202331; test loss: 0.030263757333159447; train accuracy: 0.9868713312605093; test accuracy: 0.9904999732971191; Time: 54.64153504371643\n",
            "3000: train loss: 0.042621745621820174; test loss: 0.03875679895281792; train accuracy: 0.9871327195489408; test accuracy: 0.9872999787330627; Time: 56.14028334617615\n",
            "3100: train loss: 0.047081084638404616; test loss: 0.02609901688992977; train accuracy: 0.9858853002645459; test accuracy: 0.9914000034332275; Time: 57.63494372367859\n",
            "3200: train loss: 0.04546515978115243; test loss: 0.03092821128666401; train accuracy: 0.9861051523786751; test accuracy: 0.9901000261306763; Time: 59.14083385467529\n",
            "3300: train loss: 0.04648176368753051; test loss: 0.022924646735191345; train accuracy: 0.985299693765288; test accuracy: 0.9922000169754028; Time: 60.63076639175415\n",
            "3400: train loss: 0.04453175861454488; test loss: 0.030150780454277992; train accuracy: 0.9853478516270925; test accuracy: 0.9901999831199646; Time: 62.13180088996887\n",
            "3500: train loss: 0.04146358480025996; test loss: 0.025732871145009995; train accuracy: 0.9864652466896656; test accuracy: 0.9918000102043152; Time: 63.634154319763184\n",
            "3600: train loss: 0.040045717344227565; test loss: 0.027963504195213318; train accuracy: 0.9871853920283273; test accuracy: 0.9905999898910522; Time: 65.13839411735535\n",
            "3700: train loss: 0.04031077544879862; test loss: 0.0296055655926466; train accuracy: 0.9863579385442096; test accuracy: 0.9908999800682068; Time: 66.6329197883606\n",
            "3800: train loss: 0.03524989067766413; test loss: 0.03126895800232887; train accuracy: 0.9880163168876052; test accuracy: 0.989799976348877; Time: 68.13223648071289\n",
            "3900: train loss: 0.037470995387483705; test loss: 0.028815625235438347; train accuracy: 0.9881211380422822; test accuracy: 0.991599977016449; Time: 69.63161611557007\n",
            "4000: train loss: 0.03521966873877864; test loss: 0.031351488083601; train accuracy: 0.9889288645671358; test accuracy: 0.9898999929428101; Time: 71.13396382331848\n",
            "4100: train loss: 0.03741266751999319; test loss: 0.0367552675306797; train accuracy: 0.9886449664410644; test accuracy: 0.9887999892234802; Time: 72.6325888633728\n",
            "4200: train loss: 0.03434359594962812; test loss: 0.025149930268526077; train accuracy: 0.9892841601422369; test accuracy: 0.9909999966621399; Time: 74.13017010688782\n",
            "4300: train loss: 0.03750373367846779; test loss: 0.024227509275078773; train accuracy: 0.9885191875653141; test accuracy: 0.9919999837875366; Time: 75.6259868144989\n",
            "4400: train loss: 0.03751175509151189; test loss: 0.025068463757634163; train accuracy: 0.9884189786906821; test accuracy: 0.9908000230789185; Time: 77.12293314933777\n",
            "4500: train loss: 0.0386651632518347; test loss: 0.038259197026491165; train accuracy: 0.9893315756355061; test accuracy: 0.9894999861717224; Time: 78.61979246139526\n",
            "4600: train loss: 0.03558360827097539; test loss: 0.039404869079589844; train accuracy: 0.9893209430015222; test accuracy: 0.9879999756813049; Time: 80.11756801605225\n",
            "4700: train loss: 0.03521151615990629; test loss: 0.02109927125275135; train accuracy: 0.9891612277315737; test accuracy: 0.9932000041007996; Time: 81.62233185768127\n",
            "4800: train loss: 0.03554870687633821; test loss: 0.02435401827096939; train accuracy: 0.9888096033655556; test accuracy: 0.9922000169754028; Time: 83.12459969520569\n",
            "4900: train loss: 0.03322575111072965; test loss: 0.028869567438960075; train accuracy: 0.9898638874957185; test accuracy: 0.9904000163078308; Time: 84.62488222122192\n",
            "5000: train loss: 0.034618754231828495; test loss: 0.029724929481744766; train accuracy: 0.9893003200689506; test accuracy: 0.989799976348877; Time: 86.12280631065369\n",
            "5100: train loss: 0.03443486847450468; test loss: 0.02242080122232437; train accuracy: 0.9889014070600872; test accuracy: 0.9916999936103821; Time: 87.62410378456116\n",
            "5200: train loss: 0.030947588234551265; test loss: 0.04403045028448105; train accuracy: 0.9895897560014438; test accuracy: 0.9868999719619751; Time: 89.1268949508667\n",
            "5300: train loss: 0.029225362984124984; test loss: 0.02536637894809246; train accuracy: 0.9900739547887442; test accuracy: 0.9915000200271606; Time: 90.63287687301636\n",
            "5400: train loss: 0.030636741077133822; test loss: 0.026952175423502922; train accuracy: 0.9904081679982574; test accuracy: 0.9911999702453613; Time: 92.12852430343628\n",
            "5500: train loss: 0.03192323963477653; test loss: 0.02434224635362625; train accuracy: 0.9899014578362381; test accuracy: 0.9914000034332275; Time: 93.61548900604248\n",
            "5600: train loss: 0.030472051209992565; test loss: 0.02179136872291565; train accuracy: 0.9902851408761052; test accuracy: 0.9930999875068665; Time: 95.10496640205383\n",
            "5700: train loss: 0.030961917603653986; test loss: 0.030687395483255386; train accuracy: 0.9904776686866567; test accuracy: 0.989799976348877; Time: 96.6098301410675\n",
            "5800: train loss: 0.030636613614225528; test loss: 0.03570900112390518; train accuracy: 0.9903784639199995; test accuracy: 0.9890999794006348; Time: 98.11025094985962\n",
            "5900: train loss: 0.030246748873182554; test loss: 0.02262520045042038; train accuracy: 0.9896287465243535; test accuracy: 0.9926000237464905; Time: 99.60523223876953\n",
            "6000: train loss: 0.029647743717439064; test loss: 0.024969635531306267; train accuracy: 0.9908405807680983; test accuracy: 0.9918000102043152; Time: 101.10223650932312\n",
            "6100: train loss: 0.03161362100156077; test loss: 0.02485041692852974; train accuracy: 0.9898990228688832; test accuracy: 0.9922999739646912; Time: 102.58553862571716\n",
            "6200: train loss: 0.02953697524374599; test loss: 0.023044176399707794; train accuracy: 0.9895803622988449; test accuracy: 0.992900013923645; Time: 104.08025193214417\n",
            "6300: train loss: 0.02806301061901343; test loss: 0.027787262573838234; train accuracy: 0.9908943313341927; test accuracy: 0.9912999868392944; Time: 105.58138275146484\n",
            "6400: train loss: 0.03185499447831285; test loss: 0.042503464967012405; train accuracy: 0.9901467191997837; test accuracy: 0.9879999756813049; Time: 107.08615708351135\n",
            "6500: train loss: 0.02982603299081531; test loss: 0.02712213434278965; train accuracy: 0.9910429028180137; test accuracy: 0.9919000267982483; Time: 108.58751249313354\n",
            "6600: train loss: 0.03145306447352661; test loss: 0.024769095703959465; train accuracy: 0.9902498317063355; test accuracy: 0.9927999973297119; Time: 110.08838272094727\n",
            "6700: train loss: 0.027306969199757593; test loss: 0.02521469257771969; train accuracy: 0.9914939537134562; test accuracy: 0.9919000267982483; Time: 111.5812578201294\n",
            "6800: train loss: 0.026496023589302618; test loss: 0.02182694710791111; train accuracy: 0.9916684232017033; test accuracy: 0.9926999807357788; Time: 113.07211208343506\n",
            "6900: train loss: 0.027749446330394827; test loss: 0.027060803025960922; train accuracy: 0.9909313067451702; test accuracy: 0.9916999936103821; Time: 114.57119297981262\n",
            "7000: train loss: 0.026265776129112725; test loss: 0.02125389315187931; train accuracy: 0.9914672383467934; test accuracy: 0.9925000071525574; Time: 116.06717443466187\n",
            "7100: train loss: 0.028116083075828226; test loss: 0.027517855167388916; train accuracy: 0.991569326494333; test accuracy: 0.9909999966621399; Time: 117.56907677650452\n",
            "7200: train loss: 0.030993461871445496; test loss: 0.025800833478569984; train accuracy: 0.991145505601877; test accuracy: 0.9916999936103821; Time: 119.07126951217651\n",
            "7300: train loss: 0.029511665795958302; test loss: 0.022500474005937576; train accuracy: 0.9911352746074639; test accuracy: 0.9930999875068665; Time: 120.57432842254639\n",
            "7400: train loss: 0.025876964365759026; test loss: 0.027327727526426315; train accuracy: 0.9909494231697921; test accuracy: 0.9918000102043152; Time: 122.07815504074097\n",
            "7500: train loss: 0.027808431344402453; test loss: 0.03319401293992996; train accuracy: 0.9908485179533646; test accuracy: 0.9908000230789185; Time: 123.5816400051117\n",
            "7600: train loss: 0.025473059188829864; test loss: 0.025319529697299004; train accuracy: 0.9917250060269572; test accuracy: 0.9933000206947327; Time: 125.06983804702759\n",
            "7700: train loss: 0.023298150353031795; test loss: 0.02895917370915413; train accuracy: 0.9920665742322983; test accuracy: 0.9919999837875366; Time: 126.56799864768982\n",
            "7800: train loss: 0.02420417062388792; test loss: 0.025769585743546486; train accuracy: 0.9919399838634092; test accuracy: 0.9922999739646912; Time: 128.06633496284485\n",
            "7900: train loss: 0.02451792316126895; test loss: 0.027751842513680458; train accuracy: 0.9921837871588575; test accuracy: 0.9918000102043152; Time: 129.56642508506775\n",
            "8000: train loss: 0.023745227738920585; test loss: 0.027678674086928368; train accuracy: 0.9925008810967003; test accuracy: 0.9919999837875366; Time: 131.05904817581177\n",
            "8100: train loss: 0.023296033733027945; test loss: 0.027337854728102684; train accuracy: 0.9931931671290676; test accuracy: 0.9922999739646912; Time: 132.5539195537567\n",
            "8200: train loss: 0.026837688869766684; test loss: 0.023792320862412453; train accuracy: 0.9919451306550426; test accuracy: 0.9934999942779541; Time: 134.04725074768066\n",
            "8300: train loss: 0.026246647407610804; test loss: 0.03038361854851246; train accuracy: 0.9919147003951294; test accuracy: 0.9919000267982483; Time: 135.54284071922302\n",
            "8400: train loss: 0.030007597504671614; test loss: 0.021696072071790695; train accuracy: 0.9907895157440608; test accuracy: 0.9940000176429749; Time: 137.04683661460876\n",
            "8500: train loss: 0.02568675521766643; test loss: 0.028123684227466583; train accuracy: 0.9914313968835257; test accuracy: 0.9919000267982483; Time: 138.53989100456238\n",
            "8600: train loss: 0.024063734154427266; test loss: 0.02283487096428871; train accuracy: 0.9919244991491584; test accuracy: 0.9936000108718872; Time: 140.05094647407532\n",
            "8700: train loss: 0.024657373712940644; test loss: 0.03752940520644188; train accuracy: 0.9924325004860045; test accuracy: 0.9889000058174133; Time: 141.5640048980713\n",
            "8800: train loss: 0.026587923773210134; test loss: 0.027175329625606537; train accuracy: 0.9910902253441144; test accuracy: 0.9927999973297119; Time: 143.0889184474945\n",
            "8900: train loss: 0.02518872946773369; test loss: 0.02756311371922493; train accuracy: 0.9918225063518291; test accuracy: 0.991599977016449; Time: 144.6130187511444\n",
            "9000: train loss: 0.026732493686350885; test loss: 0.02873198874294758; train accuracy: 0.9920114760281847; test accuracy: 0.991599977016449; Time: 146.14061045646667\n",
            "9100: train loss: 0.02632945237593542; test loss: 0.021826155483722687; train accuracy: 0.992436282510407; test accuracy: 0.9934999942779541; Time: 147.6613314151764\n",
            "9200: train loss: 0.025769318055708586; test loss: 0.032804880291223526; train accuracy: 0.9920775317126879; test accuracy: 0.9911999702453613; Time: 149.18448734283447\n",
            "9300: train loss: 0.026906174662571797; test loss: 0.03458663821220398; train accuracy: 0.9916237614859115; test accuracy: 0.9896000027656555; Time: 150.70452237129211\n",
            "9400: train loss: 0.0241527662502913; test loss: 0.03594076260924339; train accuracy: 0.9925585205141516; test accuracy: 0.9901999831199646; Time: 152.2181990146637\n",
            "9500: train loss: 0.02548493189886077; test loss: 0.023098839446902275; train accuracy: 0.9918159968996786; test accuracy: 0.9934999942779541; Time: 153.71888613700867\n",
            "9600: train loss: 0.02384842032280637; test loss: 0.024276020005345345; train accuracy: 0.9924268804866455; test accuracy: 0.9925000071525574; Time: 155.2166805267334\n",
            "9700: train loss: 0.02712422007882941; test loss: 0.031955819576978683; train accuracy: 0.9919355072973994; test accuracy: 0.9905999898910522; Time: 156.7183711528778\n",
            "9800: train loss: 0.024805924329846993; test loss: 0.022156234830617905; train accuracy: 0.992219539121786; test accuracy: 0.9926999807357788; Time: 158.2076060771942\n",
            "9900: train loss: 0.02630433219986215; test loss: 0.02521035633981228; train accuracy: 0.9915113997805651; test accuracy: 0.9927999973297119; Time: 159.70520210266113\n",
            "[[ 978    0    0    0    0    1    0    1    0    0]\n",
            " [   0 1121    1    1    4    0    5    3    0    0]\n",
            " [   0    0 1028    0    1    0    0    3    0    0]\n",
            " [   0    0    0 1009    0    1    0    0    0    0]\n",
            " [   0    0    1    0  980    0    0    0    1    0]\n",
            " [   0    0    0    3    0  888    1    0    0    0]\n",
            " [   4    2    1    0    3    3  943    0    2    0]\n",
            " [   0    1    3    0    1    0    0 1021    2    0]\n",
            " [   0    0    3    2    2    1    0    0  963    3]\n",
            " [   0    0    1    1   15    1    0    2    5  984]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9959    0.9980    0.9969       980\n",
            "           1     0.9973    0.9877    0.9925      1135\n",
            "           2     0.9904    0.9961    0.9932      1032\n",
            "           3     0.9931    0.9990    0.9961      1010\n",
            "           4     0.9742    0.9980    0.9859       982\n",
            "           5     0.9922    0.9955    0.9938       892\n",
            "           6     0.9937    0.9843    0.9890       958\n",
            "           7     0.9913    0.9932    0.9922      1028\n",
            "           8     0.9897    0.9887    0.9892       974\n",
            "           9     0.9970    0.9752    0.9860      1009\n",
            "\n",
            "    accuracy                         0.9915     10000\n",
            "   macro avg     0.9915    0.9916    0.9915     10000\n",
            "weighted avg     0.9916    0.9915    0.9915     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFf2zUoo8RL-",
        "colab_type": "text"
      },
      "source": [
        "## Hybrid SKron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3LE2J9e8Vzy",
        "colab_type": "code",
        "outputId": "f3f6935b-5765-429a-8bfe-a24e78ecc037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Hybrid SKron Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "\n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1, row)), tf.zeros((1, row))], axis=0), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.eye(col, dtype=dtype), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_hybrid_skron(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_hybrid_skron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'hybrid_skron_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.4049949645996094; test loss: 2.2869622707366943; train accuracy: 0.078125; test accuracy: 0.10740000009536743; Time: 9.226301193237305\n",
            "100: train loss: 0.9907160532206298; test loss: 0.2323305904865265; train accuracy: 0.6882906249999996; test accuracy: 0.9333000183105469; Time: 10.656553268432617\n",
            "200: train loss: 0.5496377775099641; test loss: 0.15136972069740295; train accuracy: 0.8275272847538304; test accuracy: 0.9538999795913696; Time: 12.089669227600098\n",
            "300: train loss: 0.3100779465309759; test loss: 0.08349881321191788; train accuracy: 0.9034438194162736; test accuracy: 0.9739000201225281; Time: 13.519777059555054\n",
            "400: train loss: 0.19530028085419654; test loss: 0.057364124804735184; train accuracy: 0.9402353645911024; test accuracy: 0.9818000197410583; Time: 14.948010444641113\n",
            "500: train loss: 0.14003243121027822; test loss: 0.06593816727399826; train accuracy: 0.9572900237605413; test accuracy: 0.9782999753952026; Time: 16.378555297851562\n",
            "600: train loss: 0.116064240506455; test loss: 0.04774750769138336; train accuracy: 0.9641026685404724; test accuracy: 0.9843999743461609; Time: 17.809383153915405\n",
            "700: train loss: 0.09666420680676674; test loss: 0.037125129252672195; train accuracy: 0.970115666511; test accuracy: 0.9890999794006348; Time: 19.236513137817383\n",
            "800: train loss: 0.08789345359049029; test loss: 0.03448076546192169; train accuracy: 0.9732320600860537; test accuracy: 0.989300012588501; Time: 20.667521476745605\n",
            "900: train loss: 0.08161447571349563; test loss: 0.0312286838889122; train accuracy: 0.9749511097843259; test accuracy: 0.9897000193595886; Time: 22.09860348701477\n",
            "1000: train loss: 0.07364830571448665; test loss: 0.04353049769997597; train accuracy: 0.9771290266450237; test accuracy: 0.9851999878883362; Time: 23.53564190864563\n",
            "1100: train loss: 0.07237225819398918; test loss: 0.03452567383646965; train accuracy: 0.9770660115066973; test accuracy: 0.9879999756813049; Time: 24.97563362121582\n",
            "1200: train loss: 0.06974070067631445; test loss: 0.03236578404903412; train accuracy: 0.9792626981578739; test accuracy: 0.9889000058174133; Time: 26.414957523345947\n",
            "1300: train loss: 0.06701261619409601; test loss: 0.028133617714047432; train accuracy: 0.9794656964788871; test accuracy: 0.989799976348877; Time: 27.855441570281982\n",
            "1400: train loss: 0.06189097251452059; test loss: 0.04292409494519234; train accuracy: 0.9805910708992754; test accuracy: 0.9855999946594238; Time: 29.292059183120728\n",
            "1500: train loss: 0.06304729669782197; test loss: 0.03270988166332245; train accuracy: 0.9812419721555418; test accuracy: 0.9894000291824341; Time: 30.732602834701538\n",
            "1600: train loss: 0.06042548176957327; test loss: 0.030709054321050644; train accuracy: 0.9816569969884539; test accuracy: 0.9894000291824341; Time: 32.16681742668152\n",
            "1700: train loss: 0.058457285996978914; test loss: 0.027841467410326004; train accuracy: 0.9815236148177744; test accuracy: 0.9907000064849854; Time: 33.60732388496399\n",
            "1800: train loss: 0.05444783370085874; test loss: 0.03052430972456932; train accuracy: 0.9820768000719993; test accuracy: 0.9900000095367432; Time: 35.03647303581238\n",
            "1900: train loss: 0.056704682100226715; test loss: 0.053640659898519516; train accuracy: 0.9817910353003284; test accuracy: 0.9815000295639038; Time: 36.46676468849182\n",
            "2000: train loss: 0.05068413251095221; test loss: 0.03002951666712761; train accuracy: 0.9840607531462622; test accuracy: 0.9897000193595886; Time: 37.89991235733032\n",
            "2100: train loss: 0.04954239191009758; test loss: 0.024721022695302963; train accuracy: 0.985291165078219; test accuracy: 0.9923999905586243; Time: 39.335490226745605\n",
            "2200: train loss: 0.044723586761586616; test loss: 0.032799623906612396; train accuracy: 0.9851545070822422; test accuracy: 0.9889000058174133; Time: 40.77225661277771\n",
            "2300: train loss: 0.043149724319581285; test loss: 0.05579361692070961; train accuracy: 0.985971918853871; test accuracy: 0.9846000075340271; Time: 42.20585751533508\n",
            "2400: train loss: 0.04574195925556053; test loss: 0.023289017379283905; train accuracy: 0.9853971416156543; test accuracy: 0.991599977016449; Time: 43.640146017074585\n",
            "2500: train loss: 0.04853090541141381; test loss: 0.02884552627801895; train accuracy: 0.9847265219822566; test accuracy: 0.9902999997138977; Time: 45.07429075241089\n",
            "2600: train loss: 0.04432476132404259; test loss: 0.02137938141822815; train accuracy: 0.9861438503331749; test accuracy: 0.9926000237464905; Time: 46.5050208568573\n",
            "2700: train loss: 0.04218191714359198; test loss: 0.026617705821990967; train accuracy: 0.9865869712686092; test accuracy: 0.9904999732971191; Time: 47.93914031982422\n",
            "2800: train loss: 0.0425267117421884; test loss: 0.02215791493654251; train accuracy: 0.9863028321142927; test accuracy: 0.9922000169754028; Time: 49.37134909629822\n",
            "2900: train loss: 0.04044060181624172; test loss: 0.028620172291994095; train accuracy: 0.9883870404349175; test accuracy: 0.9908000230789185; Time: 50.80887198448181\n",
            "3000: train loss: 0.04195575453904229; test loss: 0.040540941059589386; train accuracy: 0.9869618902689993; test accuracy: 0.9871000051498413; Time: 52.23729920387268\n",
            "3100: train loss: 0.04544835501194484; test loss: 0.031203964725136757; train accuracy: 0.9861498519153885; test accuracy: 0.9901999831199646; Time: 53.66911005973816\n",
            "3200: train loss: 0.04181412587650386; test loss: 0.022721784189343452; train accuracy: 0.9871089282023099; test accuracy: 0.9921000003814697; Time: 55.101170778274536\n",
            "3300: train loss: 0.041522613969199755; test loss: 0.027498392388224602; train accuracy: 0.9879781120953299; test accuracy: 0.991100013256073; Time: 56.53273344039917\n",
            "3400: train loss: 0.037931208164633426; test loss: 0.028419921174645424; train accuracy: 0.9885840353541473; test accuracy: 0.9908999800682068; Time: 57.96601223945618\n",
            "3500: train loss: 0.03585631593856759; test loss: 0.02386031672358513; train accuracy: 0.9888935338236324; test accuracy: 0.9919000267982483; Time: 59.391478061676025\n",
            "3600: train loss: 0.03397711849483054; test loss: 0.025401601567864418; train accuracy: 0.9891059236040154; test accuracy: 0.9912999868392944; Time: 60.82119631767273\n",
            "3700: train loss: 0.03871864613286943; test loss: 0.02256382815539837; train accuracy: 0.9885905997554528; test accuracy: 0.9925000071525574; Time: 62.252161502838135\n",
            "3800: train loss: 0.03811906950626195; test loss: 0.02315233275294304; train accuracy: 0.9881397407116951; test accuracy: 0.992900013923645; Time: 63.6805853843689\n",
            "3900: train loss: 0.03859063451828077; test loss: 0.02568119578063488; train accuracy: 0.9871842052082236; test accuracy: 0.9916999936103821; Time: 65.08513736724854\n",
            "4000: train loss: 0.04127364046630648; test loss: 0.023526789620518684; train accuracy: 0.9871085656027045; test accuracy: 0.9926000237464905; Time: 66.50401782989502\n",
            "4100: train loss: 0.03611607669004236; test loss: 0.031189972534775734; train accuracy: 0.988754278462382; test accuracy: 0.9896000027656555; Time: 67.96024012565613\n",
            "4200: train loss: 0.03784594171020854; test loss: 0.024728823453187943; train accuracy: 0.9889340826347891; test accuracy: 0.9919999837875366; Time: 69.41992163658142\n",
            "4300: train loss: 0.036756177255219895; test loss: 0.027576904743909836; train accuracy: 0.988175373430113; test accuracy: 0.9912999868392944; Time: 70.87989473342896\n",
            "4400: train loss: 0.030402056970386806; test loss: 0.02874109148979187; train accuracy: 0.9897738244812062; test accuracy: 0.9912999868392944; Time: 72.3320984840393\n",
            "4500: train loss: 0.03347609816784456; test loss: 0.03373047336935997; train accuracy: 0.9896577094024412; test accuracy: 0.9894000291824341; Time: 73.76551532745361\n",
            "4600: train loss: 0.03519689000593181; test loss: 0.02405879646539688; train accuracy: 0.9892360629147918; test accuracy: 0.9922999739646912; Time: 75.1938407421112\n",
            "4700: train loss: 0.03307300045799443; test loss: 0.03035624325275421; train accuracy: 0.9897190509919602; test accuracy: 0.9908000230789185; Time: 76.63629269599915\n",
            "4800: train loss: 0.03333257096600749; test loss: 0.021745476871728897; train accuracy: 0.9902471826587854; test accuracy: 0.9930999875068665; Time: 78.08283162117004\n",
            "4900: train loss: 0.030575960169419086; test loss: 0.019382121041417122; train accuracy: 0.9906407047795098; test accuracy: 0.993399977684021; Time: 79.51839232444763\n",
            "5000: train loss: 0.028917553743925725; test loss: 0.029403533786535263; train accuracy: 0.9911055374319571; test accuracy: 0.9909999966621399; Time: 80.95951843261719\n",
            "5100: train loss: 0.032151624969657795; test loss: 0.022858303040266037; train accuracy: 0.9903480267916324; test accuracy: 0.9927999973297119; Time: 82.39254450798035\n",
            "5200: train loss: 0.030418987585690952; test loss: 0.02805287390947342; train accuracy: 0.9899544338108591; test accuracy: 0.9911999702453613; Time: 83.82262444496155\n",
            "5300: train loss: 0.03266104600513779; test loss: 0.023747384548187256; train accuracy: 0.9895959967845386; test accuracy: 0.9926000237464905; Time: 85.24607849121094\n",
            "5400: train loss: 0.03403036493058954; test loss: 0.03076772391796112; train accuracy: 0.9890401893390043; test accuracy: 0.9907000064849854; Time: 86.67365002632141\n",
            "5500: train loss: 0.0316676520351542; test loss: 0.02600276842713356; train accuracy: 0.9905434129600564; test accuracy: 0.9916999936103821; Time: 88.1022539138794\n",
            "5600: train loss: 0.026126423177061157; test loss: 0.023495713248848915; train accuracy: 0.9911417513290176; test accuracy: 0.993399977684021; Time: 89.53345108032227\n",
            "5700: train loss: 0.031012105555484275; test loss: 0.022903958335518837; train accuracy: 0.9900221739584526; test accuracy: 0.9932000041007996; Time: 90.9677848815918\n",
            "5800: train loss: 0.03314812345883489; test loss: 0.032510414719581604; train accuracy: 0.9908068465131037; test accuracy: 0.9908999800682068; Time: 92.41974878311157\n",
            "5900: train loss: 0.030151690938597216; test loss: 0.022091297432780266; train accuracy: 0.9908534383486668; test accuracy: 0.9926000237464905; Time: 93.86579728126526\n",
            "6000: train loss: 0.02965370723430703; test loss: 0.022861352190375328; train accuracy: 0.9908932160342998; test accuracy: 0.9933000206947327; Time: 95.32441735267639\n",
            "6100: train loss: 0.030134947380103465; test loss: 0.020481888204813004; train accuracy: 0.9910505562359214; test accuracy: 0.9929999709129333; Time: 96.77994799613953\n",
            "6200: train loss: 0.030045952521679735; test loss: 0.021881118416786194; train accuracy: 0.9903929796958792; test accuracy: 0.9926999807357788; Time: 98.2379310131073\n",
            "6300: train loss: 0.03580110203522513; test loss: 0.02810760587453842; train accuracy: 0.9891468368842881; test accuracy: 0.991100013256073; Time: 99.6748595237732\n",
            "6400: train loss: 0.031695198471823366; test loss: 0.02636914886534214; train accuracy: 0.9904121150917463; test accuracy: 0.9921000003814697; Time: 101.10626006126404\n",
            "6500: train loss: 0.029394836373289646; test loss: 0.04170852154493332; train accuracy: 0.9906438376481017; test accuracy: 0.9883000254631042; Time: 102.54132461547852\n",
            "6600: train loss: 0.027946349189341062; test loss: 0.036054909229278564; train accuracy: 0.9911636409798923; test accuracy: 0.9897000193595886; Time: 103.98407745361328\n",
            "6700: train loss: 0.028365160577380345; test loss: 0.02277563139796257; train accuracy: 0.9914708169566148; test accuracy: 0.9916999936103821; Time: 105.42347550392151\n",
            "6800: train loss: 0.027612159151033325; test loss: 0.023783378303050995; train accuracy: 0.9912637967636161; test accuracy: 0.9926000237464905; Time: 106.85799932479858\n",
            "6900: train loss: 0.028565777562267945; test loss: 0.025671960785984993; train accuracy: 0.991840223088836; test accuracy: 0.9927999973297119; Time: 108.2948009967804\n",
            "7000: train loss: 0.026227964464603633; test loss: 0.02234271541237831; train accuracy: 0.9919227648098086; test accuracy: 0.9927999973297119; Time: 109.72822141647339\n",
            "7100: train loss: 0.025231356779717475; test loss: 0.02015308290719986; train accuracy: 0.991957748376832; test accuracy: 0.992900013923645; Time: 111.16141653060913\n",
            "7200: train loss: 0.02322099222060264; test loss: 0.026267968118190765; train accuracy: 0.9925442621906534; test accuracy: 0.9923999905586243; Time: 112.59420728683472\n",
            "7300: train loss: 0.023845039306480888; test loss: 0.02457478456199169; train accuracy: 0.9920295112276435; test accuracy: 0.9922000169754028; Time: 114.03297233581543\n",
            "7400: train loss: 0.027460942396458493; test loss: 0.022777214646339417; train accuracy: 0.9916692513505341; test accuracy: 0.9926999807357788; Time: 115.47047710418701\n",
            "7500: train loss: 0.022970260488674846; test loss: 0.022050103172659874; train accuracy: 0.9928512100784047; test accuracy: 0.9937999844551086; Time: 116.90646195411682\n",
            "7600: train loss: 0.02636941990893859; test loss: 0.020483896136283875; train accuracy: 0.9918658693452632; test accuracy: 0.9929999709129333; Time: 118.34388494491577\n",
            "7700: train loss: 0.024420349941507737; test loss: 0.01985120214521885; train accuracy: 0.9919008306054734; test accuracy: 0.9936000108718872; Time: 119.77459931373596\n",
            "7800: train loss: 0.029859796139609988; test loss: 0.021371055394411087; train accuracy: 0.9910009851079947; test accuracy: 0.9940000176429749; Time: 121.20904111862183\n",
            "7900: train loss: 0.025319100248848286; test loss: 0.02655194327235222; train accuracy: 0.9915800430152739; test accuracy: 0.9914000034332275; Time: 122.64870095252991\n",
            "8000: train loss: 0.023596847893985782; test loss: 0.028592681512236595; train accuracy: 0.9917684915192104; test accuracy: 0.9922999739646912; Time: 124.08263278007507\n",
            "8100: train loss: 0.020683266604888624; test loss: 0.023424549028277397; train accuracy: 0.9928170870980524; test accuracy: 0.993399977684021; Time: 125.51911616325378\n",
            "8200: train loss: 0.021957688713667933; test loss: 0.021361181512475014; train accuracy: 0.9924201618762595; test accuracy: 0.992900013923645; Time: 126.9571418762207\n",
            "8300: train loss: 0.025397509951708322; test loss: 0.018601398915052414; train accuracy: 0.9914732514708973; test accuracy: 0.9939000010490417; Time: 128.39081645011902\n",
            "8400: train loss: 0.025676681006963614; test loss: 0.023246021941304207; train accuracy: 0.9916065219657662; test accuracy: 0.992900013923645; Time: 129.824609041214\n",
            "8500: train loss: 0.023111327499234856; test loss: 0.026886243373155594; train accuracy: 0.9922447970515355; test accuracy: 0.9916999936103821; Time: 131.25884628295898\n",
            "8600: train loss: 0.02384757750759906; test loss: 0.01917283609509468; train accuracy: 0.9925044720590572; test accuracy: 0.9940000176429749; Time: 132.69718313217163\n",
            "8700: train loss: 0.020161109108929265; test loss: 0.0196832325309515; train accuracy: 0.9938689184188317; test accuracy: 0.9937999844551086; Time: 134.13630270957947\n",
            "8800: train loss: 0.022794360719759238; test loss: 0.029313793405890465; train accuracy: 0.9928293327358889; test accuracy: 0.9904999732971191; Time: 135.5708634853363\n",
            "8900: train loss: 0.019380526211969385; test loss: 0.02444039098918438; train accuracy: 0.9937516108775254; test accuracy: 0.9929999709129333; Time: 137.0122845172882\n",
            "9000: train loss: 0.023714440925868756; test loss: 0.025682367384433746; train accuracy: 0.9924914502324133; test accuracy: 0.9919999837875366; Time: 138.4474093914032\n",
            "9100: train loss: 0.023436744090544068; test loss: 0.03491181135177612; train accuracy: 0.9924117365803229; test accuracy: 0.989799976348877; Time: 139.88384079933167\n",
            "9200: train loss: 0.023034432213879606; test loss: 0.024339839816093445; train accuracy: 0.9923143012437735; test accuracy: 0.9934999942779541; Time: 141.32215905189514\n",
            "9300: train loss: 0.020171007232693928; test loss: 0.022797662764787674; train accuracy: 0.9931272989551256; test accuracy: 0.993399977684021; Time: 142.76211595535278\n",
            "9400: train loss: 0.018895291503807806; test loss: 0.02166064828634262; train accuracy: 0.9935300439426289; test accuracy: 0.9932000041007996; Time: 144.19856357574463\n",
            "9500: train loss: 0.01889845741500414; test loss: 0.025385782122612; train accuracy: 0.9933958322638516; test accuracy: 0.9925000071525574; Time: 145.63618779182434\n",
            "9600: train loss: 0.023380698938523115; test loss: 0.031455229967832565; train accuracy: 0.9925053793233769; test accuracy: 0.9916999936103821; Time: 147.07289862632751\n",
            "9700: train loss: 0.0187386740397323; test loss: 0.02253529243171215; train accuracy: 0.9938001652926773; test accuracy: 0.9939000010490417; Time: 148.50800776481628\n",
            "9800: train loss: 0.02267698065904611; test loss: 0.021326998248696327; train accuracy: 0.9927046327778891; test accuracy: 0.9937000274658203; Time: 149.93660283088684\n",
            "9900: train loss: 0.02063035131551152; test loss: 0.019755126908421516; train accuracy: 0.9933532133062054; test accuracy: 0.9944000244140625; Time: 151.35965204238892\n",
            "[[ 978    0    0    0    0    0    1    1    0    0]\n",
            " [   0 1133    2    0    0    0    0    0    0    0]\n",
            " [   1    0 1027    0    0    0    0    4    0    0]\n",
            " [   0    0    0 1009    0    1    0    0    0    0]\n",
            " [   0    0    0    0  974    0    0    2    0    6]\n",
            " [   0    0    0    8    0  882    1    0    1    0]\n",
            " [   1    3    0    1    1    0  950    0    2    0]\n",
            " [   0    3    3    0    0    0    0 1020    0    2]\n",
            " [   0    0    2    1    0    0    0    0  970    1]\n",
            " [   0    0    0    0    3    0    0    2    1 1003]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9980    0.9980    0.9980       980\n",
            "           1     0.9947    0.9982    0.9965      1135\n",
            "           2     0.9932    0.9952    0.9942      1032\n",
            "           3     0.9902    0.9990    0.9946      1010\n",
            "           4     0.9959    0.9919    0.9939       982\n",
            "           5     0.9989    0.9888    0.9938       892\n",
            "           6     0.9979    0.9916    0.9948       958\n",
            "           7     0.9913    0.9922    0.9917      1028\n",
            "           8     0.9959    0.9959    0.9959       974\n",
            "           9     0.9911    0.9941    0.9926      1009\n",
            "\n",
            "    accuracy                         0.9946     10000\n",
            "   macro avg     0.9947    0.9945    0.9946     10000\n",
            "weighted avg     0.9946    0.9946    0.9946     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX3yiYfV91gB",
        "colab_type": "text"
      },
      "source": [
        "##Hybrid KScan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMqU04pi94f0",
        "colab_type": "code",
        "outputId": "ef8d17c4-5cc0-43d5-ae84-7097c8c306b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Hybrid KScan Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "\n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left = [tf.Variable(tf.eye(row, dtype=dtype), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.concat([tf.ones((1, col)), tf.zeros((1, col))], axis=0), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_hybrid_kscan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_hybrid_kscan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'hybrid_kscan_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3887710571289062; test loss: 2.2904887199401855; train accuracy: 0.0703125; test accuracy: 0.13860000669956207; Time: 0.860135555267334\n",
            "100: train loss: 0.9845488595396279; test loss: 0.2089725136756897; train accuracy: 0.7054617187499996; test accuracy: 0.9459999799728394; Time: 2.48234224319458\n",
            "200: train loss: 0.5171744149952544; test loss: 0.09064295142889023; train accuracy: 0.8484288838832584; test accuracy: 0.9763000011444092; Time: 4.102585315704346\n",
            "300: train loss: 0.28651777161451164; test loss: 0.06740223616361618; train accuracy: 0.917428455211132; test accuracy: 0.9830999970436096; Time: 5.714547395706177\n",
            "400: train loss: 0.17853573998640934; test loss: 0.04883049800992012; train accuracy: 0.9493884733203084; test accuracy: 0.9865999817848206; Time: 7.329389810562134\n",
            "500: train loss: 0.12344305702969546; test loss: 0.043190523982048035; train accuracy: 0.9653251618815094; test accuracy: 0.9872000217437744; Time: 8.942263126373291\n",
            "600: train loss: 0.09605316508154718; test loss: 0.03660052642226219; train accuracy: 0.9735545291529728; test accuracy: 0.9900000095367432; Time: 10.553985834121704\n",
            "700: train loss: 0.07763546259125248; test loss: 0.03510921075940132; train accuracy: 0.9783735315584412; test accuracy: 0.9902999997138977; Time: 12.167233228683472\n",
            "800: train loss: 0.06661065078038031; test loss: 0.032090313732624054; train accuracy: 0.9823265165659546; test accuracy: 0.9908000230789185; Time: 13.775190830230713\n",
            "900: train loss: 0.0601823519647389; test loss: 0.030707983300089836; train accuracy: 0.9829162237335405; test accuracy: 0.9909999966621399; Time: 15.378896236419678\n",
            "1000: train loss: 0.05556045112079015; test loss: 0.03144797310233116; train accuracy: 0.9840259077278727; test accuracy: 0.9909999966621399; Time: 16.990705251693726\n",
            "1100: train loss: 0.05163250096761961; test loss: 0.03028007224202156; train accuracy: 0.985901283743577; test accuracy: 0.991100013256073; Time: 18.601763010025024\n",
            "1200: train loss: 0.048795082473212716; test loss: 0.02522038295865059; train accuracy: 0.9866699084998412; test accuracy: 0.9926999807357788; Time: 20.21332025527954\n",
            "1300: train loss: 0.045747445478178515; test loss: 0.025203540921211243; train accuracy: 0.9876770401933394; test accuracy: 0.9923999905586243; Time: 21.82049036026001\n",
            "1400: train loss: 0.04291916376774951; test loss: 0.024704383686184883; train accuracy: 0.9882249899905587; test accuracy: 0.9926000237464905; Time: 23.42846155166626\n",
            "1500: train loss: 0.04191363948338655; test loss: 0.025401080027222633; train accuracy: 0.9880254019210656; test accuracy: 0.9921000003814697; Time: 25.041038513183594\n",
            "1600: train loss: 0.03601105736089083; test loss: 0.02398800291121006; train accuracy: 0.9899051988694402; test accuracy: 0.9923999905586243; Time: 26.65381956100464\n",
            "1700: train loss: 0.03601891823115225; test loss: 0.0237435270100832; train accuracy: 0.9900328863863873; test accuracy: 0.9925000071525574; Time: 28.269259452819824\n",
            "1800: train loss: 0.0331981981711609; test loss: 0.02717926725745201; train accuracy: 0.9905741748774842; test accuracy: 0.9918000102043152; Time: 29.88205122947693\n",
            "1900: train loss: 0.03246646549797324; test loss: 0.025758039206266403; train accuracy: 0.9910583224188394; test accuracy: 0.991599977016449; Time: 31.495681047439575\n",
            "2000: train loss: 0.030975327723310922; test loss: 0.024334775283932686; train accuracy: 0.9912047287643068; test accuracy: 0.9921000003814697; Time: 33.105185747146606\n",
            "2100: train loss: 0.02898439499919123; test loss: 0.024544524028897285; train accuracy: 0.9918629534621993; test accuracy: 0.9927999973297119; Time: 34.70349431037903\n",
            "2200: train loss: 0.02733891021738546; test loss: 0.022251849994063377; train accuracy: 0.9919355902021904; test accuracy: 0.9926000237464905; Time: 36.291749238967896\n",
            "2300: train loss: 0.02476260796045828; test loss: 0.02241695486009121; train accuracy: 0.9928720114294426; test accuracy: 0.9927999973297119; Time: 37.90054798126221\n",
            "2400: train loss: 0.02429316400971037; test loss: 0.019725903868675232; train accuracy: 0.9930189841970044; test accuracy: 0.9939000010490417; Time: 39.5122172832489\n",
            "2500: train loss: 0.025565175937708098; test loss: 0.021574126556515694; train accuracy: 0.9930744757769389; test accuracy: 0.9933000206947327; Time: 41.12702536582947\n",
            "2600: train loss: 0.02440671581262312; test loss: 0.019943630322813988; train accuracy: 0.9926460402442739; test accuracy: 0.9933000206947327; Time: 42.73209095001221\n",
            "2700: train loss: 0.023673145519093507; test loss: 0.020910833030939102; train accuracy: 0.9929174770038571; test accuracy: 0.9927999973297119; Time: 44.327462911605835\n",
            "2800: train loss: 0.022291679282595005; test loss: 0.020042315125465393; train accuracy: 0.9932033377905245; test accuracy: 0.9937000274658203; Time: 45.929513692855835\n",
            "2900: train loss: 0.02157848092346142; test loss: 0.02014811709523201; train accuracy: 0.9936451583721768; test accuracy: 0.993399977684021; Time: 47.54384779930115\n",
            "3000: train loss: 0.021090345474207316; test loss: 0.020168490707874298; train accuracy: 0.9941086021071859; test accuracy: 0.993399977684021; Time: 49.15696477890015\n",
            "3100: train loss: 0.0210436759101382; test loss: 0.020752353593707085; train accuracy: 0.9939062197023616; test accuracy: 0.9934999942779541; Time: 50.76334309577942\n",
            "3200: train loss: 0.018981482886818522; test loss: 0.0198528952896595; train accuracy: 0.9945418509373335; test accuracy: 0.9930999875068665; Time: 52.37105178833008\n",
            "3300: train loss: 0.019698884140833803; test loss: 0.018915560096502304; train accuracy: 0.994859803460898; test accuracy: 0.9940999746322632; Time: 53.97615647315979\n",
            "3400: train loss: 0.01814396839568182; test loss: 0.01993056945502758; train accuracy: 0.9948360177920536; test accuracy: 0.9932000041007996; Time: 55.58767032623291\n",
            "3500: train loss: 0.01794881156164548; test loss: 0.02024129405617714; train accuracy: 0.9942435083381645; test accuracy: 0.9930999875068665; Time: 57.19733476638794\n",
            "3600: train loss: 0.016719523313321376; test loss: 0.01855713129043579; train accuracy: 0.9950787447198539; test accuracy: 0.9934999942779541; Time: 58.810343503952026\n",
            "3700: train loss: 0.01778847296659988; test loss: 0.021831708028912544; train accuracy: 0.9947753205871092; test accuracy: 0.9937000274658203; Time: 60.41707968711853\n",
            "3800: train loss: 0.017076680372507234; test loss: 0.021626221016049385; train accuracy: 0.9952854310414492; test accuracy: 0.9929999709129333; Time: 62.02256369590759\n",
            "3900: train loss: 0.018333669555244028; test loss: 0.021972307935357094; train accuracy: 0.9948603099935915; test accuracy: 0.9929999709129333; Time: 63.633465051651\n",
            "4000: train loss: 0.015763247593758398; test loss: 0.019117990508675575; train accuracy: 0.9960458925763654; test accuracy: 0.9940000176429749; Time: 65.2440459728241\n",
            "4100: train loss: 0.016776357942067204; test loss: 0.017603609710931778; train accuracy: 0.9953845344459933; test accuracy: 0.9944000244140625; Time: 66.8511872291565\n",
            "4200: train loss: 0.01568612029299069; test loss: 0.02023027464747429; train accuracy: 0.9952096237221442; test accuracy: 0.9932000041007996; Time: 68.4561653137207\n",
            "4300: train loss: 0.015727295135619702; test loss: 0.018993204459547997; train accuracy: 0.9953943572542772; test accuracy: 0.993399977684021; Time: 70.06303191184998\n",
            "4400: train loss: 0.014325121667745805; test loss: 0.017885200679302216; train accuracy: 0.9957749601626473; test accuracy: 0.9937999844551086; Time: 71.67590975761414\n",
            "4500: train loss: 0.012972129847250905; test loss: 0.01785201020538807; train accuracy: 0.9961690495746773; test accuracy: 0.9945999979972839; Time: 73.28773307800293\n",
            "4600: train loss: 0.012664541632671963; test loss: 0.019557315856218338; train accuracy: 0.9963064699735393; test accuracy: 0.9940000176429749; Time: 74.89819574356079\n",
            "4700: train loss: 0.011421239412260012; test loss: 0.018956176936626434; train accuracy: 0.9966870415713541; test accuracy: 0.9937999844551086; Time: 76.51254987716675\n",
            "4800: train loss: 0.013211343438902802; test loss: 0.018948525190353394; train accuracy: 0.9965509769007362; test accuracy: 0.9939000010490417; Time: 78.12798237800598\n",
            "4900: train loss: 0.011578294697512363; test loss: 0.01922060176730156; train accuracy: 0.9969860217534432; test accuracy: 0.9944999814033508; Time: 79.74040365219116\n",
            "5000: train loss: 0.011393925638475862; test loss: 0.018497008830308914; train accuracy: 0.9968988947237548; test accuracy: 0.993399977684021; Time: 81.35250425338745\n",
            "5100: train loss: 0.013120249686119004; test loss: 0.019447313621640205; train accuracy: 0.9962692171681324; test accuracy: 0.9926000237464905; Time: 82.96509861946106\n",
            "5200: train loss: 0.012996553141670916; test loss: 0.018257154151797295; train accuracy: 0.9961296649273573; test accuracy: 0.9937999844551086; Time: 84.57907152175903\n",
            "5300: train loss: 0.011921417222954025; test loss: 0.017661403864622116; train accuracy: 0.9968637688014814; test accuracy: 0.993399977684021; Time: 86.1881537437439\n",
            "5400: train loss: 0.012640776914419052; test loss: 0.017750583589076996; train accuracy: 0.9964025276211717; test accuracy: 0.9940999746322632; Time: 87.79670882225037\n",
            "5500: train loss: 0.011278525332925959; test loss: 0.01828409545123577; train accuracy: 0.9967953101866088; test accuracy: 0.9939000010490417; Time: 89.40730404853821\n",
            "5600: train loss: 0.009863015173404979; test loss: 0.018129151314496994; train accuracy: 0.9973493323404136; test accuracy: 0.9936000108718872; Time: 91.01605892181396\n",
            "5700: train loss: 0.01160738150298305; test loss: 0.017754685133695602; train accuracy: 0.9963457274945098; test accuracy: 0.9940999746322632; Time: 92.62753224372864\n",
            "5800: train loss: 0.011264489028210964; test loss: 0.016404742375016212; train accuracy: 0.9965744048469457; test accuracy: 0.9945999979972839; Time: 94.24166917800903\n",
            "5900: train loss: 0.010305610854195593; test loss: 0.017017632722854614; train accuracy: 0.9971048249818644; test accuracy: 0.9944999814033508; Time: 95.85539841651917\n",
            "6000: train loss: 0.010517539211053395; test loss: 0.017742611467838287; train accuracy: 0.9972403133590173; test accuracy: 0.9939000010490417; Time: 97.46957325935364\n",
            "6100: train loss: 0.01160522978040145; test loss: 0.01940290257334709; train accuracy: 0.9967696081401485; test accuracy: 0.9937000274658203; Time: 99.0833170413971\n",
            "6200: train loss: 0.011259030216322945; test loss: 0.017170650884509087; train accuracy: 0.9966469090115846; test accuracy: 0.9936000108718872; Time: 100.69364380836487\n",
            "6300: train loss: 0.008777676098074507; test loss: 0.017833935096859932; train accuracy: 0.9976221257022253; test accuracy: 0.9940000176429749; Time: 102.30664801597595\n",
            "6400: train loss: 0.009101425530491184; test loss: 0.01905152201652527; train accuracy: 0.9976280317351688; test accuracy: 0.9933000206947327; Time: 103.91900157928467\n",
            "6500: train loss: 0.008957918218450319; test loss: 0.017390176653862; train accuracy: 0.997408997612125; test accuracy: 0.9937000274658203; Time: 105.52661275863647\n",
            "6600: train loss: 0.0084576271349494; test loss: 0.017263222485780716; train accuracy: 0.9978034067186412; test accuracy: 0.9940000176429749; Time: 107.14001798629761\n",
            "6700: train loss: 0.007725043030139914; test loss: 0.018003934994339943; train accuracy: 0.9977564907046903; test accuracy: 0.9930999875068665; Time: 108.75020265579224\n",
            "6800: train loss: 0.008323330769852895; test loss: 0.019225183874368668; train accuracy: 0.9972710323122278; test accuracy: 0.992900013923645; Time: 110.3642029762268\n",
            "6900: train loss: 0.008151559888562601; test loss: 0.021121006458997726; train accuracy: 0.9976930386782252; test accuracy: 0.992900013923645; Time: 111.97908115386963\n",
            "7000: train loss: 0.007889604747577003; test loss: 0.018252933397889137; train accuracy: 0.9979921728994836; test accuracy: 0.9934999942779541; Time: 113.58987164497375\n",
            "7100: train loss: 0.007454511346753533; test loss: 0.021634824573993683; train accuracy: 0.9980622378030471; test accuracy: 0.9932000041007996; Time: 115.20216679573059\n",
            "7200: train loss: 0.01055325330644502; test loss: 0.01894189789891243; train accuracy: 0.9973575020623452; test accuracy: 0.993399977684021; Time: 116.81334495544434\n",
            "7300: train loss: 0.009586662774541432; test loss: 0.0184415765106678; train accuracy: 0.9973168819719369; test accuracy: 0.9934999942779541; Time: 118.42305421829224\n",
            "7400: train loss: 0.007849533760888486; test loss: 0.018295064568519592; train accuracy: 0.9979470952754631; test accuracy: 0.9939000010490417; Time: 120.03135991096497\n",
            "7500: train loss: 0.007877096989416965; test loss: 0.017919637262821198; train accuracy: 0.9977653951138697; test accuracy: 0.9930999875068665; Time: 121.63470101356506\n",
            "7600: train loss: 0.0077905727788599175; test loss: 0.018900103867053986; train accuracy: 0.9975941201246973; test accuracy: 0.9936000108718872; Time: 123.21961426734924\n",
            "7700: train loss: 0.00838590404092335; test loss: 0.02086724527180195; train accuracy: 0.9971580057478409; test accuracy: 0.9926000237464905; Time: 124.81974959373474\n",
            "7800: train loss: 0.006696958872477182; test loss: 0.020263204351067543; train accuracy: 0.9981780139485484; test accuracy: 0.9934999942779541; Time: 126.43123364448547\n",
            "7900: train loss: 0.006450567605174251; test loss: 0.019368549808859825; train accuracy: 0.9980464795208495; test accuracy: 0.9934999942779541; Time: 128.0456383228302\n",
            "8000: train loss: 0.007380127668502065; test loss: 0.01897314004600048; train accuracy: 0.997806581659841; test accuracy: 0.9934999942779541; Time: 129.672776222229\n",
            "8100: train loss: 0.007041856974056802; test loss: 0.019647665321826935; train accuracy: 0.9979478502107794; test accuracy: 0.9939000010490417; Time: 131.29629254341125\n",
            "8200: train loss: 0.006721826251231527; test loss: 0.02029339037835598; train accuracy: 0.9979093063905515; test accuracy: 0.9932000041007996; Time: 132.89606738090515\n",
            "8300: train loss: 0.006501335569004947; test loss: 0.018757261335849762; train accuracy: 0.998050458782802; test accuracy: 0.993399977684021; Time: 134.50031542778015\n",
            "8400: train loss: 0.0064935583985578525; test loss: 0.021217865869402885; train accuracy: 0.9978934887010107; test accuracy: 0.9932000041007996; Time: 136.11226105690002\n",
            "8500: train loss: 0.006549911214695213; test loss: 0.021692534908652306; train accuracy: 0.9981164889808232; test accuracy: 0.9930999875068665; Time: 137.71700930595398\n",
            "8600: train loss: 0.006925285048159045; test loss: 0.01898752711713314; train accuracy: 0.9977927297880392; test accuracy: 0.9936000108718872; Time: 139.31935262680054\n",
            "8700: train loss: 0.006689564724551929; test loss: 0.021527355536818504; train accuracy: 0.9977956163455977; test accuracy: 0.9930999875068665; Time: 140.9231390953064\n",
            "8800: train loss: 0.006115377878162244; test loss: 0.019508996978402138; train accuracy: 0.9979778956523792; test accuracy: 0.9934999942779541; Time: 142.53194952011108\n",
            "8900: train loss: 0.006080343713313674; test loss: 0.021712908521294594; train accuracy: 0.9982094796985675; test accuracy: 0.9933000206947327; Time: 144.14157247543335\n",
            "9000: train loss: 0.005945977653462454; test loss: 0.022061865776777267; train accuracy: 0.9985527506837614; test accuracy: 0.9930999875068665; Time: 145.75112342834473\n",
            "9100: train loss: 0.006779556972360634; test loss: 0.01863805018365383; train accuracy: 0.9978835288245329; test accuracy: 0.9944000244140625; Time: 147.35838747024536\n",
            "9200: train loss: 0.004719999437887958; test loss: 0.019503246992826462; train accuracy: 0.9988296093910697; test accuracy: 0.9940000176429749; Time: 148.96610951423645\n",
            "9300: train loss: 0.005039250938541603; test loss: 0.023317810148000717; train accuracy: 0.998775664657746; test accuracy: 0.9933000206947327; Time: 150.57322144508362\n",
            "9400: train loss: 0.004637571633331918; test loss: 0.019950183108448982; train accuracy: 0.9986655720219146; test accuracy: 0.9937999844551086; Time: 152.18167233467102\n",
            "9500: train loss: 0.006480011636041512; test loss: 0.02151867002248764; train accuracy: 0.9982432093747386; test accuracy: 0.9937000274658203; Time: 153.78652024269104\n",
            "9600: train loss: 0.005408421062119103; test loss: 0.020904600620269775; train accuracy: 0.9984078677978862; test accuracy: 0.9939000010490417; Time: 155.3920397758484\n",
            "9700: train loss: 0.005005257146305076; test loss: 0.019450385123491287; train accuracy: 0.9987517561545908; test accuracy: 0.9937000274658203; Time: 157.0189266204834\n",
            "9800: train loss: 0.004593573396705753; test loss: 0.02053799480199814; train accuracy: 0.9987374893524923; test accuracy: 0.9937000274658203; Time: 158.64587593078613\n",
            "9900: train loss: 0.004765990980658049; test loss: 0.020256508141756058; train accuracy: 0.998707382642848; test accuracy: 0.9929999709129333; Time: 160.27679991722107\n",
            "[[ 978    0    0    0    0    0    1    1    0    0]\n",
            " [   0 1127    1    0    0    1    2    3    0    1]\n",
            " [   1    0 1027    1    0    0    0    3    0    0]\n",
            " [   0    0    0 1007    0    3    0    0    0    0]\n",
            " [   0    0    0    0  979    0    0    0    0    3]\n",
            " [   1    0    0    1    0  889    1    0    0    0]\n",
            " [   3    1    0    0    2    2  949    0    1    0]\n",
            " [   0    2    3    0    0    0    0 1022    0    1]\n",
            " [   3    0    1    2    1    1    0    0  964    2]\n",
            " [   1    0    0    1    2    1    0    1    1 1002]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9909    0.9980    0.9944       980\n",
            "           1     0.9973    0.9930    0.9951      1135\n",
            "           2     0.9952    0.9952    0.9952      1032\n",
            "           3     0.9951    0.9970    0.9960      1010\n",
            "           4     0.9949    0.9969    0.9959       982\n",
            "           5     0.9911    0.9966    0.9939       892\n",
            "           6     0.9958    0.9906    0.9932       958\n",
            "           7     0.9922    0.9942    0.9932      1028\n",
            "           8     0.9979    0.9897    0.9938       974\n",
            "           9     0.9931    0.9931    0.9931      1009\n",
            "\n",
            "    accuracy                         0.9944     10000\n",
            "   macro avg     0.9943    0.9944    0.9944     10000\n",
            "weighted avg     0.9944    0.9944    0.9944     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8Za33I7fVp",
        "colab_type": "text"
      },
      "source": [
        "##KDiag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaFyhTOM7jhc",
        "colab_type": "code",
        "outputId": "3c632ab2-d3b4-4efb-af2c-6ec25e2367fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with KDiag Preconditioning \n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "\n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "\n",
        "    qs_left =  [tf.Variable(tf.ones((row,1), dtype=dtype), trainable=False) for row in rows]\n",
        "    qs_right = [tf.Variable(tf.ones((1, col), dtype=dtype), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_kdiag(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_qs = [psgd.update_precond_kdiag(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        \n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'kdiag_psgd.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3804333209991455; test loss: 2.277445077896118; train accuracy: 0.078125; test accuracy: 0.11339999735355377; Time: 1.1713142395019531\n",
            "100: train loss: 1.023000215509535; test loss: 0.2541208863258362; train accuracy: 0.6726671874999998; test accuracy: 0.9232000112533569; Time: 2.476010322570801\n",
            "200: train loss: 0.5793444622594308; test loss: 0.12295564264059067; train accuracy: 0.816227766414133; test accuracy: 0.9610999822616577; Time: 3.7732372283935547\n",
            "300: train loss: 0.3421442895983909; test loss: 0.07866337895393372; train accuracy: 0.8924765817537257; test accuracy: 0.9750999808311462; Time: 5.068522930145264\n",
            "400: train loss: 0.2248196931149301; test loss: 0.0704597756266594; train accuracy: 0.9308052099861354; test accuracy: 0.9753999710083008; Time: 6.355468034744263\n",
            "500: train loss: 0.16590465741409413; test loss: 0.054392460733652115; train accuracy: 0.9493403262563517; test accuracy: 0.9839000105857849; Time: 7.647140979766846\n",
            "600: train loss: 0.12940154191015235; test loss: 0.06881364434957504; train accuracy: 0.9608738120443081; test accuracy: 0.9782999753952026; Time: 8.936624526977539\n",
            "700: train loss: 0.10926466212648508; test loss: 0.039395771920681; train accuracy: 0.9673288768716665; test accuracy: 0.987500011920929; Time: 10.225998163223267\n",
            "800: train loss: 0.09380177839461133; test loss: 0.05304110050201416; train accuracy: 0.9715878391899954; test accuracy: 0.9833999872207642; Time: 11.514446258544922\n",
            "900: train loss: 0.08782657039877773; test loss: 0.037206828594207764; train accuracy: 0.9732215335930674; test accuracy: 0.9878000020980835; Time: 12.8056321144104\n",
            "1000: train loss: 0.08328281884692353; test loss: 0.0362805537879467; train accuracy: 0.9741014090803956; test accuracy: 0.9883999824523926; Time: 14.095227241516113\n",
            "1100: train loss: 0.07711007338407015; test loss: 0.06422464549541473; train accuracy: 0.9762115312304097; test accuracy: 0.9797999858856201; Time: 15.382198810577393\n",
            "1200: train loss: 0.07878377577894752; test loss: 0.05300694704055786; train accuracy: 0.9750726152882802; test accuracy: 0.9836999773979187; Time: 16.67372989654541\n",
            "1300: train loss: 0.0734528287298401; test loss: 0.03836562857031822; train accuracy: 0.9774289136446757; test accuracy: 0.9876999855041504; Time: 17.961449146270752\n",
            "1400: train loss: 0.07096749629294985; test loss: 0.03302131965756416; train accuracy: 0.9784174085522754; test accuracy: 0.9882000088691711; Time: 19.2504243850708\n",
            "1500: train loss: 0.0651431619124718; test loss: 0.032969146966934204; train accuracy: 0.9802186092575642; test accuracy: 0.9886999726295471; Time: 20.537315845489502\n",
            "1600: train loss: 0.061021732994583286; test loss: 0.028410784900188446; train accuracy: 0.981396516908319; test accuracy: 0.9908999800682068; Time: 21.82845377922058\n",
            "1700: train loss: 0.06035627790515516; test loss: 0.041098035871982574; train accuracy: 0.980743342160937; test accuracy: 0.987500011920929; Time: 23.124165296554565\n",
            "1800: train loss: 0.05825699173713496; test loss: 0.030793286859989166; train accuracy: 0.9817578011012859; test accuracy: 0.9889000058174133; Time: 24.416036367416382\n",
            "1900: train loss: 0.05897426162180472; test loss: 0.03628411516547203; train accuracy: 0.9821424884862922; test accuracy: 0.9889000058174133; Time: 25.70793652534485\n",
            "2000: train loss: 0.0578173637631714; test loss: 0.02743445709347725; train accuracy: 0.9825131226649796; test accuracy: 0.9904999732971191; Time: 26.996756076812744\n",
            "2100: train loss: 0.060643424122170546; test loss: 0.02758387103676796; train accuracy: 0.9816046580873301; test accuracy: 0.9911999702453613; Time: 28.290330171585083\n",
            "2200: train loss: 0.050052578555729095; test loss: 0.026856329292058945; train accuracy: 0.9845838551910252; test accuracy: 0.9907000064849854; Time: 29.57757043838501\n",
            "2300: train loss: 0.04813777509957817; test loss: 0.0319083072245121; train accuracy: 0.9856090606060388; test accuracy: 0.9901000261306763; Time: 30.868821620941162\n",
            "2400: train loss: 0.04850010001513303; test loss: 0.04258200153708458; train accuracy: 0.9843917525938349; test accuracy: 0.9858999848365784; Time: 32.15245509147644\n",
            "2500: train loss: 0.05109228094115783; test loss: 0.030503159388899803; train accuracy: 0.9833199154031915; test accuracy: 0.9901000261306763; Time: 33.4469108581543\n",
            "2600: train loss: 0.04784098755885559; test loss: 0.10298693925142288; train accuracy: 0.9840158306595622; test accuracy: 0.9667999744415283; Time: 34.73291492462158\n",
            "2700: train loss: 0.04792901639910792; test loss: 0.02765991911292076; train accuracy: 0.983712133117543; test accuracy: 0.9900000095367432; Time: 36.021204471588135\n",
            "2800: train loss: 0.04849250221300123; test loss: 0.02897058054804802; train accuracy: 0.9842728731396949; test accuracy: 0.9914000034332275; Time: 37.31048560142517\n",
            "2900: train loss: 0.04607919894161556; test loss: 0.026147326454520226; train accuracy: 0.9839712746995031; test accuracy: 0.9914000034332275; Time: 38.60039305686951\n",
            "3000: train loss: 0.04519401458100732; test loss: 0.03761935606598854; train accuracy: 0.984375280935464; test accuracy: 0.9876999855041504; Time: 39.89154267311096\n",
            "3100: train loss: 0.042749269767319666; test loss: 0.031206414103507996; train accuracy: 0.9860393543870768; test accuracy: 0.9898999929428101; Time: 41.183454275131226\n",
            "3200: train loss: 0.04407271630172419; test loss: 0.03100493736565113; train accuracy: 0.9865910871984327; test accuracy: 0.989799976348877; Time: 42.471943855285645\n",
            "3300: train loss: 0.040385887810999985; test loss: 0.03803972899913788; train accuracy: 0.9877578329992842; test accuracy: 0.9891999959945679; Time: 43.764790534973145\n",
            "3400: train loss: 0.0411221980876676; test loss: 0.03179670870304108; train accuracy: 0.9870200115216615; test accuracy: 0.9902999997138977; Time: 45.05829977989197\n",
            "3500: train loss: 0.04156856170356671; test loss: 0.029304221272468567; train accuracy: 0.9866484319434927; test accuracy: 0.991100013256073; Time: 46.34458518028259\n",
            "3600: train loss: 0.040012125065621885; test loss: 0.0269022099673748; train accuracy: 0.9873640084819527; test accuracy: 0.9918000102043152; Time: 47.632402420043945\n",
            "3700: train loss: 0.04258702012644023; test loss: 0.027041485533118248; train accuracy: 0.9866133275248601; test accuracy: 0.991599977016449; Time: 48.916178941726685\n",
            "3800: train loss: 0.038264014390316646; test loss: 0.028958618640899658; train accuracy: 0.9875033100330333; test accuracy: 0.9908000230789185; Time: 50.20184659957886\n",
            "3900: train loss: 0.04129090705567877; test loss: 0.03263603523373604; train accuracy: 0.9874017442024491; test accuracy: 0.9902999997138977; Time: 51.485310792922974\n",
            "4000: train loss: 0.0404383493736289; test loss: 0.02686949446797371; train accuracy: 0.9878168754407689; test accuracy: 0.9914000034332275; Time: 52.769169092178345\n",
            "4100: train loss: 0.03768622395535478; test loss: 0.02357659861445427; train accuracy: 0.9886959890135458; test accuracy: 0.9919999837875366; Time: 54.06216907501221\n",
            "4200: train loss: 0.037895454801117855; test loss: 0.025456780567765236; train accuracy: 0.9884291959936168; test accuracy: 0.9926000237464905; Time: 55.34754133224487\n",
            "4300: train loss: 0.03985522761281578; test loss: 0.02677154541015625; train accuracy: 0.9878647297781002; test accuracy: 0.9911999702453613; Time: 56.63873338699341\n",
            "4400: train loss: 0.04062928010487388; test loss: 0.026693228632211685; train accuracy: 0.987442250136157; test accuracy: 0.9911999702453613; Time: 57.9387001991272\n",
            "4500: train loss: 0.03862713689662986; test loss: 0.03788360580801964; train accuracy: 0.9879485941566648; test accuracy: 0.9890000224113464; Time: 59.23458194732666\n",
            "4600: train loss: 0.0377273046401662; test loss: 0.02526218257844448; train accuracy: 0.9878941723533285; test accuracy: 0.9926999807357788; Time: 60.52485156059265\n",
            "4700: train loss: 0.03541946750331391; test loss: 0.03294167295098305; train accuracy: 0.988175540504918; test accuracy: 0.9897000193595886; Time: 61.80658173561096\n",
            "4800: train loss: 0.032154295542341886; test loss: 0.032317377626895905; train accuracy: 0.9894745307808009; test accuracy: 0.9909999966621399; Time: 63.1007444858551\n",
            "4900: train loss: 0.03385119447368706; test loss: 0.026920916512608528; train accuracy: 0.9897917578425378; test accuracy: 0.991599977016449; Time: 64.39051628112793\n",
            "5000: train loss: 0.03153265599536673; test loss: 0.027461230754852295; train accuracy: 0.9900661912641114; test accuracy: 0.9914000034332275; Time: 65.67558145523071\n",
            "5100: train loss: 0.033077932234159994; test loss: 0.024655092507600784; train accuracy: 0.9899113073976947; test accuracy: 0.9926999807357788; Time: 66.95739984512329\n",
            "5200: train loss: 0.03287736433372608; test loss: 0.02729063667356968; train accuracy: 0.9897337445537773; test accuracy: 0.9919999837875366; Time: 68.24450731277466\n",
            "5300: train loss: 0.036533720596663315; test loss: 0.021814245730638504; train accuracy: 0.9882726271418597; test accuracy: 0.9929999709129333; Time: 69.53339838981628\n",
            "5400: train loss: 0.034167748999418164; test loss: 0.0354619063436985; train accuracy: 0.9892186101543082; test accuracy: 0.9886000156402588; Time: 70.81993246078491\n",
            "5500: train loss: 0.03232899880892776; test loss: 0.02076621726155281; train accuracy: 0.989246317168129; test accuracy: 0.9937000274658203; Time: 72.11303615570068\n",
            "5600: train loss: 0.031036656119124022; test loss: 0.024055637419223785; train accuracy: 0.9896480161219131; test accuracy: 0.9921000003814697; Time: 73.40228414535522\n",
            "5700: train loss: 0.032513097584083134; test loss: 0.02602158859372139; train accuracy: 0.9899078685572307; test accuracy: 0.9916999936103821; Time: 74.68395018577576\n",
            "5800: train loss: 0.03223539076165046; test loss: 0.024350393563508987; train accuracy: 0.9891997609498019; test accuracy: 0.9927999973297119; Time: 75.97323179244995\n",
            "5900: train loss: 0.03199300192241497; test loss: 0.03163328766822815; train accuracy: 0.9896967159095382; test accuracy: 0.9908999800682068; Time: 77.24328517913818\n",
            "6000: train loss: 0.03360092585982516; test loss: 0.026359371840953827; train accuracy: 0.9897949497995049; test accuracy: 0.9922999739646912; Time: 78.52066802978516\n",
            "6100: train loss: 0.03480752041957934; test loss: 0.030390625819563866; train accuracy: 0.9898087330997598; test accuracy: 0.9912999868392944; Time: 79.82446026802063\n",
            "6200: train loss: 0.03098889192262768; test loss: 0.02724299393594265; train accuracy: 0.9904012317283974; test accuracy: 0.9925000071525574; Time: 81.12019181251526\n",
            "6300: train loss: 0.032810939543683436; test loss: 0.03896074369549751; train accuracy: 0.9908671400066472; test accuracy: 0.9887999892234802; Time: 82.41054892539978\n",
            "6400: train loss: 0.03219368933532774; test loss: 0.031396783888339996; train accuracy: 0.9904691494150303; test accuracy: 0.9911999702453613; Time: 83.70282483100891\n",
            "6500: train loss: 0.03099328507358146; test loss: 0.024342915043234825; train accuracy: 0.9908833134484175; test accuracy: 0.9929999709129333; Time: 84.99027490615845\n",
            "6600: train loss: 0.029602661824804533; test loss: 0.02650284767150879; train accuracy: 0.9910615016671418; test accuracy: 0.9925000071525574; Time: 86.25308799743652\n",
            "6700: train loss: 0.02812301625580353; test loss: 0.024309173226356506; train accuracy: 0.9917149148209943; test accuracy: 0.9922999739646912; Time: 87.55097770690918\n",
            "6800: train loss: 0.026894049724703623; test loss: 0.027687689289450645; train accuracy: 0.9917486329957459; test accuracy: 0.9916999936103821; Time: 88.85093331336975\n",
            "6900: train loss: 0.02968400456719462; test loss: 0.031165802851319313; train accuracy: 0.9909359575597104; test accuracy: 0.9912999868392944; Time: 90.14137673377991\n",
            "7000: train loss: 0.025395560036543096; test loss: 0.02704201079905033; train accuracy: 0.9920906452700364; test accuracy: 0.9909999966621399; Time: 91.43078327178955\n",
            "7100: train loss: 0.030308878341692744; test loss: 0.023441951721906662; train accuracy: 0.990548865025982; test accuracy: 0.9922999739646912; Time: 92.72546720504761\n",
            "7200: train loss: 0.027885881833163897; test loss: 0.02609085664153099; train accuracy: 0.9915406958991039; test accuracy: 0.992900013923645; Time: 94.01675176620483\n",
            "7300: train loss: 0.02605951300595631; test loss: 0.02736186608672142; train accuracy: 0.9913748130535504; test accuracy: 0.9919999837875366; Time: 95.30390000343323\n",
            "7400: train loss: 0.026606650335476125; test loss: 0.023485856130719185; train accuracy: 0.9915948994481442; test accuracy: 0.9921000003814697; Time: 96.59416675567627\n",
            "7500: train loss: 0.024054003948664044; test loss: 0.02748013287782669; train accuracy: 0.9921767567089329; test accuracy: 0.9921000003814697; Time: 97.88291835784912\n",
            "7600: train loss: 0.027157192036359776; test loss: 0.03905344754457474; train accuracy: 0.9914006401496656; test accuracy: 0.9886999726295471; Time: 99.16684746742249\n",
            "7700: train loss: 0.02632504970422198; test loss: 0.0236964114010334; train accuracy: 0.9917669811558881; test accuracy: 0.9941999912261963; Time: 100.45896911621094\n",
            "7800: train loss: 0.02821044575971098; test loss: 0.02901093102991581; train accuracy: 0.9913681468072942; test accuracy: 0.9919000267982483; Time: 101.75037884712219\n",
            "7900: train loss: 0.025587560337554743; test loss: 0.025706565007567406; train accuracy: 0.9919557677797097; test accuracy: 0.993399977684021; Time: 103.03344202041626\n",
            "8000: train loss: 0.029193365885308502; test loss: 0.032869886606931686; train accuracy: 0.9911953006630725; test accuracy: 0.9908999800682068; Time: 104.3193929195404\n",
            "8100: train loss: 0.030300271158013956; test loss: 0.024448709562420845; train accuracy: 0.9910178639774475; test accuracy: 0.9927999973297119; Time: 105.6051697731018\n",
            "8200: train loss: 0.03059451115925968; test loss: 0.024276627227663994; train accuracy: 0.9909904352576192; test accuracy: 0.9926000237464905; Time: 106.89456391334534\n",
            "8300: train loss: 0.026422988655579058; test loss: 0.02657485008239746; train accuracy: 0.9914854156454626; test accuracy: 0.9927999973297119; Time: 108.18148279190063\n",
            "8400: train loss: 0.025367416461989267; test loss: 0.023465726524591446; train accuracy: 0.9922642025526451; test accuracy: 0.992900013923645; Time: 109.47182631492615\n",
            "8500: train loss: 0.02277002410658725; test loss: 0.02030719444155693; train accuracy: 0.9922999072569418; test accuracy: 0.9940999746322632; Time: 110.76115036010742\n",
            "8600: train loss: 0.023301579063397724; test loss: 0.0495326966047287; train accuracy: 0.9922221681916313; test accuracy: 0.9854999780654907; Time: 112.04800200462341\n",
            "8700: train loss: 0.025861796930858855; test loss: 0.026320818811655045; train accuracy: 0.9914062471292084; test accuracy: 0.9923999905586243; Time: 113.3316125869751\n",
            "8800: train loss: 0.027565345575828362; test loss: 0.027504047378897667; train accuracy: 0.9912075188084218; test accuracy: 0.9925000071525574; Time: 114.6122179031372\n",
            "8900: train loss: 0.026703367571043392; test loss: 0.027092717587947845; train accuracy: 0.9916447841330968; test accuracy: 0.9922000169754028; Time: 115.90119457244873\n",
            "9000: train loss: 0.024043550541328566; test loss: 0.025318149477243423; train accuracy: 0.9929419779894274; test accuracy: 0.9939000010490417; Time: 117.1888017654419\n",
            "9100: train loss: 0.02780362692314906; test loss: 0.039515092968940735; train accuracy: 0.9916931650325614; test accuracy: 0.989300012588501; Time: 118.47588539123535\n",
            "9200: train loss: 0.027930274802348835; test loss: 0.02634447254240513; train accuracy: 0.9919690434444577; test accuracy: 0.9927999973297119; Time: 119.75614523887634\n",
            "9300: train loss: 0.024053823504761148; test loss: 0.027366165071725845; train accuracy: 0.9927540706659829; test accuracy: 0.9936000108718872; Time: 121.04988670349121\n",
            "9400: train loss: 0.022553976627929465; test loss: 0.03110707737505436; train accuracy: 0.9932282559364868; test accuracy: 0.9926000237464905; Time: 122.32850885391235\n",
            "9500: train loss: 0.024925733508246044; test loss: 0.027126403525471687; train accuracy: 0.9919818418817666; test accuracy: 0.9914000034332275; Time: 123.60654377937317\n",
            "9600: train loss: 0.029292057177606374; test loss: 0.02895256318151951; train accuracy: 0.9914363568745751; test accuracy: 0.9922999739646912; Time: 124.89658689498901\n",
            "9700: train loss: 0.025661883243127195; test loss: 0.030568616464734077; train accuracy: 0.9923788922407425; test accuracy: 0.991599977016449; Time: 126.19353413581848\n",
            "9800: train loss: 0.024372001048588214; test loss: 0.02474484033882618; train accuracy: 0.9927423446003004; test accuracy: 0.9926999807357788; Time: 127.48298668861389\n",
            "9900: train loss: 0.021650621912393996; test loss: 0.02678586356341839; train accuracy: 0.993218428509106; test accuracy: 0.9922999739646912; Time: 128.76715064048767\n",
            "[[ 974    0    1    1    0    1    1    2    0    0]\n",
            " [   0 1132    1    2    0    0    0    0    0    0]\n",
            " [   1    0 1026    2    0    0    0    3    0    0]\n",
            " [   0    0    0 1010    0    0    0    0    0    0]\n",
            " [   0    1    1    0  974    0    1    1    1    3]\n",
            " [   0    0    0   11    0  880    1    0    0    0]\n",
            " [   1    1    2    1    0    1  947    0    5    0]\n",
            " [   0    5    2    2    0    0    0 1017    0    2]\n",
            " [   2    0    4    3    0    2    0    0  963    0]\n",
            " [   0    0    0    2    5    1    0    1    3  997]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9959    0.9939    0.9949       980\n",
            "           1     0.9939    0.9974    0.9956      1135\n",
            "           2     0.9894    0.9942    0.9918      1032\n",
            "           3     0.9768    1.0000    0.9883      1010\n",
            "           4     0.9949    0.9919    0.9934       982\n",
            "           5     0.9944    0.9865    0.9904       892\n",
            "           6     0.9968    0.9885    0.9927       958\n",
            "           7     0.9932    0.9893    0.9912      1028\n",
            "           8     0.9907    0.9887    0.9897       974\n",
            "           9     0.9950    0.9881    0.9915      1009\n",
            "\n",
            "    accuracy                         0.9920     10000\n",
            "   macro avg     0.9921    0.9918    0.9920     10000\n",
            "weighted avg     0.9920    0.9920    0.9920     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd0I8_5nS5yj",
        "colab_type": "text"
      },
      "source": [
        "## KRON KRON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfYlpBFjQtDP",
        "colab_type": "code",
        "outputId": "1bd389d4-a664-4e13-d9ea-78ec98acbdc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PSGD with Kronecker Product Preconditioning\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    rows = [W.shape.as_list()[0] for W in Ws]\n",
        "    cols = [W.shape.as_list()[1] for W in Ws] \n",
        "    \n",
        "    Qs_left = [tf.Variable(tf.eye(row, dtype=dtype), trainable=False) for row in rows]\n",
        "    Qs_right = [tf.Variable(tf.eye(col, dtype=dtype), trainable=False) for col in cols]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "    \n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, grads)]\n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, precond_grads)]\n",
        "\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    new_Ws = [W - (step_size_adjust*step_size)*g for (W, g) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "\n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    delta_grads = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, delta_grads)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "    \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_approx, qs_r_kron_approx = sess.run([Qs_left, Qs_right])\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "\n",
        "scipy.io.savemat(results_dir + 'kron_kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.352512836456299; test loss: 2.29834246635437; train accuracy: 0.0546875; test accuracy: 0.1054999977350235; Time: 1.598461389541626\n",
            "100: train loss: 0.7497566517174246; test loss: 0.08409646898508072; train accuracy: 0.7698203125000002; test accuracy: 0.975600004196167; Time: 3.179511070251465\n",
            "200: train loss: 0.34829148347527183; test loss: 0.04525052011013031; train accuracy: 0.8942035265331466; test accuracy: 0.9878000020980835; Time: 4.745014667510986\n",
            "300: train loss: 0.17308768155725332; test loss: 0.03595742583274841; train accuracy: 0.9481247616003815; test accuracy: 0.9890000224113464; Time: 6.313241958618164\n",
            "400: train loss: 0.09967959506637074; test loss: 0.028861716389656067; train accuracy: 0.9704332806861677; test accuracy: 0.9912999868392944; Time: 7.882060766220093\n",
            "500: train loss: 0.06535659670327855; test loss: 0.02634241245687008; train accuracy: 0.981119344067093; test accuracy: 0.9915000200271606; Time: 9.453634977340698\n",
            "600: train loss: 0.04931669948532111; test loss: 0.02331351675093174; train accuracy: 0.9852174173012928; test accuracy: 0.9926999807357788; Time: 11.027655363082886\n",
            "700: train loss: 0.03924292415107839; test loss: 0.023095542564988136; train accuracy: 0.9887195910319112; test accuracy: 0.991599977016449; Time: 12.599440097808838\n",
            "800: train loss: 0.0349930331440872; test loss: 0.01985456608235836; train accuracy: 0.9894367985419457; test accuracy: 0.9930999875068665; Time: 14.193305730819702\n",
            "900: train loss: 0.03266554888861951; test loss: 0.021026868373155594; train accuracy: 0.9905103466600053; test accuracy: 0.9929999709129333; Time: 15.793203115463257\n",
            "1000: train loss: 0.027681511276968277; test loss: 0.020357651636004448; train accuracy: 0.9922911280952855; test accuracy: 0.9934999942779541; Time: 17.394624948501587\n",
            "1100: train loss: 0.028844142092963043; test loss: 0.01966988854110241; train accuracy: 0.9914286572758738; test accuracy: 0.9936000108718872; Time: 18.994766235351562\n",
            "1200: train loss: 0.025792941992109172; test loss: 0.01894071325659752; train accuracy: 0.9919374382201702; test accuracy: 0.9936000108718872; Time: 20.592419862747192\n",
            "1300: train loss: 0.024184593514782617; test loss: 0.018662527203559875; train accuracy: 0.9927243948679544; test accuracy: 0.9936000108718872; Time: 22.189568042755127\n",
            "1400: train loss: 0.021611101088405006; test loss: 0.01910063624382019; train accuracy: 0.993520751929706; test accuracy: 0.9925000071525574; Time: 23.756492853164673\n",
            "1500: train loss: 0.020835840865736255; test loss: 0.020422790199518204; train accuracy: 0.9942470256587498; test accuracy: 0.9925000071525574; Time: 25.319632291793823\n",
            "1600: train loss: 0.020676335725108706; test loss: 0.019505521282553673; train accuracy: 0.9942469324798948; test accuracy: 0.9929999709129333; Time: 26.888141870498657\n",
            "1700: train loss: 0.017750776120655242; test loss: 0.018584372475743294; train accuracy: 0.9943360593961501; test accuracy: 0.9927999973297119; Time: 28.457595825195312\n",
            "1800: train loss: 0.01806676457484629; test loss: 0.019380226731300354; train accuracy: 0.9939938142959371; test accuracy: 0.9937000274658203; Time: 30.02728581428528\n",
            "1900: train loss: 0.015005915638237463; test loss: 0.01956305466592312; train accuracy: 0.9956389155055853; test accuracy: 0.9937000274658203; Time: 31.5947368144989\n",
            "2000: train loss: 0.016260393015343672; test loss: 0.020430423319339752; train accuracy: 0.9952223873939646; test accuracy: 0.9934999942779541; Time: 33.16613817214966\n",
            "2100: train loss: 0.015071248777242637; test loss: 0.01952100172638893; train accuracy: 0.9955935058742263; test accuracy: 0.993399977684021; Time: 34.7306911945343\n",
            "2200: train loss: 0.013042828235268717; test loss: 0.018944930285215378; train accuracy: 0.9963106727799762; test accuracy: 0.9940999746322632; Time: 36.29906630516052\n",
            "2300: train loss: 0.012564050175135332; test loss: 0.020000800490379333; train accuracy: 0.9968257003375913; test accuracy: 0.9933000206947327; Time: 37.86868667602539\n",
            "2400: train loss: 0.014157620996553131; test loss: 0.020491063594818115; train accuracy: 0.9962316238772383; test accuracy: 0.9932000041007996; Time: 39.440059661865234\n",
            "2500: train loss: 0.014575829672220184; test loss: 0.019550053402781487; train accuracy: 0.9956433029459041; test accuracy: 0.9936000108718872; Time: 41.01790499687195\n",
            "2600: train loss: 0.012217922823472732; test loss: 0.02089313603937626; train accuracy: 0.9959685139212727; test accuracy: 0.9932000041007996; Time: 42.58764338493347\n",
            "2700: train loss: 0.011442258953134932; test loss: 0.019763553515076637; train accuracy: 0.9965458319889854; test accuracy: 0.9932000041007996; Time: 44.15438437461853\n",
            "2800: train loss: 0.011122783413531633; test loss: 0.020189452916383743; train accuracy: 0.9970660158726088; test accuracy: 0.993399977684021; Time: 45.7254753112793\n",
            "2900: train loss: 0.010742897243181436; test loss: 0.019424183294177055; train accuracy: 0.997074237486455; test accuracy: 0.993399977684021; Time: 47.29011130332947\n",
            "3000: train loss: 0.013422821211475195; test loss: 0.018903719261288643; train accuracy: 0.9961583510574009; test accuracy: 0.9937999844551086; Time: 48.85523748397827\n",
            "3100: train loss: 0.010250816452504544; test loss: 0.01907171867787838; train accuracy: 0.9967645228822022; test accuracy: 0.9940999746322632; Time: 50.42832970619202\n",
            "3200: train loss: 0.00907028915677485; test loss: 0.01939631626009941; train accuracy: 0.9973109798502997; test accuracy: 0.9940999746322632; Time: 52.00031781196594\n",
            "3300: train loss: 0.009024271624875933; test loss: 0.019067009910941124; train accuracy: 0.9972771140422976; test accuracy: 0.9940999746322632; Time: 53.56629776954651\n",
            "3400: train loss: 0.010692314440965632; test loss: 0.019470050930976868; train accuracy: 0.996758109344286; test accuracy: 0.9940000176429749; Time: 55.12843871116638\n",
            "3500: train loss: 0.010789266100042882; test loss: 0.019776146858930588; train accuracy: 0.9971718765625663; test accuracy: 0.9941999912261963; Time: 56.68148183822632\n",
            "3600: train loss: 0.010195289378894764; test loss: 0.019643863663077354; train accuracy: 0.9972669561690418; test accuracy: 0.994700014591217; Time: 58.22648787498474\n",
            "3700: train loss: 0.00924887301164556; test loss: 0.019094910472631454; train accuracy: 0.9974945449600392; test accuracy: 0.994700014591217; Time: 59.79645824432373\n",
            "3800: train loss: 0.00778994279618767; test loss: 0.019554367288947105; train accuracy: 0.9978476350023552; test accuracy: 0.9944000244140625; Time: 61.365232706069946\n",
            "3900: train loss: 0.007583976507544033; test loss: 0.019668418914079666; train accuracy: 0.9977263534609403; test accuracy: 0.9944999814033508; Time: 62.930744886398315\n",
            "4000: train loss: 0.006989679055411163; test loss: 0.019282951951026917; train accuracy: 0.9980658839658412; test accuracy: 0.9941999912261963; Time: 64.49765706062317\n",
            "4100: train loss: 0.008169193832764671; test loss: 0.019594460725784302; train accuracy: 0.9981186923081884; test accuracy: 0.9945999979972839; Time: 66.05709195137024\n",
            "4200: train loss: 0.0070491763220983895; test loss: 0.020999211817979813; train accuracy: 0.9980442272898625; test accuracy: 0.9940000176429749; Time: 67.61583113670349\n",
            "4300: train loss: 0.007041243734666202; test loss: 0.019763091579079628; train accuracy: 0.9977852407585417; test accuracy: 0.994700014591217; Time: 69.18682932853699\n",
            "4400: train loss: 0.007283832171358804; test loss: 0.020714864134788513; train accuracy: 0.99812265859931; test accuracy: 0.9939000010490417; Time: 70.75673818588257\n",
            "4500: train loss: 0.006572361184195993; test loss: 0.020375506952404976; train accuracy: 0.9979435607508096; test accuracy: 0.9939000010490417; Time: 72.32769131660461\n",
            "4600: train loss: 0.005361136535333868; test loss: 0.021243564784526825; train accuracy: 0.9984458552409643; test accuracy: 0.9943000078201294; Time: 73.90646409988403\n",
            "4700: train loss: 0.004919344786574804; test loss: 0.020972756668925285; train accuracy: 0.9983444109792121; test accuracy: 0.9940999746322632; Time: 75.47415232658386\n",
            "4800: train loss: 0.005593551920946275; test loss: 0.02022853121161461; train accuracy: 0.9985797390054733; test accuracy: 0.9943000078201294; Time: 77.0408251285553\n",
            "4900: train loss: 0.005174330349677656; test loss: 0.019755052402615547; train accuracy: 0.9985660950967226; test accuracy: 0.9943000078201294; Time: 78.60702180862427\n",
            "5000: train loss: 0.004493271948290691; test loss: 0.020489182323217392; train accuracy: 0.9985771967276291; test accuracy: 0.9940000176429749; Time: 80.1773829460144\n",
            "5100: train loss: 0.00631712558682235; test loss: 0.020605145022273064; train accuracy: 0.9981550276259893; test accuracy: 0.9944999814033508; Time: 81.74647045135498\n",
            "5200: train loss: 0.005539307764070456; test loss: 0.02026006206870079; train accuracy: 0.9987587601927544; test accuracy: 0.9944999814033508; Time: 83.31508874893188\n",
            "5300: train loss: 0.004808706053515371; test loss: 0.020222077146172523; train accuracy: 0.9987810661269187; test accuracy: 0.994700014591217; Time: 84.88644671440125\n",
            "5400: train loss: 0.004605220945375094; test loss: 0.02006259746849537; train accuracy: 0.9988460793679231; test accuracy: 0.9945999979972839; Time: 86.45802927017212\n",
            "5500: train loss: 0.005033374890181184; test loss: 0.020969295874238014; train accuracy: 0.9986003800786845; test accuracy: 0.9944999814033508; Time: 88.03114557266235\n",
            "5600: train loss: 0.005161877875515495; test loss: 0.020446307957172394; train accuracy: 0.9985611355578226; test accuracy: 0.9947999715805054; Time: 89.59858703613281\n",
            "5700: train loss: 0.007025156260604668; test loss: 0.020569493994116783; train accuracy: 0.9987197173891313; test accuracy: 0.9943000078201294; Time: 91.16911292076111\n",
            "5800: train loss: 0.005543397293317522; test loss: 0.019991904497146606; train accuracy: 0.9986771250923403; test accuracy: 0.994700014591217; Time: 92.74209666252136\n",
            "5900: train loss: 0.006497527312361045; test loss: 0.0201991219073534; train accuracy: 0.9984675868215468; test accuracy: 0.9948999881744385; Time: 94.31266641616821\n",
            "6000: train loss: 0.0043371256854163485; test loss: 0.020050937309861183; train accuracy: 0.9988398171728002; test accuracy: 0.9947999715805054; Time: 95.88093757629395\n",
            "6100: train loss: 0.004261115016640073; test loss: 0.020446954295039177; train accuracy: 0.9985979608726276; test accuracy: 0.9941999912261963; Time: 97.453861951828\n",
            "6200: train loss: 0.004034846874557751; test loss: 0.020679190754890442; train accuracy: 0.9988287547079477; test accuracy: 0.9950000047683716; Time: 99.02480459213257\n",
            "6300: train loss: 0.004477639841179625; test loss: 0.020717225968837738; train accuracy: 0.9987471178191965; test accuracy: 0.9950000047683716; Time: 100.59447741508484\n",
            "6400: train loss: 0.0049207201979551115; test loss: 0.02130509912967682; train accuracy: 0.9988225442014438; test accuracy: 0.9945999979972839; Time: 102.16058397293091\n",
            "6500: train loss: 0.004480093279954499; test loss: 0.02111785300076008; train accuracy: 0.9989313414057882; test accuracy: 0.995199978351593; Time: 103.73316812515259\n",
            "6600: train loss: 0.004301994433435283; test loss: 0.021470049396157265; train accuracy: 0.9989803122486496; test accuracy: 0.9950000047683716; Time: 105.30163645744324\n",
            "6700: train loss: 0.0037599981427753294; test loss: 0.02181648649275303; train accuracy: 0.9990842113100036; test accuracy: 0.9951000213623047; Time: 106.87519025802612\n",
            "6800: train loss: 0.00315082056559283; test loss: 0.02155975066125393; train accuracy: 0.9989994766621919; test accuracy: 0.9950000047683716; Time: 108.44382047653198\n",
            "6900: train loss: 0.002387756107221385; test loss: 0.021875543519854546; train accuracy: 0.9993647877409169; test accuracy: 0.9944000244140625; Time: 110.01375389099121\n",
            "7000: train loss: 0.004627749330280906; test loss: 0.021588927134871483; train accuracy: 0.9988998806337913; test accuracy: 0.9947999715805054; Time: 111.5809690952301\n",
            "7100: train loss: 0.004047965584087299; test loss: 0.021637091413140297; train accuracy: 0.99857131660212; test accuracy: 0.9941999912261963; Time: 113.14900207519531\n",
            "7200: train loss: 0.003076121333075929; test loss: 0.022350847721099854; train accuracy: 0.9989812377694038; test accuracy: 0.9940999746322632; Time: 114.7187020778656\n",
            "7300: train loss: 0.0030600796891308087; test loss: 0.022137420251965523; train accuracy: 0.9991346423551244; test accuracy: 0.9941999912261963; Time: 116.28307557106018\n",
            "7400: train loss: 0.0045346595482788415; test loss: 0.02271472103893757; train accuracy: 0.9991027787384097; test accuracy: 0.9943000078201294; Time: 117.85438871383667\n",
            "7500: train loss: 0.003717944108674193; test loss: 0.02260906994342804; train accuracy: 0.9990366657834548; test accuracy: 0.9940000176429749; Time: 119.42424941062927\n",
            "7600: train loss: 0.0041911501167941735; test loss: 0.022346889600157738; train accuracy: 0.9988571648846293; test accuracy: 0.9941999912261963; Time: 120.98876738548279\n",
            "7700: train loss: 0.0033545666155300508; test loss: 0.022538593038916588; train accuracy: 0.9990519514935032; test accuracy: 0.9943000078201294; Time: 122.55755805969238\n",
            "7800: train loss: 0.0032574306014233967; test loss: 0.022598538547754288; train accuracy: 0.9990041113512532; test accuracy: 0.9941999912261963; Time: 124.12578797340393\n",
            "7900: train loss: 0.003619851403413367; test loss: 0.022555211558938026; train accuracy: 0.9987779718722561; test accuracy: 0.9943000078201294; Time: 125.6873893737793\n",
            "8000: train loss: 0.0031354228663105404; test loss: 0.022980093955993652; train accuracy: 0.9988283995780141; test accuracy: 0.9944000244140625; Time: 127.25186681747437\n",
            "8100: train loss: 0.0024038209871591254; test loss: 0.023364748805761337; train accuracy: 0.9991919316293348; test accuracy: 0.9943000078201294; Time: 128.8093764781952\n",
            "8200: train loss: 0.003693693271273378; test loss: 0.02323310449719429; train accuracy: 0.998903187645237; test accuracy: 0.9944000244140625; Time: 130.37451028823853\n",
            "8300: train loss: 0.0029950364626442493; test loss: 0.02252189628779888; train accuracy: 0.9992207408192556; test accuracy: 0.9943000078201294; Time: 131.93483662605286\n",
            "8400: train loss: 0.0034094688387043414; test loss: 0.02297351136803627; train accuracy: 0.9992373437238088; test accuracy: 0.9945999979972839; Time: 133.49484086036682\n",
            "8500: train loss: 0.0026836853410436806; test loss: 0.023178651928901672; train accuracy: 0.9994936563923644; test accuracy: 0.9941999912261963; Time: 135.05396914482117\n",
            "8600: train loss: 0.0038128000920472443; test loss: 0.023369286209344864; train accuracy: 0.9992338997944121; test accuracy: 0.9940000176429749; Time: 136.61941599845886\n",
            "8700: train loss: 0.0029522371248026876; test loss: 0.023549115285277367; train accuracy: 0.9991570472805873; test accuracy: 0.9939000010490417; Time: 138.17994785308838\n",
            "8800: train loss: 0.004582501151947311; test loss: 0.023868178948760033; train accuracy: 0.998956595527273; test accuracy: 0.9940999746322632; Time: 139.7413308620453\n",
            "8900: train loss: 0.0022095341430703637; test loss: 0.023803872987627983; train accuracy: 0.9995258645291156; test accuracy: 0.9943000078201294; Time: 141.30523824691772\n",
            "9000: train loss: 0.001964329927652968; test loss: 0.02372821420431137; train accuracy: 0.9995155668064112; test accuracy: 0.9943000078201294; Time: 142.863352060318\n",
            "9100: train loss: 0.0020142476286613285; test loss: 0.023740941658616066; train accuracy: 0.9994386505924875; test accuracy: 0.9940999746322632; Time: 144.40868210792542\n",
            "9200: train loss: 0.0023543389827657595; test loss: 0.023986877873539925; train accuracy: 0.9991634275652007; test accuracy: 0.9940999746322632; Time: 145.959463596344\n",
            "9300: train loss: 0.0022320987641001305; test loss: 0.02437257580459118; train accuracy: 0.999181332354496; test accuracy: 0.9939000010490417; Time: 147.52964806556702\n",
            "9400: train loss: 0.0037365284615000285; test loss: 0.02484707348048687; train accuracy: 0.9992557745441634; test accuracy: 0.9941999912261963; Time: 149.09692811965942\n",
            "9500: train loss: 0.0026849097255326098; test loss: 0.024960851296782494; train accuracy: 0.9993251218824114; test accuracy: 0.9940999746322632; Time: 150.6632595062256\n",
            "9600: train loss: 0.00618894650079624; test loss: 0.025122523307800293; train accuracy: 0.9988893256545389; test accuracy: 0.9939000010490417; Time: 152.23023080825806\n",
            "9700: train loss: 0.0059225393477778795; test loss: 0.025523046031594276; train accuracy: 0.9987457117417574; test accuracy: 0.9939000010490417; Time: 153.77455687522888\n",
            "9800: train loss: 0.0045903434958177005; test loss: 0.025325484573841095; train accuracy: 0.9991027673717092; test accuracy: 0.9940000176429749; Time: 155.3509132862091\n",
            "9900: train loss: 0.004150933896506336; test loss: 0.025061383843421936; train accuracy: 0.9992023218300828; test accuracy: 0.9937999844551086; Time: 156.9161295890808\n",
            "[[ 978    0    0    0    0    0    2    0    0    0]\n",
            " [   0 1132    1    1    0    0    0    1    0    0]\n",
            " [   1    0 1029    0    1    0    0    1    0    0]\n",
            " [   0    0    0 1008    0    1    0    1    0    0]\n",
            " [   0    0    0    0  978    0    0    1    1    2]\n",
            " [   0    0    0    4    0  887    1    0    0    0]\n",
            " [   4    1    1    0    0    3  948    0    1    0]\n",
            " [   0    3    6    0    0    0    0 1018    0    1]\n",
            " [   0    0    0    2    2    0    0    0  969    1]\n",
            " [   0    0    0    0    6    3    0    4    3  993]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9949    0.9980    0.9964       980\n",
            "           1     0.9965    0.9974    0.9969      1135\n",
            "           2     0.9923    0.9971    0.9947      1032\n",
            "           3     0.9931    0.9980    0.9956      1010\n",
            "           4     0.9909    0.9959    0.9934       982\n",
            "           5     0.9922    0.9944    0.9933       892\n",
            "           6     0.9968    0.9896    0.9932       958\n",
            "           7     0.9922    0.9903    0.9912      1028\n",
            "           8     0.9949    0.9949    0.9949       974\n",
            "           9     0.9960    0.9841    0.9900      1009\n",
            "\n",
            "    accuracy                         0.9940     10000\n",
            "   macro avg     0.9940    0.9940    0.9940     10000\n",
            "weighted avg     0.9940    0.9940    0.9940     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1IUSC6IxrgX",
        "colab_type": "text"
      },
      "source": [
        "# ADAM KRON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrxmFiTPXes",
        "colab_type": "text"
      },
      "source": [
        "##  ADAM --> Kronecker Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnW95XXHPZOL",
        "colab_type": "code",
        "outputId": "bf02e14d-c298-4a9f-9178-17344e94f640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# apply adam on grads then computing preconditioner from the adam_grads\n",
        "# grads ---> adam_grads ---> precond_grads\n",
        "\n",
        "step_size = 0.02\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    grads_vars = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "    grads_moment = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "\n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    new_grads_moment = [beta1*old + (1.0 - beta1)*new for (old, new) in zip(grads_moment, grads)]\n",
        "    new_grads_vars = [beta2*old + (1.0 - beta2)*new*new for (old, new) in zip(grads_vars, grads)]\n",
        "    new_grads_moment_hat = [m/(1-beta1**adam_step) for m in new_grads_moment]\n",
        "    new_grads_vars_hat = [v/(1-beta2**adam_step) for v in new_grads_vars]\n",
        "\n",
        "    adam_grads = [m/tf.sqrt(v + offset) for (m,v) in zip(new_grads_moment_hat,new_grads_vars_hat)]\n",
        "\n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, adam_grads)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws,precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    update_grads_vars = [tf.assign(old, new) for (old, new) in zip(grads_vars, new_grads_vars)]\n",
        "    update_grads_moment = [tf.assign(old, new) for (old, new) in zip(grads_moment, new_grads_moment)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)     \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    t = 0\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "        t = t + 1\n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _,_,_ = sess.run([train_loss, train_accuracy, update_Ws, update_grads_moment, update_grads_vars, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs, adam_step: t})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'adam_kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3422911167144775; test loss: 2.2271106243133545; train accuracy: 0.1328125; test accuracy: 0.2410999983549118; Time: 2.1239004135131836\n",
            "100: train loss: 0.5537339011058204; test loss: 0.07450881600379944; train accuracy: 0.8259718749999998; test accuracy: 0.9753999710083008; Time: 3.7569997310638428\n",
            "200: train loss: 0.25980328081247483; test loss: 0.03505900129675865; train accuracy: 0.9186728491064133; test accuracy: 0.9883999824523926; Time: 5.36476469039917\n",
            "300: train loss: 0.1368506157169717; test loss: 0.029348034411668777; train accuracy: 0.9576611711868063; test accuracy: 0.9912999868392944; Time: 6.960961103439331\n",
            "400: train loss: 0.0810309802723222; test loss: 0.03540801256895065; train accuracy: 0.9748706050655941; test accuracy: 0.9886999726295471; Time: 8.582138538360596\n",
            "500: train loss: 0.054930329053497574; test loss: 0.028640570119023323; train accuracy: 0.9827787075883204; test accuracy: 0.9900000095367432; Time: 10.199953079223633\n",
            "600: train loss: 0.04600670703315655; test loss: 0.029681390151381493; train accuracy: 0.9860455456952429; test accuracy: 0.9898999929428101; Time: 11.816858291625977\n",
            "700: train loss: 0.039736081569195114; test loss: 0.026022516191005707; train accuracy: 0.9883372883896362; test accuracy: 0.9912999868392944; Time: 13.436254739761353\n",
            "800: train loss: 0.03673650315440106; test loss: 0.027199065312743187; train accuracy: 0.9891972222931736; test accuracy: 0.9918000102043152; Time: 15.036362171173096\n",
            "900: train loss: 0.03263904466795381; test loss: 0.026287438347935677; train accuracy: 0.9896515798687022; test accuracy: 0.991599977016449; Time: 16.660921096801758\n",
            "1000: train loss: 0.031443879427781866; test loss: 0.031727295368909836; train accuracy: 0.9897866033628191; test accuracy: 0.9897000193595886; Time: 18.279751539230347\n",
            "1100: train loss: 0.03134819408188462; test loss: 0.029230032116174698; train accuracy: 0.9894521047433675; test accuracy: 0.9901000261306763; Time: 19.900912523269653\n",
            "1200: train loss: 0.029949616538406533; test loss: 0.023013511672616005; train accuracy: 0.9902165607342239; test accuracy: 0.992900013923645; Time: 21.52095055580139\n",
            "1300: train loss: 0.02868512468249333; test loss: 0.028557397425174713; train accuracy: 0.9910214034827153; test accuracy: 0.9911999702453613; Time: 23.148468494415283\n",
            "1400: train loss: 0.02852756683715352; test loss: 0.024334393441677094; train accuracy: 0.9911944436865332; test accuracy: 0.9926999807357788; Time: 24.779178857803345\n",
            "1500: train loss: 0.02612280102863945; test loss: 0.02107025682926178; train accuracy: 0.9919905814449929; test accuracy: 0.9929999709129333; Time: 26.406322240829468\n",
            "1600: train loss: 0.030142021754150965; test loss: 0.023805605247616768; train accuracy: 0.9910009455480108; test accuracy: 0.992900013923645; Time: 28.033371925354004\n",
            "1700: train loss: 0.02971795412185463; test loss: 0.028139937669038773; train accuracy: 0.990926637669876; test accuracy: 0.9915000200271606; Time: 29.651814222335815\n",
            "1800: train loss: 0.026036398959816118; test loss: 0.02344302088022232; train accuracy: 0.9917849259922731; test accuracy: 0.992900013923645; Time: 31.2664532661438\n",
            "1900: train loss: 0.02377550251627056; test loss: 0.028278237208724022; train accuracy: 0.9923626526118434; test accuracy: 0.9918000102043152; Time: 32.88005304336548\n",
            "2000: train loss: 0.024082890736589653; test loss: 0.027874194085597992; train accuracy: 0.9919709665458125; test accuracy: 0.9922999739646912; Time: 34.497058153152466\n",
            "2100: train loss: 0.02377492057566896; test loss: 0.02576792612671852; train accuracy: 0.9920621671350983; test accuracy: 0.9922999739646912; Time: 36.11598443984985\n",
            "2200: train loss: 0.022842444883925835; test loss: 0.026326416060328484; train accuracy: 0.9923818084642578; test accuracy: 0.9916999936103821; Time: 37.73648142814636\n",
            "2300: train loss: 0.02321870019282749; test loss: 0.02580914832651615; train accuracy: 0.9923281155154037; test accuracy: 0.9929999709129333; Time: 39.352017402648926\n",
            "2400: train loss: 0.02197284421598377; test loss: 0.02831445261836052; train accuracy: 0.9925226772781998; test accuracy: 0.9908999800682068; Time: 40.96989035606384\n",
            "2500: train loss: 0.02002368490285683; test loss: 0.0287445317953825; train accuracy: 0.9933171662168719; test accuracy: 0.9919000267982483; Time: 42.587461709976196\n",
            "2600: train loss: 0.022816610568486904; test loss: 0.022480681538581848; train accuracy: 0.9930037144927563; test accuracy: 0.9927999973297119; Time: 44.20313334465027\n",
            "2700: train loss: 0.022173108251616683; test loss: 0.027163119986653328; train accuracy: 0.9928887289820503; test accuracy: 0.9927999973297119; Time: 45.8189582824707\n",
            "2800: train loss: 0.023366660538897414; test loss: 0.029365643858909607; train accuracy: 0.9923268501761893; test accuracy: 0.9922000169754028; Time: 47.4323832988739\n",
            "2900: train loss: 0.022395745181750203; test loss: 0.02877039834856987; train accuracy: 0.9930885167036595; test accuracy: 0.991599977016449; Time: 49.05299472808838\n",
            "3000: train loss: 0.024753831523517553; test loss: 0.033237144351005554; train accuracy: 0.9917763452339792; test accuracy: 0.9908000230789185; Time: 50.668009757995605\n",
            "3100: train loss: 0.021381280583900427; test loss: 0.034067023545503616; train accuracy: 0.9928417148726906; test accuracy: 0.9901000261306763; Time: 52.281397104263306\n",
            "3200: train loss: 0.02197361355173714; test loss: 0.0321999192237854; train accuracy: 0.9932503720244706; test accuracy: 0.9904999732971191; Time: 53.896310567855835\n",
            "3300: train loss: 0.019170111739400707; test loss: 0.026406606659293175; train accuracy: 0.9940797349490492; test accuracy: 0.992900013923645; Time: 55.51230239868164\n",
            "3400: train loss: 0.01914675303750186; test loss: 0.028508948162198067; train accuracy: 0.9939325176206384; test accuracy: 0.991599977016449; Time: 57.129314661026\n",
            "3500: train loss: 0.01672825766918385; test loss: 0.034414954483509064; train accuracy: 0.994082899295986; test accuracy: 0.9912999868392944; Time: 58.74722909927368\n",
            "3600: train loss: 0.01771537241370051; test loss: 0.028738439083099365; train accuracy: 0.9939768761491392; test accuracy: 0.9916999936103821; Time: 60.3752863407135\n",
            "3700: train loss: 0.018127383178102155; test loss: 0.03193908557295799; train accuracy: 0.9941826134235076; test accuracy: 0.9911999702453613; Time: 62.01557970046997\n",
            "3800: train loss: 0.01829320518893569; test loss: 0.02707049623131752; train accuracy: 0.9945053491287366; test accuracy: 0.9916999936103821; Time: 63.65899658203125\n",
            "3900: train loss: 0.021479620884243666; test loss: 0.03221117705106735; train accuracy: 0.9933760260940969; test accuracy: 0.9919000267982483; Time: 65.30519652366638\n",
            "4000: train loss: 0.02019167327407408; test loss: 0.03086995892226696; train accuracy: 0.9932936876585836; test accuracy: 0.9922999739646912; Time: 66.94663286209106\n",
            "4100: train loss: 0.021753506643174936; test loss: 0.028511999174952507; train accuracy: 0.9934773159592543; test accuracy: 0.9925000071525574; Time: 68.59072375297546\n",
            "4200: train loss: 0.01733207357561735; test loss: 0.03263438120484352; train accuracy: 0.9942323268147587; test accuracy: 0.9919999837875366; Time: 70.21690559387207\n",
            "4300: train loss: 0.018596000007325437; test loss: 0.03196917846798897; train accuracy: 0.9943414359082504; test accuracy: 0.9919000267982483; Time: 71.84117197990417\n",
            "4400: train loss: 0.019257875029080444; test loss: 0.034112732857465744; train accuracy: 0.9942584849919274; test accuracy: 0.9911999702453613; Time: 73.46278929710388\n",
            "4500: train loss: 0.017251471048096583; test loss: 0.03754523769021034; train accuracy: 0.994589267138724; test accuracy: 0.9907000064849854; Time: 75.07993626594543\n",
            "4600: train loss: 0.019661544320351092; test loss: 0.029945464804768562; train accuracy: 0.9941082529397571; test accuracy: 0.9925000071525574; Time: 76.700936794281\n",
            "4700: train loss: 0.01747907745453351; test loss: 0.03421581909060478; train accuracy: 0.9947191007357615; test accuracy: 0.9912999868392944; Time: 78.3193531036377\n",
            "4800: train loss: 0.016393460257660898; test loss: 0.03530967980623245; train accuracy: 0.9949636716638598; test accuracy: 0.991599977016449; Time: 79.9375433921814\n",
            "4900: train loss: 0.017598622292457337; test loss: 0.028951724991202354; train accuracy: 0.9952614716973174; test accuracy: 0.9925000071525574; Time: 81.55629467964172\n",
            "5000: train loss: 0.013779793554524557; test loss: 0.029703455045819283; train accuracy: 0.9956474194096245; test accuracy: 0.992900013923645; Time: 83.16934657096863\n",
            "5100: train loss: 0.017176272548948473; test loss: 0.03415617346763611; train accuracy: 0.9947564610835161; test accuracy: 0.9916999936103821; Time: 84.77742266654968\n",
            "5200: train loss: 0.01485523802697105; test loss: 0.0359063595533371; train accuracy: 0.9947856472964886; test accuracy: 0.9918000102043152; Time: 86.39432191848755\n",
            "5300: train loss: 0.014599606603488255; test loss: 0.038952235132455826; train accuracy: 0.9955224521723441; test accuracy: 0.9914000034332275; Time: 88.00697922706604\n",
            "5400: train loss: 0.014624785087591427; test loss: 0.0354895256459713; train accuracy: 0.9956334312227334; test accuracy: 0.9919000267982483; Time: 89.62080240249634\n",
            "5500: train loss: 0.016899827973865644; test loss: 0.039123114198446274; train accuracy: 0.9944998411718392; test accuracy: 0.9916999936103821; Time: 91.23937654495239\n",
            "5600: train loss: 0.014653662886679606; test loss: 0.04085791856050491; train accuracy: 0.9951011047666799; test accuracy: 0.9908999800682068; Time: 92.83770370483398\n",
            "5700: train loss: 0.01683619261066023; test loss: 0.03969893977046013; train accuracy: 0.9946444754113934; test accuracy: 0.9919000267982483; Time: 94.44336080551147\n",
            "5800: train loss: 0.01728165044660127; test loss: 0.04630662128329277; train accuracy: 0.9947617283067995; test accuracy: 0.9908999800682068; Time: 96.06405711174011\n",
            "5900: train loss: 0.01498158982911001; test loss: 0.03686569258570671; train accuracy: 0.9956363483900341; test accuracy: 0.9926000237464905; Time: 97.6841413974762\n",
            "6000: train loss: 0.01665837003526848; test loss: 0.040681444108486176; train accuracy: 0.9952917600013835; test accuracy: 0.9909999966621399; Time: 99.29994511604309\n",
            "6100: train loss: 0.01495004259115702; test loss: 0.04587936773896217; train accuracy: 0.9956904640367251; test accuracy: 0.9911999702453613; Time: 100.913325548172\n",
            "6200: train loss: 0.016101388291418293; test loss: 0.04319923743605614; train accuracy: 0.9956576917125131; test accuracy: 0.9915000200271606; Time: 102.50425100326538\n",
            "6300: train loss: 0.015387880395877773; test loss: 0.03917253017425537; train accuracy: 0.9956167117255804; test accuracy: 0.9912999868392944; Time: 104.08234071731567\n",
            "6400: train loss: 0.01567564736619144; test loss: 0.040458276867866516; train accuracy: 0.9956366057117794; test accuracy: 0.9914000034332275; Time: 105.65788769721985\n",
            "6500: train loss: 0.015096452789055788; test loss: 0.04193396121263504; train accuracy: 0.9952248563161104; test accuracy: 0.9907000064849854; Time: 107.23353028297424\n",
            "6600: train loss: 0.014497487655814514; test loss: 0.04431505128741264; train accuracy: 0.995819958738829; test accuracy: 0.9904999732971191; Time: 108.81444191932678\n",
            "6700: train loss: 0.014488615707882709; test loss: 0.03776468336582184; train accuracy: 0.9959621567890623; test accuracy: 0.9919000267982483; Time: 110.39030289649963\n",
            "6800: train loss: 0.016151446975110527; test loss: 0.03775031864643097; train accuracy: 0.9957617917539462; test accuracy: 0.991599977016449; Time: 111.96757912635803\n",
            "6900: train loss: 0.014133237441886656; test loss: 0.03879375755786896; train accuracy: 0.9962143436041612; test accuracy: 0.991100013256073; Time: 113.54191303253174\n",
            "7000: train loss: 0.012807639642014456; test loss: 0.0398697666823864; train accuracy: 0.9961331885995057; test accuracy: 0.991599977016449; Time: 115.12025237083435\n",
            "7100: train loss: 0.011698846944586372; test loss: 0.035935547202825546; train accuracy: 0.9964647226878512; test accuracy: 0.9926000237464905; Time: 116.69118165969849\n",
            "7200: train loss: 0.012331481263266388; test loss: 0.04038853198289871; train accuracy: 0.9964339101407563; test accuracy: 0.9914000034332275; Time: 118.26702904701233\n",
            "7300: train loss: 0.012135815757946762; test loss: 0.04525455832481384; train accuracy: 0.9962013647013425; test accuracy: 0.991599977016449; Time: 119.84074854850769\n",
            "7400: train loss: 0.013762888025476407; test loss: 0.04324939474463463; train accuracy: 0.9962392127295041; test accuracy: 0.9914000034332275; Time: 121.42280793190002\n",
            "7500: train loss: 0.011657087706612786; test loss: 0.04584293067455292; train accuracy: 0.9964714278884421; test accuracy: 0.991100013256073; Time: 123.00055813789368\n",
            "7600: train loss: 0.0176514298591147; test loss: 0.03901979327201843; train accuracy: 0.9957455992372285; test accuracy: 0.9929999709129333; Time: 124.57372450828552\n",
            "7700: train loss: 0.013950771131199965; test loss: 0.0363699272274971; train accuracy: 0.9966276599229172; test accuracy: 0.992900013923645; Time: 126.15182042121887\n",
            "7800: train loss: 0.012152181735273162; test loss: 0.04152330383658409; train accuracy: 0.996590384210712; test accuracy: 0.9925000071525574; Time: 127.73092341423035\n",
            "7900: train loss: 0.012838370848566047; test loss: 0.04087216034531593; train accuracy: 0.9964113903216718; test accuracy: 0.9919000267982483; Time: 129.30635571479797\n",
            "8000: train loss: 0.013784253547957339; test loss: 0.03750159591436386; train accuracy: 0.9961156313785642; test accuracy: 0.9921000003814697; Time: 130.88111686706543\n",
            "8100: train loss: 0.010388559918377912; test loss: 0.045132096856832504; train accuracy: 0.9972313387145904; test accuracy: 0.9916999936103821; Time: 132.46208596229553\n",
            "8200: train loss: 0.01144675317056619; test loss: 0.04307020828127861; train accuracy: 0.9968749096499283; test accuracy: 0.9916999936103821; Time: 134.04454827308655\n",
            "8300: train loss: 0.012679917611553898; test loss: 0.04342016950249672; train accuracy: 0.9965791233586383; test accuracy: 0.9923999905586243; Time: 135.62576532363892\n",
            "8400: train loss: 0.010826196435152846; test loss: 0.04124007746577263; train accuracy: 0.9963979522216974; test accuracy: 0.9922999739646912; Time: 137.21067476272583\n",
            "8500: train loss: 0.008578853070289822; test loss: 0.040651313960552216; train accuracy: 0.9971445113088456; test accuracy: 0.9922000169754028; Time: 138.79286408424377\n",
            "8600: train loss: 0.010264869775087643; test loss: 0.040989842265844345; train accuracy: 0.9969459335319459; test accuracy: 0.9915000200271606; Time: 140.37466669082642\n",
            "8700: train loss: 0.010615016798983313; test loss: 0.04316803440451622; train accuracy: 0.9968327145528993; test accuracy: 0.9916999936103821; Time: 141.9566113948822\n",
            "8800: train loss: 0.01022553975206685; test loss: 0.04412779584527016; train accuracy: 0.9971131732208054; test accuracy: 0.9919999837875366; Time: 143.5356957912445\n",
            "8900: train loss: 0.009860824284011206; test loss: 0.04433293268084526; train accuracy: 0.9969068392529944; test accuracy: 0.9922000169754028; Time: 145.11295771598816\n",
            "9000: train loss: 0.008610143410645909; test loss: 0.04548291116952896; train accuracy: 0.9972967783940215; test accuracy: 0.9925000071525574; Time: 146.69030904769897\n",
            "9100: train loss: 0.009679307585593695; test loss: 0.043040141463279724; train accuracy: 0.9970042567617372; test accuracy: 0.9922999739646912; Time: 148.27304792404175\n",
            "9200: train loss: 0.010688739961820126; test loss: 0.04261430725455284; train accuracy: 0.9969387021598114; test accuracy: 0.9918000102043152; Time: 149.85029697418213\n",
            "9300: train loss: 0.009833039696349834; test loss: 0.04667983204126358; train accuracy: 0.9970207693921621; test accuracy: 0.9922000169754028; Time: 151.4253511428833\n",
            "9400: train loss: 0.00952277069982546; test loss: 0.04701106995344162; train accuracy: 0.9971836174129373; test accuracy: 0.9915000200271606; Time: 153.0029957294464\n",
            "9500: train loss: 0.01064104843444728; test loss: 0.04052409902215004; train accuracy: 0.9970495352537976; test accuracy: 0.9922000169754028; Time: 154.57906365394592\n",
            "9600: train loss: 0.009128524942380886; test loss: 0.04681336507201195; train accuracy: 0.9973025772269813; test accuracy: 0.9918000102043152; Time: 156.15454053878784\n",
            "9700: train loss: 0.00944229258797246; test loss: 0.046670299023389816; train accuracy: 0.9973469397316753; test accuracy: 0.9922000169754028; Time: 157.74071836471558\n",
            "9800: train loss: 0.009824128853796112; test loss: 0.0484839491546154; train accuracy: 0.9976594120203482; test accuracy: 0.9922000169754028; Time: 159.32612800598145\n",
            "9900: train loss: 0.00985538813195382; test loss: 0.048118580132722855; train accuracy: 0.9976289839488616; test accuracy: 0.9911999702453613; Time: 160.89995980262756\n",
            "[[ 979    0    0    0    0    0    0    1    0    0]\n",
            " [   0 1130    0    0    0    0    1    4    0    0]\n",
            " [   1    1 1021    2    0    0    0    4    3    0]\n",
            " [   0    0    0 1001    0    8    0    1    0    0]\n",
            " [   0    0    0    0  977    0    2    0    0    3]\n",
            " [   1    0    0    2    0  887    1    1    0    0]\n",
            " [   3    0    1    0    1    1  950    0    2    0]\n",
            " [   0    4    5    1    1    0    0 1015    0    2]\n",
            " [   2    0    0    0    1    0    0    0  971    0]\n",
            " [   1    1    0    2    6    1    0    3    4  991]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9919    0.9990    0.9954       980\n",
            "           1     0.9947    0.9956    0.9952      1135\n",
            "           2     0.9942    0.9893    0.9917      1032\n",
            "           3     0.9931    0.9911    0.9921      1010\n",
            "           4     0.9909    0.9949    0.9929       982\n",
            "           5     0.9889    0.9944    0.9916       892\n",
            "           6     0.9958    0.9916    0.9937       958\n",
            "           7     0.9864    0.9874    0.9869      1028\n",
            "           8     0.9908    0.9969    0.9939       974\n",
            "           9     0.9950    0.9822    0.9885      1009\n",
            "\n",
            "    accuracy                         0.9922     10000\n",
            "   macro avg     0.9922    0.9922    0.9922     10000\n",
            "weighted avg     0.9922    0.9922    0.9922     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICmDYUiOj2_D",
        "colab_type": "text"
      },
      "source": [
        "##Momentum+Kron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqy798bhcE6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc46e372-7624-4382-bef5-f34b6d8f42b0"
      },
      "source": [
        "# apply adam on grads then computing preconditioner from the adam_grads\n",
        "# grads ---> adam_grads ---> precond_grads\n",
        "\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    grads_moment = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "\n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    new_grads_moment = [beta1*old + (1.0 - beta1)*new for (old, new) in zip(grads_moment, grads)]\n",
        "\n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, new_grads_moment)]\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws,precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    update_grads_moment = [tf.assign(old, new) for (old, new) in zip(grads_moment, new_grads_moment)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)     \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    t = 0\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "        t = t + 1\n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _,_ = sess.run([train_loss, train_accuracy, update_Ws, update_grads_moment, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'moment_kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.406951665878296; test loss: 2.3741438388824463; train accuracy: 0.0390625; test accuracy: 0.06459999829530716; Time: 2.4746944904327393\n",
            "100: train loss: 1.089107416638732; test loss: 0.21023276448249817; train accuracy: 0.65240234375; test accuracy: 0.9447000026702881; Time: 4.005738973617554\n",
            "200: train loss: 0.5471953644593693; test loss: 0.08581697940826416; train accuracy: 0.8313060003442991; test accuracy: 0.9758999943733215; Time: 5.526403903961182\n",
            "300: train loss: 0.28972674048449093; test loss: 0.05715553089976311; train accuracy: 0.9127552949547308; test accuracy: 0.9843999743461609; Time: 7.045264959335327\n",
            "400: train loss: 0.16903055083515342; test loss: 0.04379257187247276; train accuracy: 0.9495822703673328; test accuracy: 0.9884999990463257; Time: 8.56517481803894\n",
            "500: train loss: 0.1160991962011241; test loss: 0.03813538700342178; train accuracy: 0.9663241130682976; test accuracy: 0.989300012588501; Time: 10.083621740341187\n",
            "600: train loss: 0.08950301998985868; test loss: 0.035062920302152634; train accuracy: 0.9737224868612514; test accuracy: 0.9890000224113464; Time: 11.602988481521606\n",
            "700: train loss: 0.07149673226506516; test loss: 0.035464104264974594; train accuracy: 0.9796571433578168; test accuracy: 0.9894000291824341; Time: 13.11704134941101\n",
            "800: train loss: 0.05922153870869747; test loss: 0.029540175572037697; train accuracy: 0.9823543936051654; test accuracy: 0.9911999702453613; Time: 14.635554790496826\n",
            "900: train loss: 0.05245704197678079; test loss: 0.02656589448451996; train accuracy: 0.9846754222147195; test accuracy: 0.9919000267982483; Time: 16.15033769607544\n",
            "1000: train loss: 0.048927688654125945; test loss: 0.030175110325217247; train accuracy: 0.9849985119536192; test accuracy: 0.9907000064849854; Time: 17.664510250091553\n",
            "1100: train loss: 0.047245975437758984; test loss: 0.02661779150366783; train accuracy: 0.9851531456828811; test accuracy: 0.9916999936103821; Time: 19.17883849143982\n",
            "1200: train loss: 0.0431825175505912; test loss: 0.024372601881623268; train accuracy: 0.9863756284540509; test accuracy: 0.9918000102043152; Time: 20.69098925590515\n",
            "1300: train loss: 0.037554370342277946; test loss: 0.02407877892255783; train accuracy: 0.988472313808017; test accuracy: 0.9922000169754028; Time: 22.208181142807007\n",
            "1400: train loss: 0.03592323660232977; test loss: 0.02409966289997101; train accuracy: 0.9888568320285759; test accuracy: 0.9922000169754028; Time: 23.729259252548218\n",
            "1500: train loss: 0.03307612403924562; test loss: 0.023088952526450157; train accuracy: 0.9902839034574963; test accuracy: 0.9922999739646912; Time: 25.244699716567993\n",
            "1600: train loss: 0.031149209719139957; test loss: 0.023136835545301437; train accuracy: 0.9911582692047018; test accuracy: 0.9919999837875366; Time: 26.765918970108032\n",
            "1700: train loss: 0.02966822323893983; test loss: 0.021118273958563805; train accuracy: 0.9915827350915037; test accuracy: 0.9932000041007996; Time: 28.28415298461914\n",
            "1800: train loss: 0.02721251398753001; test loss: 0.020575568079948425; train accuracy: 0.991820054938325; test accuracy: 0.9927999973297119; Time: 29.8010733127594\n",
            "1900: train loss: 0.026337327409607855; test loss: 0.019021866843104362; train accuracy: 0.9921937611851617; test accuracy: 0.9939000010490417; Time: 31.323025226593018\n",
            "2000: train loss: 0.026068306313485414; test loss: 0.01942608691751957; train accuracy: 0.9922651257239223; test accuracy: 0.9933000206947327; Time: 32.84067344665527\n",
            "2100: train loss: 0.025430801230511817; test loss: 0.019376616925001144; train accuracy: 0.9929668146433592; test accuracy: 0.9937000274658203; Time: 34.35719418525696\n",
            "2200: train loss: 0.02366375968628213; test loss: 0.019166750833392143; train accuracy: 0.992585333687329; test accuracy: 0.9943000078201294; Time: 35.87591743469238\n",
            "2300: train loss: 0.023871650275182805; test loss: 0.01937391236424446; train accuracy: 0.9932260467278787; test accuracy: 0.9944000244140625; Time: 37.39880442619324\n",
            "2400: train loss: 0.02143359218854908; test loss: 0.01970551908016205; train accuracy: 0.9939810681809327; test accuracy: 0.9939000010490417; Time: 38.912869691848755\n",
            "2500: train loss: 0.0198762133368146; test loss: 0.01933308318257332; train accuracy: 0.9941324742888981; test accuracy: 0.9937999844551086; Time: 40.4293167591095\n",
            "2600: train loss: 0.020185963047785124; test loss: 0.020165469497442245; train accuracy: 0.994100190497492; test accuracy: 0.9934999942779541; Time: 41.943368911743164\n",
            "2700: train loss: 0.020747989820881385; test loss: 0.019554344937205315; train accuracy: 0.9941504789576431; test accuracy: 0.9937999844551086; Time: 43.46299386024475\n",
            "2800: train loss: 0.02109112107782881; test loss: 0.022954445332288742; train accuracy: 0.9938442497136268; test accuracy: 0.991599977016449; Time: 44.97943449020386\n",
            "2900: train loss: 0.020296760804672253; test loss: 0.022315258160233498; train accuracy: 0.994011847202091; test accuracy: 0.9927999973297119; Time: 46.49583578109741\n",
            "3000: train loss: 0.018127944112434326; test loss: 0.019404718652367592; train accuracy: 0.9943544216691017; test accuracy: 0.9933000206947327; Time: 48.01100969314575\n",
            "3100: train loss: 0.016887017874875964; test loss: 0.02269022911787033; train accuracy: 0.9950092484105031; test accuracy: 0.9934999942779541; Time: 49.53135657310486\n",
            "3200: train loss: 0.016210059336537633; test loss: 0.018943019211292267; train accuracy: 0.9951012202998868; test accuracy: 0.9940999746322632; Time: 51.05227446556091\n",
            "3300: train loss: 0.016278421978911627; test loss: 0.02190292440354824; train accuracy: 0.9952794485419242; test accuracy: 0.9933000206947327; Time: 52.56941747665405\n",
            "3400: train loss: 0.015569364237951301; test loss: 0.02091033011674881; train accuracy: 0.9954697315091471; test accuracy: 0.9936000108718872; Time: 54.084429025650024\n",
            "3500: train loss: 0.016149938844620754; test loss: 0.022891681641340256; train accuracy: 0.9956199176195404; test accuracy: 0.9926999807357788; Time: 55.604408502578735\n",
            "3600: train loss: 0.013998849909448764; test loss: 0.018763573840260506; train accuracy: 0.9957552408648362; test accuracy: 0.9939000010490417; Time: 57.12336778640747\n",
            "3700: train loss: 0.015362012685523872; test loss: 0.019777487963438034; train accuracy: 0.995573284142491; test accuracy: 0.9932000041007996; Time: 58.6429705619812\n",
            "3800: train loss: 0.014160106735851045; test loss: 0.019717475399374962; train accuracy: 0.996136864123192; test accuracy: 0.993399977684021; Time: 60.15937829017639\n",
            "3900: train loss: 0.010771371302806402; test loss: 0.017665741965174675; train accuracy: 0.9971866162125418; test accuracy: 0.9943000078201294; Time: 61.69597768783569\n",
            "4000: train loss: 0.013025686412335721; test loss: 0.020919332280755043; train accuracy: 0.9962857875318382; test accuracy: 0.9926000237464905; Time: 63.22341537475586\n",
            "4100: train loss: 0.013008051349130401; test loss: 0.020917030051350594; train accuracy: 0.9963214804890714; test accuracy: 0.993399977684021; Time: 64.74965167045593\n",
            "4200: train loss: 0.01356200770330494; test loss: 0.018853725865483284; train accuracy: 0.9962644481663294; test accuracy: 0.9937999844551086; Time: 66.26512789726257\n",
            "4300: train loss: 0.012291882561244132; test loss: 0.019977601245045662; train accuracy: 0.9969217667892104; test accuracy: 0.9937999844551086; Time: 67.77900123596191\n",
            "4400: train loss: 0.013401364687457954; test loss: 0.02063455432653427; train accuracy: 0.9970621715736099; test accuracy: 0.993399977684021; Time: 69.29746556282043\n",
            "4500: train loss: 0.010755456280522416; test loss: 0.021231060847640038; train accuracy: 0.9972891231773164; test accuracy: 0.9941999912261963; Time: 70.81515169143677\n",
            "4600: train loss: 0.011245944097946284; test loss: 0.019672080874443054; train accuracy: 0.9968102080187414; test accuracy: 0.9934999942779541; Time: 72.32988977432251\n",
            "4700: train loss: 0.010630092819857889; test loss: 0.0200112946331501; train accuracy: 0.9972428074977698; test accuracy: 0.9936000108718872; Time: 73.84843111038208\n",
            "4800: train loss: 0.011089244015710986; test loss: 0.022599883377552032; train accuracy: 0.9970314425542823; test accuracy: 0.9932000041007996; Time: 75.36419606208801\n",
            "4900: train loss: 0.009470901389138303; test loss: 0.02077329345047474; train accuracy: 0.9974582618436898; test accuracy: 0.9930999875068665; Time: 76.87705421447754\n",
            "5000: train loss: 0.009601953505657698; test loss: 0.019142217934131622; train accuracy: 0.997479077963235; test accuracy: 0.9943000078201294; Time: 78.39356994628906\n",
            "5100: train loss: 0.008913450909060825; test loss: 0.020198876038193703; train accuracy: 0.997834953867267; test accuracy: 0.9930999875068665; Time: 79.90577602386475\n",
            "5200: train loss: 0.01071938052685157; test loss: 0.020577073097229004; train accuracy: 0.9974244453417894; test accuracy: 0.993399977684021; Time: 81.42221641540527\n",
            "5300: train loss: 0.010368701793696217; test loss: 0.019694024696946144; train accuracy: 0.9972782050058449; test accuracy: 0.9934999942779541; Time: 82.93888688087463\n",
            "5400: train loss: 0.008941833240606862; test loss: 0.018727729097008705; train accuracy: 0.9977513076967363; test accuracy: 0.9944000244140625; Time: 84.45212006568909\n",
            "5500: train loss: 0.0086524793865529; test loss: 0.01905798353254795; train accuracy: 0.9977906011916851; test accuracy: 0.9944000244140625; Time: 85.96738767623901\n",
            "5600: train loss: 0.008765476819859626; test loss: 0.019943933933973312; train accuracy: 0.9978739876045337; test accuracy: 0.9943000078201294; Time: 87.48467946052551\n",
            "5700: train loss: 0.007159144449037183; test loss: 0.01950518600642681; train accuracy: 0.9985161647447304; test accuracy: 0.9944999814033508; Time: 88.99960827827454\n",
            "5800: train loss: 0.008331940117210897; test loss: 0.019450636580586433; train accuracy: 0.9977023528077176; test accuracy: 0.9945999979972839; Time: 90.52040314674377\n",
            "5900: train loss: 0.006661923454671249; test loss: 0.02060515433549881; train accuracy: 0.9983540529690466; test accuracy: 0.9940000176429749; Time: 92.04268908500671\n",
            "6000: train loss: 0.006742670880773854; test loss: 0.020803213119506836; train accuracy: 0.9982854587546242; test accuracy: 0.9940000176429749; Time: 93.55691742897034\n",
            "6100: train loss: 0.007327297070319899; test loss: 0.018728408962488174; train accuracy: 0.9981936597750936; test accuracy: 0.9941999912261963; Time: 95.0743510723114\n",
            "6200: train loss: 0.0067707519694188735; test loss: 0.018973348662257195; train accuracy: 0.9982892206243666; test accuracy: 0.9943000078201294; Time: 96.58704137802124\n",
            "6300: train loss: 0.007891356079478511; test loss: 0.019696040078997612; train accuracy: 0.9977666786823648; test accuracy: 0.9941999912261963; Time: 98.10427761077881\n",
            "6400: train loss: 0.008182358575689776; test loss: 0.018421491608023643; train accuracy: 0.9974803577897312; test accuracy: 0.9943000078201294; Time: 99.6195182800293\n",
            "6500: train loss: 0.008428341993530214; test loss: 0.018812859430909157; train accuracy: 0.9976830078995279; test accuracy: 0.9937000274658203; Time: 101.13218259811401\n",
            "6600: train loss: 0.007701407413153654; test loss: 0.01892489194869995; train accuracy: 0.9978202858667238; test accuracy: 0.9937999844551086; Time: 102.66236805915833\n",
            "6700: train loss: 0.006808991703964281; test loss: 0.019524410367012024; train accuracy: 0.998099835306946; test accuracy: 0.9937999844551086; Time: 104.19716453552246\n",
            "6800: train loss: 0.0054913800173436765; test loss: 0.01917913742363453; train accuracy: 0.9985892671598375; test accuracy: 0.9940000176429749; Time: 105.7265772819519\n",
            "6900: train loss: 0.006242886853627055; test loss: 0.01882331073284149; train accuracy: 0.9984010621514189; test accuracy: 0.9936000108718872; Time: 107.26570129394531\n",
            "7000: train loss: 0.006495251866290742; test loss: 0.017652997747063637; train accuracy: 0.9981617337211088; test accuracy: 0.9944000244140625; Time: 108.8007824420929\n",
            "7100: train loss: 0.0055766514934932655; test loss: 0.019268661737442017; train accuracy: 0.9985497250153775; test accuracy: 0.9936000108718872; Time: 110.33601975440979\n",
            "7200: train loss: 0.004260839787742252; test loss: 0.018533121794462204; train accuracy: 0.9989058648895133; test accuracy: 0.9948999881744385; Time: 111.86150288581848\n",
            "7300: train loss: 0.006681574752583417; test loss: 0.022336119785904884; train accuracy: 0.9980249865265675; test accuracy: 0.9939000010490417; Time: 113.37612509727478\n",
            "7400: train loss: 0.0062616935944897105; test loss: 0.01907701976597309; train accuracy: 0.9979779464842116; test accuracy: 0.9944000244140625; Time: 114.89105248451233\n",
            "7500: train loss: 0.006087400904038968; test loss: 0.019203007221221924; train accuracy: 0.9981782979274735; test accuracy: 0.9945999979972839; Time: 116.41116523742676\n",
            "7600: train loss: 0.005652492930280514; test loss: 0.018677685409784317; train accuracy: 0.998203599908795; test accuracy: 0.9947999715805054; Time: 117.92609691619873\n",
            "7700: train loss: 0.0052084745316229135; test loss: 0.019348017871379852; train accuracy: 0.9981996038556541; test accuracy: 0.9945999979972839; Time: 119.44412541389465\n",
            "7800: train loss: 0.005441063462151155; test loss: 0.022320520132780075; train accuracy: 0.9981942592114642; test accuracy: 0.9940999746322632; Time: 120.96458339691162\n",
            "7900: train loss: 0.0065340241152132125; test loss: 0.019056718796491623; train accuracy: 0.9982193053525198; test accuracy: 0.9940999746322632; Time: 122.48353719711304\n",
            "8000: train loss: 0.00729003098578196; test loss: 0.019482817500829697; train accuracy: 0.9981847801181439; test accuracy: 0.9951000213623047; Time: 123.99802541732788\n",
            "8100: train loss: 0.005341274985036785; test loss: 0.01976882293820381; train accuracy: 0.9987990811913274; test accuracy: 0.994700014591217; Time: 125.51384663581848\n",
            "8200: train loss: 0.004225558567601743; test loss: 0.01963002234697342; train accuracy: 0.9991040722997768; test accuracy: 0.9944999814033508; Time: 127.03003787994385\n",
            "8300: train loss: 0.004790774957999044; test loss: 0.021178949624300003; train accuracy: 0.9984756348203578; test accuracy: 0.9936000108718872; Time: 128.54436922073364\n",
            "8400: train loss: 0.005336895499149158; test loss: 0.01905699446797371; train accuracy: 0.9983311063986006; test accuracy: 0.9945999979972839; Time: 130.06476092338562\n",
            "8500: train loss: 0.005752579864745315; test loss: 0.019385013729333878; train accuracy: 0.9984268948806838; test accuracy: 0.9936000108718872; Time: 131.57906675338745\n",
            "8600: train loss: 0.005066000939904213; test loss: 0.01904871128499508; train accuracy: 0.9985913714704107; test accuracy: 0.9948999881744385; Time: 133.0964777469635\n",
            "8700: train loss: 0.005040661292869558; test loss: 0.019646557047963142; train accuracy: 0.9984983298225065; test accuracy: 0.9954000115394592; Time: 134.61700916290283\n",
            "8800: train loss: 0.005115326829250158; test loss: 0.02204742096364498; train accuracy: 0.9984720767251786; test accuracy: 0.9944999814033508; Time: 136.13114881515503\n",
            "8900: train loss: 0.00507311033803359; test loss: 0.019741253927350044; train accuracy: 0.9983355285882245; test accuracy: 0.9943000078201294; Time: 137.65035581588745\n",
            "9000: train loss: 0.005274493341953679; test loss: 0.021219035610556602; train accuracy: 0.9981789687102318; test accuracy: 0.9945999979972839; Time: 139.16659784317017\n",
            "9100: train loss: 0.005742874251892339; test loss: 0.01989152655005455; train accuracy: 0.9982139016307663; test accuracy: 0.9941999912261963; Time: 140.6862530708313\n",
            "9200: train loss: 0.0041789684537947005; test loss: 0.019068507477641106; train accuracy: 0.9987022006815649; test accuracy: 0.995199978351593; Time: 142.20050430297852\n",
            "9300: train loss: 0.004437537458864054; test loss: 0.02170622907578945; train accuracy: 0.9987743131239132; test accuracy: 0.9937000274658203; Time: 143.72217059135437\n",
            "9400: train loss: 0.004337888659317694; test loss: 0.023600051179528236; train accuracy: 0.9990060139490732; test accuracy: 0.9933000206947327; Time: 145.23829078674316\n",
            "9500: train loss: 0.004644042211460359; test loss: 0.02439855970442295; train accuracy: 0.9985945361308602; test accuracy: 0.9930999875068665; Time: 146.75130081176758\n",
            "9600: train loss: 0.004503118070533307; test loss: 0.02236659824848175; train accuracy: 0.9987366444766916; test accuracy: 0.9940999746322632; Time: 148.2660620212555\n",
            "9700: train loss: 0.005050665562893201; test loss: 0.022117959335446358; train accuracy: 0.9984057619725; test accuracy: 0.9940999746322632; Time: 149.77999138832092\n",
            "9800: train loss: 0.004609726810818671; test loss: 0.02000083588063717; train accuracy: 0.9986975080321229; test accuracy: 0.9940000176429749; Time: 151.2935631275177\n",
            "9900: train loss: 0.0053879718492190155; test loss: 0.02154221013188362; train accuracy: 0.9983084637684035; test accuracy: 0.9944999814033508; Time: 152.80971431732178\n",
            "[[ 977    0    0    0    0    0    0    2    1    0]\n",
            " [   0 1131    1    1    1    0    1    0    0    0]\n",
            " [   1    0 1030    1    0    0    0    0    0    0]\n",
            " [   0    0    0 1006    0    2    0    1    1    0]\n",
            " [   0    1    1    0  976    0    0    0    1    3]\n",
            " [   1    0    0    4    0  886    1    0    0    0]\n",
            " [   2    1    0    0    0    3  951    0    1    0]\n",
            " [   0    2    4    0    1    0    0 1021    0    0]\n",
            " [   1    0    0    1    0    1    3    0  966    2]\n",
            " [   0    0    2    1    5    3    0    2    1  995]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9949    0.9969    0.9959       980\n",
            "           1     0.9965    0.9965    0.9965      1135\n",
            "           2     0.9923    0.9981    0.9952      1032\n",
            "           3     0.9921    0.9960    0.9941      1010\n",
            "           4     0.9929    0.9939    0.9934       982\n",
            "           5     0.9899    0.9933    0.9916       892\n",
            "           6     0.9948    0.9927    0.9937       958\n",
            "           7     0.9951    0.9932    0.9942      1028\n",
            "           8     0.9949    0.9918    0.9933       974\n",
            "           9     0.9950    0.9861    0.9905      1009\n",
            "\n",
            "    accuracy                         0.9939     10000\n",
            "   macro avg     0.9938    0.9938    0.9938     10000\n",
            "weighted avg     0.9939    0.9939    0.9939     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BipVcsFZNSy",
        "colab_type": "text"
      },
      "source": [
        "## Kronecker Product --> ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKa7xdbLS4F",
        "colab_type": "code",
        "outputId": "55b560bf-620c-4e50-a798-1c556dc11448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# using grads to compute preconditioner, then applying adam on precond_grads\n",
        "# grads ---> precond_grads ---> adam_grads\n",
        "\n",
        "step_size = 0.02\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    grads_vars = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "    grads_moment = [tf.Variable(tf.zeros(W.shape, dtype=dtype), trainable=False) for W in Ws]\n",
        "\n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    precond_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, grads)]\n",
        "\n",
        "    new_grads_moment = [beta1*old + (1.0 - beta1)*new for (old, new) in zip(grads_moment, precond_grads)]\n",
        "    new_grads_vars = [beta2*old + (1.0 - beta2)*new*new for (old, new) in zip(grads_vars, precond_grads)]\n",
        "    new_grads_moment_hat = [m/(1-beta1**adam_step) for m in new_grads_moment]\n",
        "    new_grads_vars_hat = [v/(1-beta2**adam_step) for v in new_grads_vars]\n",
        "\n",
        "    adam_grads = [m/tf.sqrt(v + offset) for (m,v) in zip(new_grads_moment_hat,new_grads_vars_hat)]\n",
        "\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in adam_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws,adam_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    update_grads_vars = [tf.assign(old, new) for (old, new) in zip(grads_vars, new_grads_vars)]\n",
        "    update_grads_moment = [tf.assign(old, new) for (old, new) in zip(grads_moment, new_grads_moment)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)  \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    t = 0\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "        t = t + 1\n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _,_,_ = sess.run([train_loss, train_accuracy, update_Ws, update_grads_moment, update_grads_vars, update_Qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs, adam_step: t})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'kron_adam.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3989572525024414; test loss: 2.201631784439087; train accuracy: 0.03125; test accuracy: 0.20160000026226044; Time: 2.9666478633880615\n",
            "100: train loss: 0.5689112619027499; test loss: 0.06846974790096283; train accuracy: 0.8184710937500004; test accuracy: 0.9786999821662903; Time: 4.533591032028198\n",
            "200: train loss: 0.27734027824759167; test loss: 0.041201137006282806; train accuracy: 0.9118718769629609; test accuracy: 0.9873999953269958; Time: 6.088185548782349\n",
            "300: train loss: 0.1422005983467293; test loss: 0.040101733058691025; train accuracy: 0.9550320690532619; test accuracy: 0.9873999953269958; Time: 7.639142751693726\n",
            "400: train loss: 0.08607102500470497; test loss: 0.03758822754025459; train accuracy: 0.9734741435426392; test accuracy: 0.9876999855041504; Time: 9.197100639343262\n",
            "500: train loss: 0.0670070886840364; test loss: 0.035305242985486984; train accuracy: 0.9795461754261564; test accuracy: 0.9882000088691711; Time: 10.751317024230957\n",
            "600: train loss: 0.054595971387078154; test loss: 0.029755456373095512; train accuracy: 0.9836905570175506; test accuracy: 0.9907000064849854; Time: 12.305785417556763\n",
            "700: train loss: 0.04670483506651941; test loss: 0.02833983674645424; train accuracy: 0.9862222788380056; test accuracy: 0.9901999831199646; Time: 13.866960048675537\n",
            "800: train loss: 0.045263762707995696; test loss: 0.03159576281905174; train accuracy: 0.985968502985306; test accuracy: 0.9901000261306763; Time: 15.422956466674805\n",
            "900: train loss: 0.04098901525308728; test loss: 0.03421070799231529; train accuracy: 0.987631042160202; test accuracy: 0.9898999929428101; Time: 16.977330684661865\n",
            "1000: train loss: 0.03627164890641308; test loss: 0.026790020987391472; train accuracy: 0.9893419226138332; test accuracy: 0.9914000034332275; Time: 18.530442476272583\n",
            "1100: train loss: 0.035933596162494454; test loss: 0.03565593808889389; train accuracy: 0.9888649845894456; test accuracy: 0.9894999861717224; Time: 20.083557844161987\n",
            "1200: train loss: 0.03740470831584319; test loss: 0.030666394159197807; train accuracy: 0.9882674519396599; test accuracy: 0.9909999966621399; Time: 21.638367652893066\n",
            "1300: train loss: 0.03566060861012663; test loss: 0.03090306930243969; train accuracy: 0.9888508355707416; test accuracy: 0.9904999732971191; Time: 23.187798500061035\n",
            "1400: train loss: 0.031203184945590632; test loss: 0.03538352623581886; train accuracy: 0.9900050558666241; test accuracy: 0.989300012588501; Time: 24.741326332092285\n",
            "1500: train loss: 0.03572587189929079; test loss: 0.026632925495505333; train accuracy: 0.9895356055585817; test accuracy: 0.9912999868392944; Time: 26.292872667312622\n",
            "1600: train loss: 0.033595826416218756; test loss: 0.027380740270018578; train accuracy: 0.9901999877091996; test accuracy: 0.9911999702453613; Time: 27.845728874206543\n",
            "1700: train loss: 0.035593536492339733; test loss: 0.027624310925602913; train accuracy: 0.9898411209258807; test accuracy: 0.9915000200271606; Time: 29.395040273666382\n",
            "1800: train loss: 0.034610347254752066; test loss: 0.02634331025183201; train accuracy: 0.9894396922511819; test accuracy: 0.9921000003814697; Time: 30.95285439491272\n",
            "1900: train loss: 0.03434686096100018; test loss: 0.025038544088602066; train accuracy: 0.990033567166896; test accuracy: 0.9918000102043152; Time: 32.5031418800354\n",
            "2000: train loss: 0.032793615003792194; test loss: 0.028257431462407112; train accuracy: 0.9900678377135248; test accuracy: 0.9912999868392944; Time: 34.0569543838501\n",
            "2100: train loss: 0.0301270660389097; test loss: 0.03327511250972748; train accuracy: 0.9903207818620616; test accuracy: 0.9908999800682068; Time: 35.612104415893555\n",
            "2200: train loss: 0.03061295144644168; test loss: 0.02805328369140625; train accuracy: 0.9906950998956855; test accuracy: 0.9909999966621399; Time: 37.17040157318115\n",
            "2300: train loss: 0.03099787284507257; test loss: 0.028990503400564194; train accuracy: 0.9899713924017489; test accuracy: 0.9918000102043152; Time: 38.72458815574646\n",
            "2400: train loss: 0.0318447968667708; test loss: 0.03126181662082672; train accuracy: 0.9899343161125097; test accuracy: 0.9905999898910522; Time: 40.279165267944336\n",
            "2500: train loss: 0.03031467882160588; test loss: 0.03177367523312569; train accuracy: 0.9901415095613009; test accuracy: 0.989799976348877; Time: 41.835124492645264\n",
            "2600: train loss: 0.029575319456405645; test loss: 0.026914581656455994; train accuracy: 0.9909033121486788; test accuracy: 0.9915000200271606; Time: 43.39071178436279\n",
            "2700: train loss: 0.033605136361839004; test loss: 0.028577813878655434; train accuracy: 0.9897904165344136; test accuracy: 0.991100013256073; Time: 44.940889835357666\n",
            "2800: train loss: 0.03079343944521685; test loss: 0.03048667684197426; train accuracy: 0.9907627547315855; test accuracy: 0.9904999732971191; Time: 46.4929518699646\n",
            "2900: train loss: 0.03186681463548253; test loss: 0.030079083517193794; train accuracy: 0.9907966910451035; test accuracy: 0.9908000230789185; Time: 48.04659557342529\n",
            "3000: train loss: 0.02928870209289265; test loss: 0.02703796699643135; train accuracy: 0.9900921000264274; test accuracy: 0.9922000169754028; Time: 49.59739279747009\n",
            "3100: train loss: 0.027440561900818717; test loss: 0.04636677727103233; train accuracy: 0.990699313004057; test accuracy: 0.9873999953269958; Time: 51.14856839179993\n",
            "3200: train loss: 0.030682343397071207; test loss: 0.026126891374588013; train accuracy: 0.9899066218036132; test accuracy: 0.9926000237464905; Time: 52.700883865356445\n",
            "3300: train loss: 0.03241431823112813; test loss: 0.02986927144229412; train accuracy: 0.9898027781108579; test accuracy: 0.991100013256073; Time: 54.254488468170166\n",
            "3400: train loss: 0.03217718398429589; test loss: 0.034394074231386185; train accuracy: 0.9904577809584282; test accuracy: 0.9909999966621399; Time: 55.80487251281738\n",
            "3500: train loss: 0.03133854163958328; test loss: 0.024921990931034088; train accuracy: 0.9905212290967833; test accuracy: 0.9927999973297119; Time: 57.35874009132385\n",
            "3600: train loss: 0.02589452739391239; test loss: 0.03855263069272041; train accuracy: 0.9914745722874788; test accuracy: 0.9897000193595886; Time: 58.916762351989746\n",
            "3700: train loss: 0.028945565470007005; test loss: 0.030336620286107063; train accuracy: 0.9910378485584574; test accuracy: 0.9909999966621399; Time: 60.46944308280945\n",
            "3800: train loss: 0.02925067942851729; test loss: 0.03562519699335098; train accuracy: 0.9910712692308857; test accuracy: 0.9902999997138977; Time: 62.0191650390625\n",
            "3900: train loss: 0.02739633506648016; test loss: 0.031343042850494385; train accuracy: 0.9910833641769945; test accuracy: 0.9911999702453613; Time: 63.56836247444153\n",
            "4000: train loss: 0.03165673015233687; test loss: 0.026380475610494614; train accuracy: 0.9906210400904595; test accuracy: 0.9922000169754028; Time: 65.12164402008057\n",
            "4100: train loss: 0.02891502845434463; test loss: 0.028116336092352867; train accuracy: 0.9910390020727806; test accuracy: 0.9926999807357788; Time: 66.67087817192078\n",
            "4200: train loss: 0.030060413201752713; test loss: 0.02735818177461624; train accuracy: 0.9911034275313938; test accuracy: 0.992900013923645; Time: 68.2215187549591\n",
            "4300: train loss: 0.03005848090400127; test loss: 0.03890059143304825; train accuracy: 0.9905184023778665; test accuracy: 0.9890000224113464; Time: 69.77476930618286\n",
            "4400: train loss: 0.03275818546465764; test loss: 0.030220888555049896; train accuracy: 0.9899874529006079; test accuracy: 0.9914000034332275; Time: 71.32380318641663\n",
            "4500: train loss: 0.028970969032968506; test loss: 0.02632533572614193; train accuracy: 0.99090149373989; test accuracy: 0.991599977016449; Time: 72.88045477867126\n",
            "4600: train loss: 0.029644199389812803; test loss: 0.03492772951722145; train accuracy: 0.9900146423473676; test accuracy: 0.9891999959945679; Time: 74.44595122337341\n",
            "4700: train loss: 0.02697322354173014; test loss: 0.022979721426963806; train accuracy: 0.9911023843548322; test accuracy: 0.992900013923645; Time: 75.99986052513123\n",
            "4800: train loss: 0.029196441112233382; test loss: 0.030171344056725502; train accuracy: 0.9905069376190955; test accuracy: 0.9911999702453613; Time: 77.55222034454346\n",
            "4900: train loss: 0.02895889621890003; test loss: 0.0251137875020504; train accuracy: 0.9904491649856658; test accuracy: 0.9909999966621399; Time: 79.10212397575378\n",
            "5000: train loss: 0.03085442955501733; test loss: 0.029926683753728867; train accuracy: 0.9897301855826602; test accuracy: 0.9908999800682068; Time: 80.65108370780945\n",
            "5100: train loss: 0.027053974163209916; test loss: 0.034572627395391464; train accuracy: 0.9912358328041111; test accuracy: 0.9901999831199646; Time: 82.20750188827515\n",
            "5200: train loss: 0.029139781506941676; test loss: 0.026684798300266266; train accuracy: 0.9908333596667919; test accuracy: 0.991100013256073; Time: 83.76019406318665\n",
            "5300: train loss: 0.028431918816138272; test loss: 0.03325048089027405; train accuracy: 0.9905850610823421; test accuracy: 0.9907000064849854; Time: 85.31210589408875\n",
            "5400: train loss: 0.030213727458659988; test loss: 0.025629747658967972; train accuracy: 0.990534327877374; test accuracy: 0.9926999807357788; Time: 86.86311674118042\n",
            "5500: train loss: 0.03197761736888443; test loss: 0.029464349150657654; train accuracy: 0.9897863968471212; test accuracy: 0.9921000003814697; Time: 88.41839694976807\n",
            "5600: train loss: 0.03215738520331341; test loss: 0.03461770713329315; train accuracy: 0.9906223998206592; test accuracy: 0.989799976348877; Time: 89.97229385375977\n",
            "5700: train loss: 0.032229618149574514; test loss: 0.03413810580968857; train accuracy: 0.990023171936714; test accuracy: 0.9891999959945679; Time: 91.52594780921936\n",
            "5800: train loss: 0.03234255287982308; test loss: 0.025728289037942886; train accuracy: 0.9902605348124732; test accuracy: 0.9922999739646912; Time: 93.07832646369934\n",
            "5900: train loss: 0.030889748895542392; test loss: 0.02482379972934723; train accuracy: 0.9904529993626963; test accuracy: 0.9923999905586243; Time: 94.63056683540344\n",
            "6000: train loss: 0.02785958022375697; test loss: 0.024708861485123634; train accuracy: 0.9912105179089373; test accuracy: 0.9916999936103821; Time: 96.18344712257385\n",
            "6100: train loss: 0.03148498765719991; test loss: 0.027024170383810997; train accuracy: 0.9903621202081649; test accuracy: 0.9915000200271606; Time: 97.73561239242554\n",
            "6200: train loss: 0.03041633996419389; test loss: 0.03442850708961487; train accuracy: 0.9910480128612725; test accuracy: 0.9907000064849854; Time: 99.28772115707397\n",
            "6300: train loss: 0.025011869166153958; test loss: 0.02569938264787197; train accuracy: 0.9923481418183556; test accuracy: 0.9922000169754028; Time: 100.84495401382446\n",
            "6400: train loss: 0.02738918479303702; test loss: 0.028840014711022377; train accuracy: 0.9914258526354236; test accuracy: 0.9914000034332275; Time: 102.40251636505127\n",
            "6500: train loss: 0.02794766120917903; test loss: 0.02574295736849308; train accuracy: 0.9911703314196869; test accuracy: 0.9927999973297119; Time: 103.96472883224487\n",
            "6600: train loss: 0.029017029326481543; test loss: 0.026497283950448036; train accuracy: 0.9912064212151792; test accuracy: 0.9926000237464905; Time: 105.53836464881897\n",
            "6700: train loss: 0.026788330898261298; test loss: 0.03165736049413681; train accuracy: 0.9918575221936538; test accuracy: 0.9904000163078308; Time: 107.09940600395203\n",
            "6800: train loss: 0.025794526864808633; test loss: 0.02814430557191372; train accuracy: 0.9915029878224783; test accuracy: 0.9916999936103821; Time: 108.65266680717468\n",
            "6900: train loss: 0.02615608731763992; test loss: 0.03471522778272629; train accuracy: 0.9920178263353115; test accuracy: 0.9909999966621399; Time: 110.20867323875427\n",
            "7000: train loss: 0.02783442967650154; test loss: 0.029018549248576164; train accuracy: 0.9915120694349373; test accuracy: 0.9936000108718872; Time: 111.7621374130249\n",
            "7100: train loss: 0.02616886170751854; test loss: 0.031408943235874176; train accuracy: 0.9921088438502363; test accuracy: 0.9921000003814697; Time: 113.31494140625\n",
            "7200: train loss: 0.029462167374756015; test loss: 0.028970742598176003; train accuracy: 0.99205309783093; test accuracy: 0.9923999905586243; Time: 114.87021040916443\n",
            "7300: train loss: 0.028091419054185694; test loss: 0.03181867673993111; train accuracy: 0.9913341953500674; test accuracy: 0.991599977016449; Time: 116.42163395881653\n",
            "7400: train loss: 0.02646446520014816; test loss: 0.028185980394482613; train accuracy: 0.991783079902019; test accuracy: 0.9921000003814697; Time: 117.97222805023193\n",
            "7500: train loss: 0.030054758917007766; test loss: 0.03217016160488129; train accuracy: 0.9912173181208229; test accuracy: 0.9919999837875366; Time: 119.52508354187012\n",
            "7600: train loss: 0.0313904681988186; test loss: 0.0335787832736969; train accuracy: 0.9913245999329612; test accuracy: 0.9916999936103821; Time: 121.07714557647705\n",
            "7700: train loss: 0.028688299144269902; test loss: 0.029830163344740868; train accuracy: 0.9918534322862133; test accuracy: 0.9919000267982483; Time: 122.62667751312256\n",
            "7800: train loss: 0.029451972195845208; test loss: 0.04152831435203552; train accuracy: 0.9921059369805452; test accuracy: 0.9891999959945679; Time: 124.17934036254883\n",
            "7900: train loss: 0.029715483764026593; test loss: 0.03601668030023575; train accuracy: 0.9906540085343479; test accuracy: 0.9919999837875366; Time: 125.73179769515991\n",
            "8000: train loss: 0.029918617469580595; test loss: 0.03894621878862381; train accuracy: 0.9913336162764418; test accuracy: 0.9908000230789185; Time: 127.2874026298523\n",
            "8100: train loss: 0.030625005502911074; test loss: 0.034955911338329315; train accuracy: 0.9914827015771182; test accuracy: 0.9898999929428101; Time: 128.8403787612915\n",
            "8200: train loss: 0.0321839612399409; test loss: 0.028913794085383415; train accuracy: 0.9909729885601983; test accuracy: 0.991599977016449; Time: 130.38862252235413\n",
            "8300: train loss: 0.026206193138616284; test loss: 0.029134295880794525; train accuracy: 0.9922216882517978; test accuracy: 0.9926999807357788; Time: 131.93924713134766\n",
            "8400: train loss: 0.02507570457301325; test loss: 0.03010363131761551; train accuracy: 0.9921463998134302; test accuracy: 0.9915000200271606; Time: 133.49086689949036\n",
            "8500: train loss: 0.026585022135918367; test loss: 0.031169192865490913; train accuracy: 0.9917911479741652; test accuracy: 0.9915000200271606; Time: 135.0465133190155\n",
            "8600: train loss: 0.025498876741066146; test loss: 0.030379723757505417; train accuracy: 0.9923069335494706; test accuracy: 0.991599977016449; Time: 136.59960651397705\n",
            "8700: train loss: 0.027212925357080007; test loss: 0.038698967546224594; train accuracy: 0.9925272434167232; test accuracy: 0.9905999898910522; Time: 138.1527910232544\n",
            "8800: train loss: 0.027945955129550608; test loss: 0.03244291618466377; train accuracy: 0.9916112434501084; test accuracy: 0.9912999868392944; Time: 139.7038984298706\n",
            "8900: train loss: 0.029299874046207305; test loss: 0.03486352041363716; train accuracy: 0.991676465610887; test accuracy: 0.9919999837875366; Time: 141.254403591156\n",
            "9000: train loss: 0.02606927789031002; test loss: 0.03200072422623634; train accuracy: 0.9926976423231187; test accuracy: 0.9915000200271606; Time: 142.81096601486206\n",
            "9100: train loss: 0.0248325742961512; test loss: 0.0325855053961277; train accuracy: 0.9929126583649773; test accuracy: 0.9919999837875366; Time: 144.36367225646973\n",
            "9200: train loss: 0.02748167180702952; test loss: 0.03245304152369499; train accuracy: 0.9927204483308136; test accuracy: 0.9918000102043152; Time: 145.91612195968628\n",
            "9300: train loss: 0.025901966166938666; test loss: 0.03223814815282822; train accuracy: 0.9917483224117236; test accuracy: 0.9922999739646912; Time: 147.46933960914612\n",
            "9400: train loss: 0.02661374559056359; test loss: 0.03344757854938507; train accuracy: 0.9919447257249592; test accuracy: 0.9923999905586243; Time: 149.03116488456726\n",
            "9500: train loss: 0.02379894440729445; test loss: 0.03000299073755741; train accuracy: 0.9924227408682527; test accuracy: 0.9921000003814697; Time: 150.59924507141113\n",
            "9600: train loss: 0.027344903913398624; test loss: 0.03439360857009888; train accuracy: 0.9919760257246492; test accuracy: 0.9918000102043152; Time: 152.17385125160217\n",
            "9700: train loss: 0.02653455717240525; test loss: 0.032907478511333466; train accuracy: 0.9919842674834756; test accuracy: 0.991100013256073; Time: 153.74236416816711\n",
            "9800: train loss: 0.026112050407839326; test loss: 0.0318891778588295; train accuracy: 0.9925480123186367; test accuracy: 0.9914000034332275; Time: 155.3112177848816\n",
            "9900: train loss: 0.02881544682715783; test loss: 0.026385119184851646; train accuracy: 0.991831832128507; test accuracy: 0.9918000102043152; Time: 156.88046836853027\n",
            "[[ 972    0    0    0    0    0    5    1    2    0]\n",
            " [   0 1131    2    2    0    0    0    0    0    0]\n",
            " [   1    1 1025    1    0    1    0    1    2    0]\n",
            " [   0    0    2 1002    0    5    0    0    0    1]\n",
            " [   0    1    0    0  969    0    0    0    0   12]\n",
            " [   0    0    0    3    0  887    1    0    0    1]\n",
            " [   4    1    1    0    1    5  942    0    4    0]\n",
            " [   0    1    3    2    1    0    0 1015    2    4]\n",
            " [   1    0    3    1    2    0    0    1  962    4]\n",
            " [   1    0    0    1    3    0    0    0    1 1003]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9928    0.9918    0.9923       980\n",
            "           1     0.9965    0.9965    0.9965      1135\n",
            "           2     0.9894    0.9932    0.9913      1032\n",
            "           3     0.9901    0.9921    0.9911      1010\n",
            "           4     0.9928    0.9868    0.9898       982\n",
            "           5     0.9878    0.9944    0.9911       892\n",
            "           6     0.9937    0.9833    0.9885       958\n",
            "           7     0.9971    0.9874    0.9922      1028\n",
            "           8     0.9887    0.9877    0.9882       974\n",
            "           9     0.9785    0.9941    0.9862      1009\n",
            "\n",
            "    accuracy                         0.9908     10000\n",
            "   macro avg     0.9907    0.9907    0.9907     10000\n",
            "weighted avg     0.9908    0.9908    0.9908     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q09_Jk8ex4cW",
        "colab_type": "text"
      },
      "source": [
        "# KRON & SCAN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yREe1JYGBpKL",
        "colab_type": "text"
      },
      "source": [
        "## Scan + Kron AVERAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y61sJ-LKBuqg",
        "colab_type": "code",
        "outputId": "9ef57caa-48ad-41b5-d01f-39cf24a4104f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# grads ---> kron_grads ---> scan_grads\n",
        "\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1, W.shape.as_list()[0])),tf.zeros((1, W.shape.as_list()[0]))], axis=0), trainable=False) for W in Ws]\n",
        "    qs_right = [tf.Variable(tf.ones((1, W.shape.as_list()[1])), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    scan_grads = [psgd.precond_grad_scan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    kron_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, grads)]\n",
        "    precond_grads = [(s+k)/2 for (s,k) in zip(scan_grads, kron_grads)]\n",
        "\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in precond_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws, precond_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    new_qs = [psgd.update_precond_scan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "  \n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "        \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _,_ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'scan_kron_avg.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.39605712890625; test loss: 2.278923273086548; train accuracy: 0.0703125; test accuracy: 0.13449999690055847; Time: 3.580127239227295\n",
            "100: train loss: 0.9646626208752392; test loss: 0.1982114017009735; train accuracy: 0.7088625; test accuracy: 0.9476000070571899; Time: 5.319075345993042\n",
            "200: train loss: 0.51470116346077; test loss: 0.09686628729104996; train accuracy: 0.8433943271644563; test accuracy: 0.9732000231742859; Time: 7.038480520248413\n",
            "300: train loss: 0.2836559792185827; test loss: 0.07133886218070984; train accuracy: 0.9152173601127125; test accuracy: 0.978600025177002; Time: 8.764195680618286\n",
            "400: train loss: 0.18007598716941076; test loss: 0.04824352264404297; train accuracy: 0.9451301947411288; test accuracy: 0.9855999946594238; Time: 10.489600896835327\n",
            "500: train loss: 0.12124288467226452; test loss: 0.04143662005662918; train accuracy: 0.9628255803023215; test accuracy: 0.9866999983787537; Time: 12.214603900909424\n",
            "600: train loss: 0.0989577408634995; test loss: 0.04198107123374939; train accuracy: 0.9694104484011601; test accuracy: 0.9879000186920166; Time: 13.940243005752563\n",
            "700: train loss: 0.07793846843942978; test loss: 0.04103194549679756; train accuracy: 0.9761327865612313; test accuracy: 0.9878000020980835; Time: 15.662561655044556\n",
            "800: train loss: 0.0742193158088017; test loss: 0.039343904703855515; train accuracy: 0.9775585412227464; test accuracy: 0.987500011920929; Time: 17.3831148147583\n",
            "900: train loss: 0.06591956597539456; test loss: 0.03204772621393204; train accuracy: 0.9803011529566419; test accuracy: 0.9900000095367432; Time: 19.107710123062134\n",
            "1000: train loss: 0.0587871275570041; test loss: 0.031834907829761505; train accuracy: 0.982358509202623; test accuracy: 0.989799976348877; Time: 20.829487323760986\n",
            "1100: train loss: 0.05573710675140494; test loss: 0.030027203261852264; train accuracy: 0.9834932418057928; test accuracy: 0.9897000193595886; Time: 22.55009651184082\n",
            "1200: train loss: 0.05100747099067389; test loss: 0.02563505247235298; train accuracy: 0.9845696562190828; test accuracy: 0.9922000169754028; Time: 24.270548105239868\n",
            "1300: train loss: 0.049494434059305425; test loss: 0.03356659412384033; train accuracy: 0.9839965004941176; test accuracy: 0.9896000027656555; Time: 25.990870714187622\n",
            "1400: train loss: 0.04364172112245846; test loss: 0.02272397093474865; train accuracy: 0.9862216695361891; test accuracy: 0.9921000003814697; Time: 27.712148666381836\n",
            "1500: train loss: 0.04054491687903847; test loss: 0.02435600571334362; train accuracy: 0.987528151462393; test accuracy: 0.9919999837875366; Time: 29.43436884880066\n",
            "1600: train loss: 0.039661808651073584; test loss: 0.02302682213485241; train accuracy: 0.9877349910543184; test accuracy: 0.9915000200271606; Time: 31.15682363510132\n",
            "1700: train loss: 0.037773049984200495; test loss: 0.024417759850621223; train accuracy: 0.9883084934220907; test accuracy: 0.991599977016449; Time: 32.878121852874756\n",
            "1800: train loss: 0.03537943244039769; test loss: 0.022530848160386086; train accuracy: 0.9888979653146105; test accuracy: 0.9925000071525574; Time: 34.5987868309021\n",
            "1900: train loss: 0.03419320545003245; test loss: 0.02619817480444908; train accuracy: 0.9893205748375288; test accuracy: 0.9912999868392944; Time: 36.32220387458801\n",
            "2000: train loss: 0.03448101425074432; test loss: 0.02282657101750374; train accuracy: 0.9882254282320617; test accuracy: 0.9922999739646912; Time: 38.04661965370178\n",
            "2100: train loss: 0.03597341255514868; test loss: 0.019727639853954315; train accuracy: 0.9883151586578969; test accuracy: 0.9927999973297119; Time: 39.7712676525116\n",
            "2200: train loss: 0.03300702390995712; test loss: 0.025460653007030487; train accuracy: 0.9899614510593818; test accuracy: 0.991100013256073; Time: 41.49353647232056\n",
            "2300: train loss: 0.02976625772231412; test loss: 0.021615050733089447; train accuracy: 0.9903735237675104; test accuracy: 0.992900013923645; Time: 43.2166006565094\n",
            "2400: train loss: 0.030131112615704086; test loss: 0.024255089461803436; train accuracy: 0.9903169008387498; test accuracy: 0.9919000267982483; Time: 44.94083619117737\n",
            "2500: train loss: 0.02961923624776479; test loss: 0.022045500576496124; train accuracy: 0.9910368322806925; test accuracy: 0.9916999936103821; Time: 46.665810108184814\n",
            "2600: train loss: 0.02613771249317762; test loss: 0.020568793639540672; train accuracy: 0.9917920228721163; test accuracy: 0.9934999942779541; Time: 48.384047508239746\n",
            "2700: train loss: 0.030026713672998143; test loss: 0.02286936715245247; train accuracy: 0.9906986330949763; test accuracy: 0.9926000237464905; Time: 50.10457396507263\n",
            "2800: train loss: 0.027209089740789335; test loss: 0.025147341191768646; train accuracy: 0.9914120177965695; test accuracy: 0.9911999702453613; Time: 51.82596492767334\n",
            "2900: train loss: 0.026797760579168432; test loss: 0.02549450285732746; train accuracy: 0.9915762087427384; test accuracy: 0.9919999837875366; Time: 53.5471510887146\n",
            "3000: train loss: 0.025500171792056235; test loss: 0.019838709384202957; train accuracy: 0.9920439328271143; test accuracy: 0.9933000206947327; Time: 55.26856708526611\n",
            "3100: train loss: 0.02431148957429125; test loss: 0.022583454847335815; train accuracy: 0.9919817804123121; test accuracy: 0.9921000003814697; Time: 56.98994827270508\n",
            "3200: train loss: 0.02489379841492213; test loss: 0.02288135699927807; train accuracy: 0.9917426273585344; test accuracy: 0.9932000041007996; Time: 58.71282410621643\n",
            "3300: train loss: 0.024408034733316694; test loss: 0.02142373099923134; train accuracy: 0.9916866110596205; test accuracy: 0.9922999739646912; Time: 60.433849573135376\n",
            "3400: train loss: 0.024215703868505502; test loss: 0.02097061090171337; train accuracy: 0.9919768107698803; test accuracy: 0.9934999942779541; Time: 62.15909242630005\n",
            "3500: train loss: 0.022550155656484907; test loss: 0.022970935329794884; train accuracy: 0.9921635893818309; test accuracy: 0.9922999739646912; Time: 63.88241720199585\n",
            "3600: train loss: 0.022626742106381024; test loss: 0.02504142001271248; train accuracy: 0.9924771408870665; test accuracy: 0.9923999905586243; Time: 65.60746645927429\n",
            "3700: train loss: 0.020824369016038163; test loss: 0.02121046930551529; train accuracy: 0.993108265582126; test accuracy: 0.9930999875068665; Time: 67.33071374893188\n",
            "3800: train loss: 0.020263476289664312; test loss: 0.02146664820611477; train accuracy: 0.9937703867019252; test accuracy: 0.9936000108718872; Time: 69.05101442337036\n",
            "3900: train loss: 0.023683932621966483; test loss: 0.023538880050182343; train accuracy: 0.9925134825667449; test accuracy: 0.9926000237464905; Time: 70.77489900588989\n",
            "4000: train loss: 0.02085029576269061; test loss: 0.021947361528873444; train accuracy: 0.9929433650698513; test accuracy: 0.9926000237464905; Time: 72.49309301376343\n",
            "4100: train loss: 0.018806411230379086; test loss: 0.02120158262550831; train accuracy: 0.9938693554438044; test accuracy: 0.9932000041007996; Time: 74.21426558494568\n",
            "4200: train loss: 0.017252029897912215; test loss: 0.019811490550637245; train accuracy: 0.9944750696602944; test accuracy: 0.9934999942779541; Time: 75.93772006034851\n",
            "4300: train loss: 0.020593721646872936; test loss: 0.01883338950574398; train accuracy: 0.9930800445681898; test accuracy: 0.9944000244140625; Time: 77.66158866882324\n",
            "4400: train loss: 0.015506434455874667; test loss: 0.024971086531877518; train accuracy: 0.9949922155277809; test accuracy: 0.9921000003814697; Time: 79.3836874961853\n",
            "4500: train loss: 0.02003029943235987; test loss: 0.023382114246487617; train accuracy: 0.9937616007927607; test accuracy: 0.9916999936103821; Time: 81.10754132270813\n",
            "4600: train loss: 0.02176318618975823; test loss: 0.01845264807343483; train accuracy: 0.9934110175494465; test accuracy: 0.9940999746322632; Time: 82.82791090011597\n",
            "4700: train loss: 0.018570947082450988; test loss: 0.019486861303448677; train accuracy: 0.994012791299956; test accuracy: 0.9940999746322632; Time: 84.55391097068787\n",
            "4800: train loss: 0.01715038908265596; test loss: 0.024171270430088043; train accuracy: 0.994019563679492; test accuracy: 0.9927999973297119; Time: 86.27916884422302\n",
            "4900: train loss: 0.016643603476843294; test loss: 0.022025713697075844; train accuracy: 0.9943198909305284; test accuracy: 0.993399977684021; Time: 88.00420427322388\n",
            "5000: train loss: 0.017909571190744895; test loss: 0.018986733630299568; train accuracy: 0.9940622343084852; test accuracy: 0.9934999942779541; Time: 89.72433423995972\n",
            "5100: train loss: 0.01801795380750455; test loss: 0.017140543088316917; train accuracy: 0.9947165265094418; test accuracy: 0.9945999979972839; Time: 91.4480390548706\n",
            "5200: train loss: 0.017386960249908257; test loss: 0.01972496323287487; train accuracy: 0.9946612503962412; test accuracy: 0.9934999942779541; Time: 93.16935443878174\n",
            "5300: train loss: 0.01791035564555801; test loss: 0.021065987646579742; train accuracy: 0.9945766978283354; test accuracy: 0.9927999973297119; Time: 94.89204001426697\n",
            "5400: train loss: 0.019830365891212252; test loss: 0.018651597201824188; train accuracy: 0.9938667955174174; test accuracy: 0.993399977684021; Time: 96.61504650115967\n",
            "5500: train loss: 0.015371565981900112; test loss: 0.027701474726200104; train accuracy: 0.9953901406429477; test accuracy: 0.9915000200271606; Time: 98.34084224700928\n",
            "5600: train loss: 0.017395161108718665; test loss: 0.02170480601489544; train accuracy: 0.9948727474007488; test accuracy: 0.9937000274658203; Time: 100.0629403591156\n",
            "5700: train loss: 0.014328589483094656; test loss: 0.01642211526632309; train accuracy: 0.9954048955982627; test accuracy: 0.9945999979972839; Time: 101.7847695350647\n",
            "5800: train loss: 0.014816599508918348; test loss: 0.019177965819835663; train accuracy: 0.9952887612967899; test accuracy: 0.9940000176429749; Time: 103.50828790664673\n",
            "5900: train loss: 0.013653947297367552; test loss: 0.02042367495596409; train accuracy: 0.9956272278558199; test accuracy: 0.9939000010490417; Time: 105.22995662689209\n",
            "6000: train loss: 0.013925997645550118; test loss: 0.023449664935469627; train accuracy: 0.9957707016229361; test accuracy: 0.9927999973297119; Time: 106.94811654090881\n",
            "6100: train loss: 0.015400600437436381; test loss: 0.01854984648525715; train accuracy: 0.9952011930971052; test accuracy: 0.9940000176429749; Time: 108.66917848587036\n",
            "6200: train loss: 0.013579377369479156; test loss: 0.01930125802755356; train accuracy: 0.9954961482540305; test accuracy: 0.9936000108718872; Time: 110.39167428016663\n",
            "6300: train loss: 0.011838563466330882; test loss: 0.019622165709733963; train accuracy: 0.9960448946416487; test accuracy: 0.9943000078201294; Time: 112.11442399024963\n",
            "6400: train loss: 0.012623655995533209; test loss: 0.024219922721385956; train accuracy: 0.9961406801116396; test accuracy: 0.9926999807357788; Time: 113.83635663986206\n",
            "6500: train loss: 0.0123107563343365; test loss: 0.02068844623863697; train accuracy: 0.996399331154688; test accuracy: 0.9939000010490417; Time: 115.55675649642944\n",
            "6600: train loss: 0.012931083311422136; test loss: 0.01807071454823017; train accuracy: 0.9958567279420409; test accuracy: 0.9951000213623047; Time: 117.28039622306824\n",
            "6700: train loss: 0.013632296119514204; test loss: 0.01785883866250515; train accuracy: 0.9961715719714724; test accuracy: 0.9944000244140625; Time: 119.00110793113708\n",
            "6800: train loss: 0.012152642412923685; test loss: 0.020917586982250214; train accuracy: 0.9959425409853938; test accuracy: 0.9934999942779541; Time: 120.72135591506958\n",
            "6900: train loss: 0.013194909004458898; test loss: 0.02005576901137829; train accuracy: 0.9954624667927905; test accuracy: 0.9934999942779541; Time: 122.44248700141907\n",
            "7000: train loss: 0.012369276842270339; test loss: 0.03008069097995758; train accuracy: 0.9959159381855172; test accuracy: 0.9922999739646912; Time: 124.16141867637634\n",
            "7100: train loss: 0.013051505365756241; test loss: 0.019028035923838615; train accuracy: 0.9959832024828718; test accuracy: 0.9944000244140625; Time: 125.88419961929321\n",
            "7200: train loss: 0.01242988760454739; test loss: 0.020317962393164635; train accuracy: 0.9959680262452333; test accuracy: 0.9939000010490417; Time: 127.60423135757446\n",
            "7300: train loss: 0.011633551013559977; test loss: 0.020102985203266144; train accuracy: 0.996190830732885; test accuracy: 0.9944999814033508; Time: 129.32550477981567\n",
            "7400: train loss: 0.01155376388307466; test loss: 0.01827278360724449; train accuracy: 0.996365861111443; test accuracy: 0.9944000244140625; Time: 131.0479452610016\n",
            "7500: train loss: 0.012059656289999393; test loss: 0.01840331219136715; train accuracy: 0.9959236592082107; test accuracy: 0.9940999746322632; Time: 132.7723867893219\n",
            "7600: train loss: 0.012111229985209497; test loss: 0.02254185825586319; train accuracy: 0.9956212164422221; test accuracy: 0.992900013923645; Time: 134.49196982383728\n",
            "7700: train loss: 0.010622987173550598; test loss: 0.018062323331832886; train accuracy: 0.9965342596072754; test accuracy: 0.9944999814033508; Time: 136.2134084701538\n",
            "7800: train loss: 0.010883401680620219; test loss: 0.022527512162923813; train accuracy: 0.9961679555949441; test accuracy: 0.9937000274658203; Time: 137.9474585056305\n",
            "7900: train loss: 0.011262667372847389; test loss: 0.024315521121025085; train accuracy: 0.9962450940691338; test accuracy: 0.9926999807357788; Time: 139.67977738380432\n",
            "8000: train loss: 0.009842570229166648; test loss: 0.024348756298422813; train accuracy: 0.9967416325186526; test accuracy: 0.9930999875068665; Time: 141.40517115592957\n",
            "8100: train loss: 0.0122375129125951; test loss: 0.020418990403413773; train accuracy: 0.9959078701274431; test accuracy: 0.9941999912261963; Time: 143.12968039512634\n",
            "8200: train loss: 0.011056182971837594; test loss: 0.021803639829158783; train accuracy: 0.9963547980831696; test accuracy: 0.9940999746322632; Time: 144.85273504257202\n",
            "8300: train loss: 0.008812225442988165; test loss: 0.0231539998203516; train accuracy: 0.9969091896405027; test accuracy: 0.9933000206947327; Time: 146.57246851921082\n",
            "8400: train loss: 0.011299438527281904; test loss: 0.024335559457540512; train accuracy: 0.9960996962310432; test accuracy: 0.9937999844551086; Time: 148.29654693603516\n",
            "8500: train loss: 0.010177895103011427; test loss: 0.021267453208565712; train accuracy: 0.9964065140197763; test accuracy: 0.9940000176429749; Time: 150.01560807228088\n",
            "8600: train loss: 0.009757541166261135; test loss: 0.02188793569803238; train accuracy: 0.9965034568287253; test accuracy: 0.9941999912261963; Time: 151.73577618598938\n",
            "8700: train loss: 0.009526948289951872; test loss: 0.02031043916940689; train accuracy: 0.9967692084781223; test accuracy: 0.9948999881744385; Time: 153.46277236938477\n",
            "8800: train loss: 0.00965549868847932; test loss: 0.02212219312787056; train accuracy: 0.9969014528073961; test accuracy: 0.9941999912261963; Time: 155.18204998970032\n",
            "8900: train loss: 0.009222856219224944; test loss: 0.023411205038428307; train accuracy: 0.9966610120829824; test accuracy: 0.993399977684021; Time: 156.90500569343567\n",
            "9000: train loss: 0.008881529414480989; test loss: 0.02105586789548397; train accuracy: 0.9970301770841274; test accuracy: 0.9937000274658203; Time: 158.6273820400238\n",
            "9100: train loss: 0.008555953373597212; test loss: 0.021993430331349373; train accuracy: 0.9971256239519994; test accuracy: 0.9937000274658203; Time: 160.34983372688293\n",
            "9200: train loss: 0.007640332024410691; test loss: 0.01949959807097912; train accuracy: 0.9976657099747591; test accuracy: 0.9944000244140625; Time: 162.07400679588318\n",
            "9300: train loss: 0.008419621240161984; test loss: 0.024499664083123207; train accuracy: 0.9971451285221269; test accuracy: 0.993399977684021; Time: 163.79698824882507\n",
            "9400: train loss: 0.009826761141035334; test loss: 0.018800418823957443; train accuracy: 0.996684729817399; test accuracy: 0.9947999715805054; Time: 165.51963067054749\n",
            "9500: train loss: 0.008413692868425176; test loss: 0.02623533271253109; train accuracy: 0.9971552670613001; test accuracy: 0.9926999807357788; Time: 167.24507880210876\n",
            "9600: train loss: 0.007041802169694319; test loss: 0.023746216669678688; train accuracy: 0.9977117533926549; test accuracy: 0.9932000041007996; Time: 168.96435403823853\n",
            "9700: train loss: 0.006934725150877544; test loss: 0.02161240205168724; train accuracy: 0.997737228035474; test accuracy: 0.9940999746322632; Time: 170.68382477760315\n",
            "9800: train loss: 0.009714820516473379; test loss: 0.02341199479997158; train accuracy: 0.9973923391254209; test accuracy: 0.993399977684021; Time: 172.40749835968018\n",
            "9900: train loss: 0.008455096015569914; test loss: 0.02400144189596176; train accuracy: 0.9973573570443652; test accuracy: 0.9929999709129333; Time: 174.12932705879211\n",
            "[[ 978    0    0    0    0    0    0    1    1    0]\n",
            " [   0 1130    1    1    1    0    0    2    0    0]\n",
            " [   1    1 1028    0    0    0    1    0    1    0]\n",
            " [   0    0    0 1006    0    3    0    1    0    0]\n",
            " [   0    0    0    0  976    0    0    0    0    6]\n",
            " [   0    0    0    2    0  888    1    0    1    0]\n",
            " [   3    2    0    1    0    3  946    0    3    0]\n",
            " [   0    1    4    1    0    0    0 1020    0    2]\n",
            " [   1    0    0    1    0    0    1    0  971    0]\n",
            " [   0    0    0    0    5    1    0    3    4  996]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9949    0.9980    0.9964       980\n",
            "           1     0.9965    0.9956    0.9960      1135\n",
            "           2     0.9952    0.9961    0.9956      1032\n",
            "           3     0.9941    0.9960    0.9951      1010\n",
            "           4     0.9939    0.9939    0.9939       982\n",
            "           5     0.9922    0.9955    0.9938       892\n",
            "           6     0.9968    0.9875    0.9921       958\n",
            "           7     0.9932    0.9922    0.9927      1028\n",
            "           8     0.9898    0.9969    0.9934       974\n",
            "           9     0.9920    0.9871    0.9896      1009\n",
            "\n",
            "    accuracy                         0.9939     10000\n",
            "   macro avg     0.9939    0.9939    0.9939     10000\n",
            "weighted avg     0.9939    0.9939    0.9939     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY3LQ3EgSjTX",
        "colab_type": "text"
      },
      "source": [
        "## Scan -> Kron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ-Wu4hBiOcx",
        "colab_type": "code",
        "outputId": "6e5a6330-b2ef-45cf-e52f-3a99ab92a114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# grads ---> kron_grads ---> scan_grads\n",
        "\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1, W.shape.as_list()[0])),tf.zeros((1, W.shape.as_list()[0]))], axis=0), trainable=False) for W in Ws]\n",
        "    qs_right = [tf.Variable(tf.ones((1, W.shape.as_list()[1])), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    scan_grads = [psgd.precond_grad_scan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, grads)]\n",
        "    kron_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, scan_grads)]\n",
        "    \n",
        "\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in kron_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws, kron_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    new_qs = [psgd.update_precond_scan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)   \n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    t = 0\n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "        \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy, _, _,_ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'scan_kron.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3781280517578125; test loss: 2.277127265930176; train accuracy: 0.0859375; test accuracy: 0.1200999990105629; Time: 4.070016860961914\n",
            "100: train loss: 0.7676858145534993; test loss: 0.09397528320550919; train accuracy: 0.7621632812499998; test accuracy: 0.9725000262260437; Time: 5.792638778686523\n",
            "200: train loss: 0.36941787397162573; test loss: 0.05511372163891792; train accuracy: 0.8865091566246319; test accuracy: 0.984499990940094; Time: 7.506967306137085\n",
            "300: train loss: 0.19741662952486794; test loss: 0.04214391112327576; train accuracy: 0.9401871865058342; test accuracy: 0.9876999855041504; Time: 9.223902940750122\n",
            "400: train loss: 0.11716683577549737; test loss: 0.034236837178468704; train accuracy: 0.9656504383839707; test accuracy: 0.9894999861717224; Time: 10.942944765090942\n",
            "500: train loss: 0.08279337729603953; test loss: 0.031033361330628395; train accuracy: 0.9757425624348106; test accuracy: 0.9908999800682068; Time: 12.666457891464233\n",
            "600: train loss: 0.06558767772825455; test loss: 0.031000573188066483; train accuracy: 0.980573470627665; test accuracy: 0.9894000291824341; Time: 14.376853227615356\n",
            "700: train loss: 0.052831329636382464; test loss: 0.028848635032773018; train accuracy: 0.9846534624651041; test accuracy: 0.9900000095367432; Time: 16.091418981552124\n",
            "800: train loss: 0.0486161234313973; test loss: 0.026860076934099197; train accuracy: 0.9853915463758931; test accuracy: 0.9918000102043152; Time: 17.802938222885132\n",
            "900: train loss: 0.041599874231102486; test loss: 0.024829095229506493; train accuracy: 0.9876434068501294; test accuracy: 0.9919000267982483; Time: 19.514976501464844\n",
            "1000: train loss: 0.0411908142043103; test loss: 0.02377522923052311; train accuracy: 0.9878419525769364; test accuracy: 0.9921000003814697; Time: 21.23062562942505\n",
            "1100: train loss: 0.03950857585873979; test loss: 0.023175343871116638; train accuracy: 0.988515643122243; test accuracy: 0.9923999905586243; Time: 22.95224642753601\n",
            "1200: train loss: 0.037139180184717575; test loss: 0.02229227125644684; train accuracy: 0.9883649934822927; test accuracy: 0.9923999905586243; Time: 24.667819261550903\n",
            "1300: train loss: 0.036009130604278794; test loss: 0.022164661437273026; train accuracy: 0.9896967549654535; test accuracy: 0.9926000237464905; Time: 26.386810779571533\n",
            "1400: train loss: 0.03466132194137316; test loss: 0.022222978994250298; train accuracy: 0.9903341005301414; test accuracy: 0.9922000169754028; Time: 28.11806559562683\n",
            "1500: train loss: 0.035047174247153735; test loss: 0.021327108144760132; train accuracy: 0.9900259274150554; test accuracy: 0.9926000237464905; Time: 29.852478504180908\n",
            "1600: train loss: 0.031515941075688725; test loss: 0.021237313747406006; train accuracy: 0.9904604269882967; test accuracy: 0.9930999875068665; Time: 31.57755947113037\n",
            "1700: train loss: 0.030644987023328716; test loss: 0.020352182909846306; train accuracy: 0.9902145995958527; test accuracy: 0.992900013923645; Time: 33.30673050880432\n",
            "1800: train loss: 0.0276929925308006; test loss: 0.018928350880742073; train accuracy: 0.9914127335143056; test accuracy: 0.9940999746322632; Time: 35.046722412109375\n",
            "1900: train loss: 0.030202507100920466; test loss: 0.019893890246748924; train accuracy: 0.9904250387634921; test accuracy: 0.992900013923645; Time: 36.799829721450806\n",
            "2000: train loss: 0.02527428026139286; test loss: 0.019578665494918823; train accuracy: 0.9922988036768414; test accuracy: 0.992900013923645; Time: 38.59396004676819\n",
            "2100: train loss: 0.028206722441734097; test loss: 0.01954510062932968; train accuracy: 0.9920284463086759; test accuracy: 0.9934999942779541; Time: 40.33125686645508\n",
            "2200: train loss: 0.0269197390884549; test loss: 0.019940124824643135; train accuracy: 0.9919329826664217; test accuracy: 0.9930999875068665; Time: 42.06219124794006\n",
            "2300: train loss: 0.025965587198992317; test loss: 0.02003556862473488; train accuracy: 0.9923004430297305; test accuracy: 0.9932000041007996; Time: 43.79426670074463\n",
            "2400: train loss: 0.024748412216968108; test loss: 0.019904818385839462; train accuracy: 0.9924804163951784; test accuracy: 0.9934999942779541; Time: 45.521880865097046\n",
            "2500: train loss: 0.026372073177891576; test loss: 0.01945984549820423; train accuracy: 0.9918916355819318; test accuracy: 0.9929999709129333; Time: 47.24579668045044\n",
            "2600: train loss: 0.026297737260074325; test loss: 0.019582286477088928; train accuracy: 0.9918524862573258; test accuracy: 0.992900013923645; Time: 48.99518013000488\n",
            "2700: train loss: 0.022036783482690975; test loss: 0.019892478361725807; train accuracy: 0.9931111738248221; test accuracy: 0.9930999875068665; Time: 50.726929664611816\n",
            "2800: train loss: 0.02059072324782621; test loss: 0.01937096007168293; train accuracy: 0.993641567597038; test accuracy: 0.9926000237464905; Time: 52.45765209197998\n",
            "2900: train loss: 0.01885855825725907; test loss: 0.019689857959747314; train accuracy: 0.9942569190556081; test accuracy: 0.9926999807357788; Time: 54.18066620826721\n",
            "3000: train loss: 0.022441098652315264; test loss: 0.01864578388631344; train accuracy: 0.9938562144052581; test accuracy: 0.9937000274658203; Time: 55.906620264053345\n",
            "3100: train loss: 0.02239194580774317; test loss: 0.018792757764458656; train accuracy: 0.993414008965271; test accuracy: 0.9927999973297119; Time: 57.64301371574402\n",
            "3200: train loss: 0.022302057471047586; test loss: 0.01876392588019371; train accuracy: 0.9931695888049853; test accuracy: 0.9937999844551086; Time: 59.37759447097778\n",
            "3300: train loss: 0.020401147621380667; test loss: 0.019210437312722206; train accuracy: 0.9939191034564195; test accuracy: 0.9941999912261963; Time: 61.10382056236267\n",
            "3400: train loss: 0.02108763807422195; test loss: 0.018404943868517876; train accuracy: 0.9935115509237171; test accuracy: 0.9940999746322632; Time: 62.833410024642944\n",
            "3500: train loss: 0.019851859756704587; test loss: 0.01832525245845318; train accuracy: 0.9938636062737528; test accuracy: 0.9940000176429749; Time: 64.56206846237183\n",
            "3600: train loss: 0.017736058455409574; test loss: 0.018446367233991623; train accuracy: 0.9943509159260976; test accuracy: 0.9937000274658203; Time: 66.28966021537781\n",
            "3700: train loss: 0.01906611629441182; test loss: 0.019313016906380653; train accuracy: 0.9942426756253222; test accuracy: 0.9936000108718872; Time: 68.02014708518982\n",
            "3800: train loss: 0.017082030150352204; test loss: 0.018895510584115982; train accuracy: 0.9950802073891999; test accuracy: 0.9937000274658203; Time: 69.74819588661194\n",
            "3900: train loss: 0.018016918771755472; test loss: 0.018017547205090523; train accuracy: 0.9941477320850792; test accuracy: 0.9940999746322632; Time: 71.47864890098572\n",
            "4000: train loss: 0.017175820966654397; test loss: 0.018296316266059875; train accuracy: 0.9951497374237128; test accuracy: 0.9936000108718872; Time: 73.20511198043823\n",
            "4100: train loss: 0.018016499378234317; test loss: 0.017927153035998344; train accuracy: 0.9946677927119475; test accuracy: 0.9940000176429749; Time: 74.92591667175293\n",
            "4200: train loss: 0.017009949109643065; test loss: 0.017462996765971184; train accuracy: 0.9947016319174833; test accuracy: 0.9943000078201294; Time: 76.64762544631958\n",
            "4300: train loss: 0.017381259984294864; test loss: 0.017140164971351624; train accuracy: 0.9941695665933485; test accuracy: 0.9940000176429749; Time: 78.36720418930054\n",
            "4400: train loss: 0.015779460185796037; test loss: 0.017432188615202904; train accuracy: 0.9944982978462387; test accuracy: 0.9937999844551086; Time: 80.09367322921753\n",
            "4500: train loss: 0.01863090896608087; test loss: 0.017597289755940437; train accuracy: 0.9949416790968275; test accuracy: 0.9940999746322632; Time: 81.81333112716675\n",
            "4600: train loss: 0.0172625874193586; test loss: 0.01695290580391884; train accuracy: 0.9946444411531958; test accuracy: 0.9940999746322632; Time: 83.53782987594604\n",
            "4700: train loss: 0.017000243585774243; test loss: 0.017238764092326164; train accuracy: 0.9943405320243781; test accuracy: 0.9939000010490417; Time: 85.2733063697815\n",
            "4800: train loss: 0.01748940428606153; test loss: 0.0176177266985178; train accuracy: 0.9945274510417472; test accuracy: 0.9937999844551086; Time: 87.00604009628296\n",
            "4900: train loss: 0.015423021614210379; test loss: 0.018120910972356796; train accuracy: 0.9952728974046376; test accuracy: 0.9937000274658203; Time: 88.73226356506348\n",
            "5000: train loss: 0.016433251568558176; test loss: 0.018149925395846367; train accuracy: 0.995246167034246; test accuracy: 0.9936000108718872; Time: 90.4622950553894\n",
            "5100: train loss: 0.017071618602747043; test loss: 0.017607424408197403; train accuracy: 0.9954545472505671; test accuracy: 0.9937000274658203; Time: 92.18983030319214\n",
            "5200: train loss: 0.016325221148103017; test loss: 0.017632469534873962; train accuracy: 0.9952739463644137; test accuracy: 0.9934999942779541; Time: 93.9236171245575\n",
            "5300: train loss: 0.015475049170973088; test loss: 0.01740550994873047; train accuracy: 0.9954444001496102; test accuracy: 0.9940000176429749; Time: 95.65070176124573\n",
            "5400: train loss: 0.01558791506061657; test loss: 0.017567681148648262; train accuracy: 0.99535153576656; test accuracy: 0.9943000078201294; Time: 97.37784051895142\n",
            "5500: train loss: 0.015257133475995145; test loss: 0.01762102171778679; train accuracy: 0.9957561168842002; test accuracy: 0.9940999746322632; Time: 99.10278606414795\n",
            "5600: train loss: 0.01592191897287038; test loss: 0.01734214462339878; train accuracy: 0.9952020251671732; test accuracy: 0.9944000244140625; Time: 100.82718181610107\n",
            "5700: train loss: 0.016262579692318774; test loss: 0.01785033382475376; train accuracy: 0.9951422159378742; test accuracy: 0.993399977684021; Time: 102.55312323570251\n",
            "5800: train loss: 0.015345702065762783; test loss: 0.01716429553925991; train accuracy: 0.9951568890490374; test accuracy: 0.9941999912261963; Time: 104.27396416664124\n",
            "5900: train loss: 0.014424867437463811; test loss: 0.017249975353479385; train accuracy: 0.9955776309708853; test accuracy: 0.9940999746322632; Time: 106.00605249404907\n",
            "6000: train loss: 0.01473282718591882; test loss: 0.017600636929273605; train accuracy: 0.995724309794966; test accuracy: 0.9943000078201294; Time: 107.73583006858826\n",
            "6100: train loss: 0.016027105896250636; test loss: 0.017411215230822563; train accuracy: 0.9952412457324601; test accuracy: 0.9941999912261963; Time: 109.45673894882202\n",
            "6200: train loss: 0.014886436205881745; test loss: 0.01805107481777668; train accuracy: 0.995178636983513; test accuracy: 0.9940000176429749; Time: 111.18055558204651\n",
            "6300: train loss: 0.015197040170699264; test loss: 0.017921868711709976; train accuracy: 0.9959361401648772; test accuracy: 0.9940999746322632; Time: 112.91104888916016\n",
            "6400: train loss: 0.016231658659507184; test loss: 0.017250001430511475; train accuracy: 0.9955189826790586; test accuracy: 0.9941999912261963; Time: 114.64734601974487\n",
            "6500: train loss: 0.013784116061336919; test loss: 0.017174532637000084; train accuracy: 0.9962542175136097; test accuracy: 0.9941999912261963; Time: 116.38296175003052\n",
            "6600: train loss: 0.015175276057824115; test loss: 0.017747629433870316; train accuracy: 0.9958664657155097; test accuracy: 0.9937999844551086; Time: 118.11991596221924\n",
            "6700: train loss: 0.014779252696890333; test loss: 0.017444221302866936; train accuracy: 0.9961463519583612; test accuracy: 0.9941999912261963; Time: 119.86027336120605\n",
            "6800: train loss: 0.013668180316053538; test loss: 0.016853323206305504; train accuracy: 0.9962922403874247; test accuracy: 0.9941999912261963; Time: 121.59081983566284\n",
            "6900: train loss: 0.012335605168379804; test loss: 0.016812914982438087; train accuracy: 0.9966135525368758; test accuracy: 0.9944000244140625; Time: 123.32629656791687\n",
            "7000: train loss: 0.01310611637402252; test loss: 0.016777390614151955; train accuracy: 0.9963428699972334; test accuracy: 0.9943000078201294; Time: 125.05129766464233\n",
            "7100: train loss: 0.013891185773343239; test loss: 0.01723790541291237; train accuracy: 0.9959049687550159; test accuracy: 0.9943000078201294; Time: 126.7771966457367\n",
            "7200: train loss: 0.013675780264172599; test loss: 0.017058299854397774; train accuracy: 0.9964285842129423; test accuracy: 0.9943000078201294; Time: 128.512601852417\n",
            "7300: train loss: 0.012952488597371905; test loss: 0.01671868935227394; train accuracy: 0.996584349338735; test accuracy: 0.994700014591217; Time: 130.24614000320435\n",
            "7400: train loss: 0.011704064883800134; test loss: 0.016837462782859802; train accuracy: 0.996800896478377; test accuracy: 0.9941999912261963; Time: 131.95950889587402\n",
            "7500: train loss: 0.012620934737242982; test loss: 0.016842028126120567; train accuracy: 0.9960144059950014; test accuracy: 0.9940000176429749; Time: 133.67902827262878\n",
            "7600: train loss: 0.012481842021400789; test loss: 0.01716681569814682; train accuracy: 0.9963763993717691; test accuracy: 0.9943000078201294; Time: 135.40815091133118\n",
            "7700: train loss: 0.011491693422865318; test loss: 0.016902875155210495; train accuracy: 0.9967803233375567; test accuracy: 0.9940999746322632; Time: 137.1350862979889\n",
            "7800: train loss: 0.01078794182977343; test loss: 0.017160039395093918; train accuracy: 0.9966055795909905; test accuracy: 0.9940000176429749; Time: 138.85688400268555\n",
            "7900: train loss: 0.011702847527125058; test loss: 0.016886664554476738; train accuracy: 0.9966849907873331; test accuracy: 0.9943000078201294; Time: 140.58145809173584\n",
            "8000: train loss: 0.013237477465994709; test loss: 0.017007267102599144; train accuracy: 0.9963374881013745; test accuracy: 0.9943000078201294; Time: 142.3030138015747\n",
            "8100: train loss: 0.011098519174621303; test loss: 0.01731577143073082; train accuracy: 0.996894707001056; test accuracy: 0.9944000244140625; Time: 144.0283751487732\n",
            "8200: train loss: 0.012321230378446924; test loss: 0.017180191352963448; train accuracy: 0.9962577207512309; test accuracy: 0.9940999746322632; Time: 145.75750875473022\n",
            "8300: train loss: 0.01269612074511761; test loss: 0.017419271171092987; train accuracy: 0.9961813622154738; test accuracy: 0.9940999746322632; Time: 147.48158144950867\n",
            "8400: train loss: 0.013694250449845036; test loss: 0.017275936901569366; train accuracy: 0.9961307840191611; test accuracy: 0.9941999912261963; Time: 149.20486426353455\n",
            "8500: train loss: 0.012834278309750695; test loss: 0.01721077226102352; train accuracy: 0.9963886237234719; test accuracy: 0.9943000078201294; Time: 150.93164253234863\n",
            "8600: train loss: 0.011289411556867884; test loss: 0.01736234501004219; train accuracy: 0.9968168668654098; test accuracy: 0.994700014591217; Time: 152.66632056236267\n",
            "8700: train loss: 0.013017321936214925; test loss: 0.017269546166062355; train accuracy: 0.9962345818952063; test accuracy: 0.9945999979972839; Time: 154.39652562141418\n",
            "8800: train loss: 0.014496224350400658; test loss: 0.017436599358916283; train accuracy: 0.9956576495408389; test accuracy: 0.9945999979972839; Time: 156.12999725341797\n",
            "8900: train loss: 0.013426333096632536; test loss: 0.01747256889939308; train accuracy: 0.9960425053903578; test accuracy: 0.9944999814033508; Time: 157.85797429084778\n",
            "9000: train loss: 0.013848208349377588; test loss: 0.017605481669306755; train accuracy: 0.9959614925902277; test accuracy: 0.9945999979972839; Time: 159.5858166217804\n",
            "9100: train loss: 0.0129213019548187; test loss: 0.017657212913036346; train accuracy: 0.9963222193320576; test accuracy: 0.9944999814033508; Time: 161.3157935142517\n",
            "9200: train loss: 0.012895322598186968; test loss: 0.01786412112414837; train accuracy: 0.9967879903168413; test accuracy: 0.9943000078201294; Time: 163.0420436859131\n",
            "9300: train loss: 0.013746684866932736; test loss: 0.017960159108042717; train accuracy: 0.9960156368342495; test accuracy: 0.9940000176429749; Time: 164.76586627960205\n",
            "9400: train loss: 0.013238624479199637; test loss: 0.017971878871321678; train accuracy: 0.9964735969640597; test accuracy: 0.9940999746322632; Time: 166.4919137954712\n",
            "9500: train loss: 0.01288279248399114; test loss: 0.017641358077526093; train accuracy: 0.9963676927817179; test accuracy: 0.9944000244140625; Time: 168.21921563148499\n",
            "9600: train loss: 0.010917143991373494; test loss: 0.01772988960146904; train accuracy: 0.9965597486724423; test accuracy: 0.9943000078201294; Time: 169.93731141090393\n",
            "9700: train loss: 0.012480659685247704; test loss: 0.017638137564063072; train accuracy: 0.9959325424036825; test accuracy: 0.9943000078201294; Time: 171.6622016429901\n",
            "9800: train loss: 0.011933074420264207; test loss: 0.01727725751698017; train accuracy: 0.9966313836467231; test accuracy: 0.994700014591217; Time: 173.3804576396942\n",
            "9900: train loss: 0.011455592351460795; test loss: 0.017134729772806168; train accuracy: 0.9967345442041232; test accuracy: 0.9941999912261963; Time: 175.10957980155945\n",
            "[[ 978    0    0    0    0    0    1    1    0    0]\n",
            " [   0 1129    1    1    1    0    2    1    0    0]\n",
            " [   2    0 1028    0    0    0    0    2    0    0]\n",
            " [   0    0    0 1007    0    2    0    0    1    0]\n",
            " [   0    0    0    0  976    0    1    0    1    4]\n",
            " [   0    0    0    4    0  887    1    0    0    0]\n",
            " [   3    1    0    1    1    2  949    0    1    0]\n",
            " [   0    1    3    0    2    0    0 1021    0    1]\n",
            " [   1    0    0    2    1    0    0    0  970    0]\n",
            " [   0    0    0    1    4    2    0    2    1  999]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9939    0.9980    0.9959       980\n",
            "           1     0.9982    0.9947    0.9965      1135\n",
            "           2     0.9961    0.9961    0.9961      1032\n",
            "           3     0.9911    0.9970    0.9941      1010\n",
            "           4     0.9909    0.9939    0.9924       982\n",
            "           5     0.9933    0.9944    0.9938       892\n",
            "           6     0.9948    0.9906    0.9927       958\n",
            "           7     0.9942    0.9932    0.9937      1028\n",
            "           8     0.9959    0.9959    0.9959       974\n",
            "           9     0.9950    0.9901    0.9925      1009\n",
            "\n",
            "    accuracy                         0.9944     10000\n",
            "   macro avg     0.9943    0.9944    0.9944     10000\n",
            "weighted avg     0.9944    0.9944    0.9944     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AaK9JqPSo7R",
        "colab_type": "text"
      },
      "source": [
        "## Kron -> Scan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "152bfMaZ5_Xi",
        "colab_type": "code",
        "outputId": "566a5871-c2a6-4520-aff5-f4befdf63403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# grads ---> kron_grads ---> scan_grads\n",
        "\n",
        "step_size = 0.05\n",
        "grad_norm_clip_thr = 10   # gradients clipping may be necessary for RNN training; \n",
        "                            # set it to an extremely large value if no clipping is required   \n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "offset = 1e-9\n",
        "\n",
        "with tf.Session() as sess:   \n",
        "    eps = max([np.finfo(W.dtype.as_numpy_dtype).eps for W in Ws])\n",
        "    \n",
        "    Qs_left = [tf.Variable(tf.eye(W.shape.as_list()[0], dtype=dtype), trainable=False) for W in Ws]\n",
        "    Qs_right = [tf.Variable(tf.eye(W.shape.as_list()[1], dtype=dtype), trainable=False) for W in Ws]\n",
        "    qs_left = [tf.Variable(tf.concat([tf.ones((1, W.shape.as_list()[0])),tf.zeros((1, W.shape.as_list()[0]))], axis=0), trainable=False) for W in Ws]\n",
        "    qs_right = [tf.Variable(tf.ones((1, W.shape.as_list()[1])), trainable=False) for W in Ws]\n",
        "    \n",
        "    train_loss, train_accuracy = train_criterion(Ws)\n",
        "    grads = tf.gradients(train_loss, Ws)\n",
        "\n",
        "    kron_grads = [psgd.precond_grad_kron(ql, qr, g) for (ql, qr, g) in zip(Qs_left, Qs_right, grads)]\n",
        "    scan_grads = [psgd.precond_grad_scan(ql, qr, g) for (ql, qr, g) in zip(qs_left, qs_right, kron_grads)]\n",
        "\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum([tf.reduce_sum(g*g) for g in scan_grads]))\n",
        "    step_size_adjust = tf.minimum(1.0, grad_norm_clip_thr/(grad_norm + 1.2e-38))\n",
        "    \n",
        "    new_Ws = [W - (step_size_adjust*step_size)*pG for (W, pG) in zip(Ws, scan_grads)]\n",
        "    update_Ws = [tf.assign(W, new_W) for (W, new_W) in zip(Ws, new_Ws)]\n",
        "    \n",
        "    delta_Ws = [tf.random_normal(W.shape, dtype=dtype) for W in Ws]\n",
        "    grad_deltaw = tf.reduce_sum([tf.reduce_sum(g*v) for (g, v) in zip(grads, delta_Ws)])\n",
        "    hess_deltaw = tf.gradients(grad_deltaw, Ws)\n",
        "    \n",
        "    new_Qs = [psgd.update_precond_kron(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(Qs_left, Qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_Qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(Qs_left, Qs_right, new_Qs)]\n",
        "    new_qs = [psgd.update_precond_scan(ql, qr, dw, dg) for (ql, qr, dw, dg) in zip(qs_left, qs_right, delta_Ws, hess_deltaw)]\n",
        "    update_qs = [[tf.assign(old_ql, new_q[0]), tf.assign(old_qr, new_q[1])] for (old_ql, old_qr, new_q) in zip(qs_left, qs_right, new_qs)]\n",
        "    \n",
        "    \n",
        "    test_loss, test_accuracy = test_criterion(Ws)\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    avg_train_loss = 0.0\n",
        "    avg_train_acc= 0.0\n",
        "    TrainLoss = list()\n",
        "    TestLoss = list()\n",
        "    TrainAccuracy = list()\n",
        "    TestAccuracy = list()\n",
        "    Time = list()\n",
        "    \n",
        "    for num_iter in range(ITERATIONS):    \n",
        "        _train_inputs, _train_outputs = get_batches( )\n",
        "      \n",
        "        t0 = time.time()\n",
        "        _train_loss, _train_accuracy,  _, _,_ = sess.run([train_loss, train_accuracy, update_Ws, update_Qs, update_qs],\n",
        "                                     {train_inputs: _train_inputs, train_outputs: _train_outputs})\n",
        "        Time.append(time.time() - t0)\n",
        "        nu = min(num_iter/(1.0 + num_iter), 0.99)\n",
        "        avg_train_loss = nu*avg_train_loss + (1.0 - nu)*_train_loss\n",
        "        avg_train_acc = nu*avg_train_acc + (1.0 - nu)*_train_accuracy\n",
        "        TrainLoss.append(avg_train_loss)\n",
        "        TrainAccuracy.append(avg_train_acc)\n",
        "        if num_iter % GAP == 0:\n",
        "            _test_loss, _test_accuracy = sess.run([test_loss, test_accuracy])\n",
        "            TestLoss.append(_test_loss)\n",
        "            TestAccuracy.append(_test_accuracy)\n",
        "            print('{}: train loss: {}; test loss: {}; train accuracy: {}; test accuracy: {}; Time: {}'.format(\n",
        "                num_iter, TrainLoss[-1], TestLoss[-1], TrainAccuracy[-1], TestAccuracy[-1], np.sum(Time)))\n",
        "        if num_iter % 19999 == 0:\n",
        "           qs_l_kron_adam, qs_r_kron_adam = sess.run([Qs_left, Qs_right])\n",
        "\n",
        "    t = final_outputs(Ws)\n",
        "    test_predictions = sess.run(t)\n",
        "\n",
        "# determine precision and recall\n",
        "y_ = np.asarray(test_predictions)\n",
        "y_ = np.reshape(y_, (10000,))\n",
        "print(metrics.confusion_matrix(test_outputs, y_))\n",
        "print(metrics.classification_report(test_outputs, y_, digits=4))\n",
        "  \n",
        "scipy.io.savemat(results_dir + 'kron_scan.mat', {'TrainLoss': TrainLoss, 'TestLoss': TestLoss, 'TrainAccuracy': TrainAccuracy,'TestAccuracy': TestAccuracy, 'Time':Time})"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: train loss: 2.3923301696777344; test loss: 2.274977445602417; train accuracy: 0.078125; test accuracy: 0.11710000038146973; Time: 4.593583106994629\n",
            "100: train loss: 0.7739226437717679; test loss: 0.12078545987606049; train accuracy: 0.7606890625000002; test accuracy: 0.9631999731063843; Time: 6.326172351837158\n",
            "200: train loss: 0.3625540239915501; test loss: 0.04556639492511749; train accuracy: 0.8884151881095436; test accuracy: 0.9861999750137329; Time: 8.035696268081665\n",
            "300: train loss: 0.1766458531688637; test loss: 0.0329323410987854; train accuracy: 0.9455331704944209; test accuracy: 0.989300012588501; Time: 9.752628803253174\n",
            "400: train loss: 0.10188747403483543; test loss: 0.03168304264545441; train accuracy: 0.9690290092832704; test accuracy: 0.9901999831199646; Time: 11.467976331710815\n",
            "500: train loss: 0.06749131294550925; test loss: 0.027846014127135277; train accuracy: 0.9794810425534345; test accuracy: 0.9911999702453613; Time: 13.184107065200806\n",
            "600: train loss: 0.04896234065547075; test loss: 0.025554494932293892; train accuracy: 0.9848030087493967; test accuracy: 0.9921000003814697; Time: 14.905658721923828\n",
            "700: train loss: 0.042942281320791545; test loss: 0.024277126416563988; train accuracy: 0.9864020203043996; test accuracy: 0.9914000034332275; Time: 16.620864152908325\n",
            "800: train loss: 0.03567143273527427; test loss: 0.022743845358490944; train accuracy: 0.9890309814014228; test accuracy: 0.9919999837875366; Time: 18.33879065513611\n",
            "900: train loss: 0.035986762393792664; test loss: 0.022041412070393562; train accuracy: 0.9892290359883467; test accuracy: 0.9923999905586243; Time: 20.048065185546875\n",
            "1000: train loss: 0.0306959904433188; test loss: 0.026926977559924126; train accuracy: 0.9908616673158996; test accuracy: 0.9904999732971191; Time: 21.76520037651062\n",
            "1100: train loss: 0.02978852634764782; test loss: 0.021559281274676323; train accuracy: 0.9914149204200349; test accuracy: 0.9934999942779541; Time: 23.477484464645386\n",
            "1200: train loss: 0.02819709404415364; test loss: 0.020613979548215866; train accuracy: 0.9919361653134899; test accuracy: 0.9921000003814697; Time: 25.19045329093933\n",
            "1300: train loss: 0.027914612715123188; test loss: 0.020672151818871498; train accuracy: 0.991460849378401; test accuracy: 0.9940000176429749; Time: 26.902590036392212\n",
            "1400: train loss: 0.026736745964540228; test loss: 0.01966385915875435; train accuracy: 0.9919912766184779; test accuracy: 0.9926999807357788; Time: 28.628754138946533\n",
            "1500: train loss: 0.023160627095738774; test loss: 0.017457380890846252; train accuracy: 0.9931464709891797; test accuracy: 0.9939000010490417; Time: 30.34134840965271\n",
            "1600: train loss: 0.02162361172425116; test loss: 0.01866120472550392; train accuracy: 0.9937368441497313; test accuracy: 0.9937000274658203; Time: 32.05462908744812\n",
            "1700: train loss: 0.023175831082455536; test loss: 0.01807553507387638; train accuracy: 0.9928377982575431; test accuracy: 0.9937999844551086; Time: 33.76466703414917\n",
            "1800: train loss: 0.023904092575358273; test loss: 0.017665572464466095; train accuracy: 0.992753955068309; test accuracy: 0.9941999912261963; Time: 35.47765398025513\n",
            "1900: train loss: 0.021988222180736594; test loss: 0.018185056746006012; train accuracy: 0.9935370330774592; test accuracy: 0.9934999942779541; Time: 37.19099473953247\n",
            "2000: train loss: 0.023213815308456648; test loss: 0.018958386033773422; train accuracy: 0.9932685676686955; test accuracy: 0.993399977684021; Time: 38.90964365005493\n",
            "2100: train loss: 0.019425156284919115; test loss: 0.01603062078356743; train accuracy: 0.9940988263729351; test accuracy: 0.9937999844551086; Time: 40.61732292175293\n",
            "2200: train loss: 0.018856584435756166; test loss: 0.01722608506679535; train accuracy: 0.9939780641319563; test accuracy: 0.9937000274658203; Time: 42.32653760910034\n",
            "2300: train loss: 0.017491477904323788; test loss: 0.016267046332359314; train accuracy: 0.9946280973648665; test accuracy: 0.9940000176429749; Time: 44.0443160533905\n",
            "2400: train loss: 0.017478740594210077; test loss: 0.017491936683654785; train accuracy: 0.994852285400563; test accuracy: 0.9945999979972839; Time: 45.76292037963867\n",
            "2500: train loss: 0.018084676432051697; test loss: 0.017572559416294098; train accuracy: 0.9949789724902685; test accuracy: 0.9939000010490417; Time: 47.4750394821167\n",
            "2600: train loss: 0.01846334337607906; test loss: 0.0165597815066576; train accuracy: 0.9946862021696405; test accuracy: 0.9940000176429749; Time: 49.19304299354553\n",
            "2700: train loss: 0.01790720417314424; test loss: 0.01632230170071125; train accuracy: 0.9948934999984976; test accuracy: 0.9945999979972839; Time: 50.91299057006836\n",
            "2800: train loss: 0.01677644027635936; test loss: 0.017127664759755135; train accuracy: 0.9949310510501312; test accuracy: 0.9940999746322632; Time: 52.62702703475952\n",
            "2900: train loss: 0.015276961541096059; test loss: 0.017723599448800087; train accuracy: 0.9957036205331674; test accuracy: 0.9940999746322632; Time: 54.34577465057373\n",
            "3000: train loss: 0.014694575414048918; test loss: 0.01809743046760559; train accuracy: 0.9958391502523687; test accuracy: 0.994700014591217; Time: 56.06025004386902\n",
            "3100: train loss: 0.013620169936674536; test loss: 0.019162021577358246; train accuracy: 0.9960254162709609; test accuracy: 0.9940000176429749; Time: 57.7696053981781\n",
            "3200: train loss: 0.014199549767029949; test loss: 0.017203345894813538; train accuracy: 0.9959785536751041; test accuracy: 0.9941999912261963; Time: 59.488773822784424\n",
            "3300: train loss: 0.013155680105331592; test loss: 0.016965631395578384; train accuracy: 0.9962693810375614; test accuracy: 0.9944000244140625; Time: 61.206698179244995\n",
            "3400: train loss: 0.014238591182063947; test loss: 0.01573006808757782; train accuracy: 0.9958383787525348; test accuracy: 0.9948999881744385; Time: 62.917248249053955\n",
            "3500: train loss: 0.01504748222596333; test loss: 0.01680855080485344; train accuracy: 0.9961786500592879; test accuracy: 0.9944999814033508; Time: 64.6290214061737\n",
            "3600: train loss: 0.013857927791341956; test loss: 0.01944749802350998; train accuracy: 0.9959361560176112; test accuracy: 0.9945999979972839; Time: 66.34760117530823\n",
            "3700: train loss: 0.01262447086969599; test loss: 0.017756668850779533; train accuracy: 0.9963521114804658; test accuracy: 0.9952999949455261; Time: 68.0595223903656\n",
            "3800: train loss: 0.012445655338654455; test loss: 0.01709296926856041; train accuracy: 0.9962603649337123; test accuracy: 0.9952999949455261; Time: 69.772634267807\n",
            "3900: train loss: 0.01578501385873223; test loss: 0.018595488741993904; train accuracy: 0.9955043900637809; test accuracy: 0.9947999715805054; Time: 71.48958659172058\n",
            "4000: train loss: 0.010747568751457243; test loss: 0.017469782382249832; train accuracy: 0.9966176444344833; test accuracy: 0.9944000244140625; Time: 73.20506048202515\n",
            "4100: train loss: 0.012257842976110564; test loss: 0.017935605719685555; train accuracy: 0.9963499594462185; test accuracy: 0.9948999881744385; Time: 74.92424416542053\n",
            "4200: train loss: 0.010697899996008366; test loss: 0.017226651310920715; train accuracy: 0.9969472283216455; test accuracy: 0.994700014591217; Time: 76.63590049743652\n",
            "4300: train loss: 0.0097697883151134; test loss: 0.018627852201461792; train accuracy: 0.9967074059561917; test accuracy: 0.9939000010490417; Time: 78.35508275032043\n",
            "4400: train loss: 0.010071918752292897; test loss: 0.017034998163580894; train accuracy: 0.9966992465191267; test accuracy: 0.9945999979972839; Time: 80.07632493972778\n",
            "4500: train loss: 0.008823072013867134; test loss: 0.017133226618170738; train accuracy: 0.9971641888223809; test accuracy: 0.9947999715805054; Time: 81.788076877594\n",
            "4600: train loss: 0.009247289442295801; test loss: 0.01710013672709465; train accuracy: 0.9970156948425617; test accuracy: 0.994700014591217; Time: 83.49495005607605\n",
            "4700: train loss: 0.011325016527484097; test loss: 0.016549644991755486; train accuracy: 0.9963082631974757; test accuracy: 0.9952999949455261; Time: 85.20296835899353\n",
            "4800: train loss: 0.009432663393957177; test loss: 0.01583382487297058; train accuracy: 0.9970543395473254; test accuracy: 0.9945999979972839; Time: 86.91640257835388\n",
            "4900: train loss: 0.008385407771328117; test loss: 0.016388146206736565; train accuracy: 0.9969344029902026; test accuracy: 0.9951000213623047; Time: 88.62869071960449\n",
            "5000: train loss: 0.008453170153726838; test loss: 0.016973592340946198; train accuracy: 0.9969349074323557; test accuracy: 0.9944000244140625; Time: 90.34514498710632\n",
            "5100: train loss: 0.009116898657211664; test loss: 0.017242252826690674; train accuracy: 0.9971872619174241; test accuracy: 0.9945999979972839; Time: 92.06169724464417\n",
            "5200: train loss: 0.010743221084853517; test loss: 0.01785576157271862; train accuracy: 0.9973198364280581; test accuracy: 0.9944000244140625; Time: 93.77481245994568\n",
            "5300: train loss: 0.013421635458225202; test loss: 0.01787359081208706; train accuracy: 0.9968144103487893; test accuracy: 0.9944000244140625; Time: 95.48435997962952\n",
            "5400: train loss: 0.010324040907658373; test loss: 0.018818184733390808; train accuracy: 0.9972609422641432; test accuracy: 0.9944999814033508; Time: 97.1909511089325\n",
            "5500: train loss: 0.009402575751215465; test loss: 0.017671316862106323; train accuracy: 0.9970538528566301; test accuracy: 0.9947999715805054; Time: 98.89551877975464\n",
            "5600: train loss: 0.00832344701903843; test loss: 0.017519604414701462; train accuracy: 0.9971723338304093; test accuracy: 0.994700014591217; Time: 100.60664010047913\n",
            "5700: train loss: 0.008526127989080971; test loss: 0.017443371936678886; train accuracy: 0.9975241984743604; test accuracy: 0.9948999881744385; Time: 102.32303595542908\n",
            "5800: train loss: 0.009528428106891345; test loss: 0.018159694969654083; train accuracy: 0.9973070220323766; test accuracy: 0.9943000078201294; Time: 104.03479075431824\n",
            "5900: train loss: 0.009194601172915682; test loss: 0.017821384593844414; train accuracy: 0.9974321892834919; test accuracy: 0.9944000244140625; Time: 105.74995112419128\n",
            "6000: train loss: 0.008812445406179256; test loss: 0.017715323716402054; train accuracy: 0.9974058058831264; test accuracy: 0.994700014591217; Time: 107.46603298187256\n",
            "6100: train loss: 0.008610946639854487; test loss: 0.018194912001490593; train accuracy: 0.9974933717629356; test accuracy: 0.994700014591217; Time: 109.18435311317444\n",
            "6200: train loss: 0.008631712611102718; test loss: 0.01836433634161949; train accuracy: 0.997579270248882; test accuracy: 0.9945999979972839; Time: 110.9013729095459\n",
            "6300: train loss: 0.008767982826108066; test loss: 0.018160978332161903; train accuracy: 0.9975237258902592; test accuracy: 0.9943000078201294; Time: 112.61508965492249\n",
            "6400: train loss: 0.0075969390595111; test loss: 0.017697101458907127; train accuracy: 0.9981343834908004; test accuracy: 0.9944999814033508; Time: 114.33187651634216\n",
            "6500: train loss: 0.008538707696193628; test loss: 0.01778426580131054; train accuracy: 0.9977649200071655; test accuracy: 0.994700014591217; Time: 116.0411684513092\n",
            "6600: train loss: 0.008081685641780493; test loss: 0.01690972037613392; train accuracy: 0.9977613188941584; test accuracy: 0.9947999715805054; Time: 117.75322842597961\n",
            "6700: train loss: 0.006860016661536093; test loss: 0.016901202499866486; train accuracy: 0.9978498129383137; test accuracy: 0.9948999881744385; Time: 119.46508836746216\n",
            "6800: train loss: 0.00706566988876757; test loss: 0.01697416417300701; train accuracy: 0.9976684967002365; test accuracy: 0.9948999881744385; Time: 121.18307709693909\n",
            "6900: train loss: 0.00807156007914148; test loss: 0.017947671934962273; train accuracy: 0.9981476560764349; test accuracy: 0.9944999814033508; Time: 122.90670228004456\n",
            "7000: train loss: 0.00786268420043354; test loss: 0.01771855726838112; train accuracy: 0.9979647530306557; test accuracy: 0.9944999814033508; Time: 124.63135719299316\n",
            "7100: train loss: 0.008437185987638257; test loss: 0.017990538850426674; train accuracy: 0.9979060343465127; test accuracy: 0.9944999814033508; Time: 126.34656620025635\n",
            "7200: train loss: 0.0074527753219151725; test loss: 0.018155241385102272; train accuracy: 0.9980171857253243; test accuracy: 0.9947999715805054; Time: 128.0616853237152\n",
            "7300: train loss: 0.00743138871346372; test loss: 0.017724890261888504; train accuracy: 0.9977447451925439; test accuracy: 0.9944999814033508; Time: 129.76899886131287\n",
            "7400: train loss: 0.006749440510432449; test loss: 0.017231380566954613; train accuracy: 0.9977093602439993; test accuracy: 0.9944999814033508; Time: 131.48364973068237\n",
            "7500: train loss: 0.008016559237415765; test loss: 0.017525723204016685; train accuracy: 0.9976900989130908; test accuracy: 0.9947999715805054; Time: 133.2070655822754\n",
            "7600: train loss: 0.006130995814112366; test loss: 0.017676686868071556; train accuracy: 0.9981937929136461; test accuracy: 0.9950000047683716; Time: 134.92624759674072\n",
            "7700: train loss: 0.005354025292442368; test loss: 0.016679797321558; train accuracy: 0.9983117330018465; test accuracy: 0.9947999715805054; Time: 136.6390130519867\n",
            "7800: train loss: 0.007445423701601192; test loss: 0.017352750524878502; train accuracy: 0.9982237124064448; test accuracy: 0.9944999814033508; Time: 138.35068702697754\n",
            "7900: train loss: 0.007180731923153982; test loss: 0.017601653933525085; train accuracy: 0.9979171431173933; test accuracy: 0.9948999881744385; Time: 140.05916571617126\n",
            "8000: train loss: 0.00641649985555084; test loss: 0.017722876742482185; train accuracy: 0.9983149090990421; test accuracy: 0.9944999814033508; Time: 141.76902389526367\n",
            "8100: train loss: 0.0064697006996340825; test loss: 0.0182541161775589; train accuracy: 0.9982156894183697; test accuracy: 0.9944999814033508; Time: 143.4786193370819\n",
            "8200: train loss: 0.006836427813420648; test loss: 0.01762990653514862; train accuracy: 0.9981208025534082; test accuracy: 0.9944000244140625; Time: 145.1910080909729\n",
            "8300: train loss: 0.0073969330877656295; test loss: 0.017128797248005867; train accuracy: 0.9980308380253653; test accuracy: 0.9948999881744385; Time: 146.9073784351349\n",
            "8400: train loss: 0.005881478215415057; test loss: 0.018146369606256485; train accuracy: 0.9981026197760322; test accuracy: 0.995199978351593; Time: 148.62741136550903\n",
            "8500: train loss: 0.004824819607419402; test loss: 0.01827053911983967; train accuracy: 0.9984363348625088; test accuracy: 0.9941999912261963; Time: 150.33567261695862\n",
            "8600: train loss: 0.005546882168970249; test loss: 0.017764858901500702; train accuracy: 0.9982131869954757; test accuracy: 0.9944999814033508; Time: 152.0479485988617\n",
            "8700: train loss: 0.006347391398139655; test loss: 0.01848607510328293; train accuracy: 0.9982670201480071; test accuracy: 0.9945999979972839; Time: 153.75832104682922\n",
            "8800: train loss: 0.007079607825398356; test loss: 0.017484767362475395; train accuracy: 0.9982186978430062; test accuracy: 0.9950000047683716; Time: 155.46512174606323\n",
            "8900: train loss: 0.005971317133574541; test loss: 0.017659367993474007; train accuracy: 0.9983753727020539; test accuracy: 0.9944999814033508; Time: 157.18224596977234\n",
            "9000: train loss: 0.006326668790476918; test loss: 0.01738712377846241; train accuracy: 0.997932100690334; test accuracy: 0.9950000047683716; Time: 158.9065556526184\n",
            "9100: train loss: 0.006013695652975468; test loss: 0.017551319673657417; train accuracy: 0.9983011909073092; test accuracy: 0.9947999715805054; Time: 160.61331605911255\n",
            "9200: train loss: 0.0064709394217253285; test loss: 0.017775261774659157; train accuracy: 0.9980086556402906; test accuracy: 0.9948999881744385; Time: 162.32241010665894\n",
            "9300: train loss: 0.005415175043935605; test loss: 0.017404651269316673; train accuracy: 0.9982504876996514; test accuracy: 0.994700014591217; Time: 164.03785347938538\n",
            "9400: train loss: 0.006444783412311987; test loss: 0.017902623862028122; train accuracy: 0.9980186468079228; test accuracy: 0.9948999881744385; Time: 165.75139093399048\n",
            "9500: train loss: 0.006387774981458338; test loss: 0.01798572950065136; train accuracy: 0.9980493028459146; test accuracy: 0.9948999881744385; Time: 167.46457505226135\n",
            "9600: train loss: 0.006087803103555615; test loss: 0.018553931266069412; train accuracy: 0.9982977705048343; test accuracy: 0.9945999979972839; Time: 169.1791546344757\n",
            "9700: train loss: 0.005691267724724787; test loss: 0.01895873248577118; train accuracy: 0.9984119756637133; test accuracy: 0.9944000244140625; Time: 170.8861825466156\n",
            "9800: train loss: 0.00620800412059432; test loss: 0.01824500784277916; train accuracy: 0.998181235912101; test accuracy: 0.9947999715805054; Time: 172.5951006412506\n",
            "9900: train loss: 0.006871416498709122; test loss: 0.017806213349103928; train accuracy: 0.9979164628976055; test accuracy: 0.9950000047683716; Time: 174.30367016792297\n",
            "[[ 979    0    0    0    0    0    0    1    0    0]\n",
            " [   0 1130    2    0    0    0    1    2    0    0]\n",
            " [   2    0 1026    0    0    0    1    3    0    0]\n",
            " [   0    0    0 1008    0    1    0    1    0    0]\n",
            " [   0    0    0    0  980    0    0    0    0    2]\n",
            " [   0    0    0    5    0  886    1    0    0    0]\n",
            " [   4    1    1    0    0    0  951    0    1    0]\n",
            " [   1    1    2    0    0    0    0 1023    0    1]\n",
            " [   0    0    0    2    0    1    0    0  971    0]\n",
            " [   0    0    1    0    5    1    0    2    5  995]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9929    0.9990    0.9959       980\n",
            "           1     0.9982    0.9956    0.9969      1135\n",
            "           2     0.9942    0.9942    0.9942      1032\n",
            "           3     0.9931    0.9980    0.9956      1010\n",
            "           4     0.9949    0.9980    0.9964       982\n",
            "           5     0.9966    0.9933    0.9949       892\n",
            "           6     0.9969    0.9927    0.9948       958\n",
            "           7     0.9913    0.9951    0.9932      1028\n",
            "           8     0.9939    0.9969    0.9954       974\n",
            "           9     0.9970    0.9861    0.9915      1009\n",
            "\n",
            "    accuracy                         0.9949     10000\n",
            "   macro avg     0.9949    0.9949    0.9949     10000\n",
            "weighted avg     0.9949    0.9949    0.9949     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsW76pTYQe85",
        "colab_type": "text"
      },
      "source": [
        "# Comparison of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdRz65myTog1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opts = ['SGD','adam','rmsprop','kron_psgd','scan_psgd','scaw_psgd', 'reverse_scan_psgd','reverse_scaw_psgd',\n",
        "        'scan_pro_psgd','hybrid_skron_psgd','hybrid_kscan_psgd','kdiag_psgd', 'kron_adam','adam_kron',\n",
        "\t\t\t\t'kron_scan','scan_kron','scan_kron_avg','moment_kron','test']\n",
        "\n",
        "opts = ['SGD','adam','kron_psgd','hybrid_kscan_psgd','scan_psgd','kron_kron','kron_scan']\n",
        "\n",
        "total_train_time = {}\n",
        "opts_data = {}\n",
        "times = {}\n",
        "train_times = {}\n",
        "test_times = {}\n",
        "train_losses = {}\n",
        "test_losses = {}\n",
        "train_accs = {}\n",
        "test_accs = {}\n",
        "\n",
        "\n",
        "for opt in opts:\n",
        "\topts_data[opt] = scipy.io.loadmat(results_dir+opt+'.mat')\t\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMG5BHM91sry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for opt in opts:\n",
        "  # print(opt)\n",
        "  data = opts_data[opt]\n",
        "  times[opt] = data.get('Time')\n",
        "  train_times[opt] = np.cumsum(times[opt])\n",
        "  total_train_time[opt] = np.sum(times[opt])\n",
        "  train_losses[opt] = data.get('TrainLoss').reshape(ITERATIONS,)\n",
        "  train_accs[opt] = data.get('TrainAccuracy').reshape(ITERATIONS,)\n",
        "  test_losses[opt] = data.get('TestLoss').reshape(int(ITERATIONS/GAP),)\n",
        "  test_accs[opt] = data.get('TestAccuracy').reshape(int(ITERATIONS/GAP,))\n",
        "  tt = []\n",
        "  for i in range(0,ITERATIONS,GAP):\n",
        "    tt.append(train_times[opt][i])\n",
        "  test_times[opt] = tt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp2hCgFOg3ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# colors = ['b','g','r','c','m','y','k','orange','lime','aqua','deeppink','darkviolet','chocolate','midnightblue','coral'] ,matplotlib\n",
        "# colors = px.colors.qualitative.Dark24_r\n",
        "colors = ['#0000FF','#00FF00','#FF0000','#33F0FF','#FFA833','#FFF933','#83FF33','#33E0FF','#FF33E6','#D433FF','#888A0B','#8A0B1E','#B498DF','#1B786D']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC2ib0d_oSxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# plot train_losses vs Iterations\n",
        "plot_loss_metrics(None,train_losses,'Train Loss vs Iterations', 'Iterations','Train Loss')\n",
        "# plot test_losses vs Iterations\n",
        "plot_loss_metrics(None,test_losses,'Test Loss vs Iterations', 'Iterations','Test Loss')\n",
        "# # plot test_losses vs Iterations\n",
        "plot_loss_metrics(train_times,train_losses,'Train Loss vs Time', 'Time','Train Loss')\n",
        "# plot test_losses vs Iterations\n",
        "plot_loss_metrics(test_times,test_losses,'Test Loss vs Time', 'Time','Test Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq_xb1l5V-36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot train_losses vs Iterations\n",
        "plot_acc_metrics(None,train_accs,'Train Accuracy vs Iterations', 'Iterations','Train Accuracy')\n",
        "# plot test_losses vs Iterations\n",
        "plot_acc_metrics(None,test_accs,'Test Accuracys vs Iterations', 'Iterations','Test Accuracy')\n",
        "# # plot test_losses vs Iterations\n",
        "plot_acc_metrics(train_times,train_accs,'Train Accuracy vs Time', 'Time','Train Accuracy')\n",
        "# plot test_losses vs Iterations\n",
        "plot_acc_metrics(test_times,test_accs,'Test Accuracy vs Time', 'Time','Test Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88CZ_Ax35eUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " print(tabulate(zip([(k) for k,v in total_train_time.items()],\n",
        "                   [v for k,v in total_train_time.items()],\n",
        "                   [v/ITERATIONS for k,v in total_train_time.items()]),headers = ['optimizers','total time (sec)','time per iteration']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvu0gvExLLnZ",
        "colab_type": "text"
      },
      "source": [
        "**A. Loss vs Epochs**\n",
        "\n",
        "\n",
        "1.   Training:  KRON_KRON is best. Though At the start, all combinations converge better than simple KRONECKER preconditioner. But later, above mentioned seems to work best while SCAN_KRON and KRON_ADAM seems to work worst. \n",
        "Although KRON_SCAN works comparable even in latter stages. Not too bad \n",
        "2.   Testing: SCAN_KRON_AVG, KRON_KRON which work best in training start to overfit. KRON_SCAN is the best one. And SCAN_KRON works comparable to it\n",
        "\n",
        "SCAN.KRON and its sqrt doesn't seem to work\n",
        "**B. Accuracy vs Epochs**\n",
        "KRON_SCAN is best in testing and training. ADAM_KRON overfits though good in training\n",
        "\n",
        "CONCLUSION: KRON_SCAN is the winner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqlX4uXJDcVv",
        "colab_type": "text"
      },
      "source": [
        "# NGROK LAUNCH TENSORBOARD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2jS78xdCVV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz9QBEbOCSzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzI5IGxeCaA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjmTnRh_CdbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MirKR3NA3bdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(10)\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=x, y=x**2))\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SrhDoFTXhZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}